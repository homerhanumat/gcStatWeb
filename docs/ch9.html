<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<title>Chapter 9: Confidence Intervals</title>

<script src="ch9_files/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link href="ch9_files/bootstrap-2.3.2/css/bootstrap.min.css" rel="stylesheet" />
<link href="ch9_files/bootstrap-2.3.2/css/bootstrap-responsive.min.css" rel="stylesheet" />
<script src="ch9_files/bootstrap-2.3.2/js/bootstrap.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="ch9_files/highlight/default.css"
      type="text/css" />
<script src="ch9_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>


<link rel="stylesheet" href="css/coursenotes.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
</style>
<div class="container-fluid main-container">


<div id="header">
<h1 class="title">Chapter 9: Confidence Intervals</h1>
</div>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#summary-of-formulas">Summary of Formulas</a></li>
</ul></li>
<li><a href="#conceptual-understanding-of-confidence-intervals">Conceptual Understanding of Confidence Intervals</a></li>
<li><a href="#chapter-outline">Chapter Outline</a></li>
<li><a href="#one-population-mean-mu">One Population Mean, <span class="math">\(\mu\)</span></a><ul>
<li><a href="#the-four-steps">The Four Steps</a></li>
<li><a href="#under-the-hood">Under the Hood</a></li>
<li><a href="#additional-example-and-further-ideas">Additional Example, and Further Ideas</a></li>
</ul></li>
<li><a href="#difference-of-two-population-means-mu_1-mu_2">Difference of Two Population Means, <span class="math">\(\mu_1-\mu_2\)</span></a><ul>
<li><a href="#the-four-steps-1">The Four Steps</a></li>
<li><a href="#under-the-hood-1">Under the Hood</a><ul>
<li><a href="#calculation-of-ci">Calculation of CI</a></li>
<li><a href="#order-of-groups">Order of Groups</a></li>
</ul></li>
<li><a href="#additional-example">Additional Example</a></li>
</ul></li>
<li><a href="#mean-of-differences-mu_d">Mean of Differences, <span class="math">\(\mu_d\)</span></a><ul>
<li><a href="#the-four-steps-2">The Four Steps</a></li>
<li><a href="#under-the-hood-2">Under the Hood</a></li>
<li><a href="#additional-example-1">Additional Example</a></li>
</ul></li>
<li><a href="#one-population-proportion-p">One Population Proportion, <span class="math">\(p\)</span></a><ul>
<li><a href="#the-four-steps-3">The Four Steps</a></li>
<li><a href="#under-the-hood-3">Under the Hood</a><ul>
<li><a href="#how-the-test-works">How the Test Works</a></li>
<li><a href="#values-in-the-confidence-interval">Values in the Confidence Interval</a></li>
</ul></li>
<li><a href="#additional-example-2">Additional Example</a></li>
</ul></li>
<li><a href="#difference-of-two-population-proportions-p_1-p_2">Difference of Two Population Proportions, <span class="math">\(p_1-p_2\)</span></a><ul>
<li><a href="#the-four-steps-4">The Four Steps</a></li>
<li><a href="#under-the-hood-4">Under the Hood</a></li>
<li><a href="#additional-example-3">Additional Example</a></li>
</ul></li>
<li><a href="#thoughts-on-r">Thoughts on R</a><ul>
<li><a href="#new-r-functions">New R Functions</a></li>
</ul></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>One of the main goals in statistics is <strong>inference</strong>. We are interested in using the knowledge we obtain from a <em>sample</em> to infer, or predict, something about the <em>population</em> at large. Numbers that describe a particular aspect of the sample are called <strong>statistics</strong> and numbers that describe an aspect of the population are called <strong>parameters</strong>. Since we typically do not know the value of a parameter, a statistic is our <em>single best guess</em> at it.</p>
<p>In Chapter 8, you learned about the Big Five Parameters and the statistics that estimate them, the Big Five Estimators. Due to the randomness involved in selecting a sample, a statistic has variation associated with it. This means that if we take a large number of samples from the same population, the value of the statistic computed from each sample won’t always be the same value.</p>
<p>The variation in the value of a statistic is what gives us a <em>distribution</em> for the statistic. Each of the Big Five Estimators have an associated distribution with a center, spread, and shape.</p>
<p>The center of the distribution is what we call the <em>expected value, EV</em>, - the number that we expect the statistic to be. The spread of the distribution is what we call the <em>standard deviation, SD</em>, - how far off we think our statistic might be from the actual value of the parameter. You can think of the standard deviation as the “give or take”. We saw that, at large sample sizes, the shape of each of the distributions for the Big Five Estimators was approximately normal (bell-shaped).</p>
<p>Let’s look again at the summary from Chapter 8.</p>
<div id="summary-of-formulas" class="section level2">
<h2>Summary of Formulas</h2>
<ul>
<li>For one mean <span class="math">\(\mu\)</span>:
<ul>
<li>Estimator is <span class="math">\(\bar{x}\)</span></li>
<li>EV is <span class="math">\(\mu\)</span></li>
<li>SD is <span class="math">\(\frac{\sigma}{\sqrt{n}}\)</span></li>
<li>SE is <span class="math">\(\frac{s}{\sqrt{n}}\)</span></li>
</ul></li>
<li>For the difference of two means <span class="math">\(\mu_1-\mu_2\)</span>:
<ul>
<li>Estimator is <span class="math">\(\bar{x}_1-\bar{x}_2\)</span></li>
<li>EV is <span class="math">\(\mu_1-\mu_2\)</span></li>
<li>SD is <span class="math">\(\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}\)</span></li>
<li>SE is <span class="math">\(\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}\)</span></li>
</ul></li>
<li>For one proportion <span class="math">\(p\)</span>:
<ul>
<li>Estimator is <span class="math">\(\hat{p}\)</span></li>
<li>EV is <span class="math">\(p\)</span></li>
<li>SD is <span class="math">\(\sqrt{\frac{p(1-p)}{n}}\)</span></li>
<li>SE is <span class="math">\(\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\)</span></li>
</ul></li>
<li>For the difference of two proportions <span class="math">\(p_1-p_2\)</span>:
<ul>
<li>Estimator is <span class="math">\(\hat{p}_1-\hat{p}_2\)</span></li>
<li>EV is <span class="math">\(p_1-p_2\)</span></li>
<li>SD is <span class="math">\(\sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}}\)</span></li>
<li>SE is <span class="math">\(\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}\)</span></li>
</ul></li>
<li>For the mean of differences <span class="math">\(\mu_d\)</span>:
<ul>
<li>Estimator is <span class="math">\(\bar{d}\)</span></li>
<li>EV is <span class="math">\(\mu_d\)</span></li>
<li>SD is <span class="math">\(\frac{\sigma_d}{\sqrt{n}}\)</span></li>
<li>SE is <span class="math">\(\frac{s_d}{\sqrt{n}}\)</span></li>
</ul></li>
</ul>
<p>A statistic is our <em>single best guess</em> at the value of a parameter. Since we know that the value of a statistic varies depending on the sample, this leads to the question, “How <strong>sure</strong> are we that the value of the statistic that we’ve calculated is actually close to the value of the parameter?”. Instead of reporting a single number for our estimate, even if it is our <em>best guess</em>, perhaps we would be better off giving a range of values such that we have some degree of confidence that it contains the true value of the parameter. Let’s explore this idea through an example.</p>
<blockquote>
<p><strong>Research Question</strong>: What is the average annual household income for the folks from <code>imagpop</code>?</p>
</blockquote>
<p>This is a research question about one population mean, <span class="math">\(\mu\)</span>. Let’s estimate <span class="math">\(\mu\)</span> by drawing a SRS of size 50 from <code>imagpop</code>.</p>
<pre class="r"><code>srs.income&lt;-popsamp(50,imagpop)
mean(~income,data=srs.income)</code></pre>
<pre><code>## [1] 46184</code></pre>
<p>Since we took a SRS, we can be fairly sure that this estimate, <span class="math">\(\bar{x}=\)</span> $ 46184 is <em>close</em> to the true parameter value. However, if we re-run the code above, we will get a different value.</p>
<pre class="r"><code>srs.income&lt;-popsamp(50,imagpop)
mean(~income,data=srs.income)</code></pre>
<pre><code>## [1] 40996</code></pre>
<p>Because of this variation, we can also be reasonably certain that our statistic is <em>not exactly equal</em> to the true parameter value. However, this gives us a good idea of a range into which our parameter value likely falls. Look at the <span class="math">\(\bar{x}\)</span>’s from 15 different samples:</p>
<pre><code>## Loading required package: parallel</code></pre>
<pre><code>##    result
## 1   50018
## 2   36928
## 3   48100
## 4   44886
## 5   39492
## 6   33856
## 7   43362
## 8   47086
## 9   41120
## 10  39954
## 11  44512
## 12  41970
## 13  47072
## 14  40020
## 15  38442</code></pre>
<p>Based on these samples, it seems likely that the true average annual income (the parameter value) probably falls somewhere in between <span class="math">\(33856\)</span> and <span class="math">\(50018.\)</span></p>
<p>[How <em>likely</em> is it that the parameter falls in this range, though? In other words, how <em>confident</em> are we that the true parameter value falls between <span class="math">\(33856\)</span> and <span class="math">\(50018\)</span>? When we start trying to think of this in terms of likelihood or probability, we need a more mathematical way to compute such an interval. The method we used above is very rudimentary. We’ll get to a better method of construction momentarily!]</p>
<p>Reporting a range of reasonable values for the parameter is a better approach to giving an estimate of a population parameter than giving one single best guess. Reporting a single value as our estimate is like throwing all of our eggs into one basket - “This is our best guess and that’s it!”. We, as researchers, would be failing to recognize the chance variation that is an essential part of our process of estimation. People reading the results of such a study also need to be made aware of the “give or take” that is associated with any estimate.</p>
<p><strong>Warning:</strong> Although confidence intervals take into account the error, or variability, associated with the estimation of a parameter, they do not account for all sources of error. For example, errors from an incorrect experiment design or sampling bias can creep into a statistical analysis and are not accounted for in a confidence interval. Refer to Chapters 5 &amp; 6 for these types of errors.</p>
</div>
</div>
<div id="conceptual-understanding-of-confidence-intervals" class="section level1">
<h1>Conceptual Understanding of Confidence Intervals</h1>
<p>The “range of values” that we have been discussing is called a <strong>confidence interval</strong>. A confidence interval answers the question, “Given the data at hand, within what range of values does the parameter of interest <em>probably</em> lie?”</p>
<blockquote>
<p><em>Definition:</em> A <strong>confidence interval</strong> is an interval of values for the population parameter that could be considered reasonable, based on the data at hand.</p>
</blockquote>
<p>A confidence interval is calculated using the general equation <span class="math">\[\mbox{Sample Estimate }\pm\mbox{ Margin of Error}\]</span> where <span class="math">\[\mbox{Margin of Error }=\mbox{ Multiplier }*\mbox{ Standard Error.}\]</span></p>
<p>The <em>sample estimate</em>, <em>multiplier</em>, and <em>standard error</em> depend on the parameter being estimated, but the <strong>interpretation of the confidence interval</strong> for any parameter is the same.</p>
<ul>
<li><p>The <span class="math">\(\mbox{Sample Estimate}\)</span> is our single best guess at the value of the population parameter. It is the center of the confidence interval.</p></li>
<li><p>The <span class="math">\(\mbox{Standard Error}\)</span> is the estimate of the standard deviation of the population parameter. It is affected by the sample size, <span class="math">\(n\)</span>. Recall from Chapter 8 that larger sample sizes yield smaller standard deviations. The same holds true for standard error, since the formulas for SE have the same structure as the formulas for SD. (Look at the formulas in the summary shown above.) <strong>Larger sample sizes produce smaller standard errors</strong>.</p></li>
<li>The <span class="math">\(\mbox{Multiplier}\)</span> is determined by the desired <strong>confidence level</strong>. The <strong>confidence level</strong> gives the probability that the <em>method</em> of constructing confidence intervals will produce an interval that contains the parameter value. Commonly used <strong>confidence levels</strong> include
<ul>
<li>90%</li>
<li>95% (most common)</li>
<li>98%</li>
<li>99%.</li>
</ul></li>
</ul>
<p>The confidence level aids in the interpretation of a confidence interval. For example, suppose you want to construct a 95% confidence interval for the estimation of a population parameter.</p>
<p>A correct interpretation of the confidence interval is: <strong>If the study were repeated many times, and on each occasion a 95% confidence interval were calculated, then about 95% of these intervals would contain the true parameter value and about 5% would not</strong>.</p>
<p><strong>Warning:</strong> One incorrect statement that is often made about a 95% confidence interval is: “There is a 95% chance (probability) that the confidence interval contains the true parameter value.” The reason that this is an incorrect interpretation is quite subtle. Once a confidence interval is constructed, it either contains the parameter value or it does not. The probability associated with a confidence interval refers only to the <em>method</em> that is used to construct it, not the interval itself.</p>
<p>Let’s put these ideas together. The <em>sample estimate</em> determines the center and the <em>margin of error</em> determines the width of the confidence interval. Since the <em>margin of error</em> is determined by the <em>multiplier</em> and the <em>standard error</em>, we can see the role that <strong>sample size</strong> and <strong>confidence level</strong> play in the width of the confidence interval.</p>
<ul>
<li><p>The larger the sample size, <span class="math">\(n\)</span>, the smaller the SE. So, <strong><em>larger</em> sample sizes produce more <em>narrow</em> confidence intervals</strong>.</p></li>
<li><p>The higher level of confidence we have that an interval contains the true parameter value, the wider the interval is. So, <strong><em>larger</em> confidence levels produce <em>wider</em> confidence intervals</strong>.</p></li>
</ul>
</div>
<div id="chapter-outline" class="section level1">
<h1>Chapter Outline</h1>
<p>The next two chapters will follow the same format. We will go through each of the Big Five parameters, one by one, doing the following:</p>
<ul>
<li><p>State a Research Question that involves the parameter of interest.</p></li>
<li><p>Write out a series of steps - using a “packaged” R-function to do most of the computations that we will need.</p></li>
<li><p>Look “under the hood” of the test to see how the packaged R-function is computing certain values.</p></li>
<li><p>Work a couple of examples involving the same parameter.</p></li>
</ul>
<p>As we work through the confidence interval construction for each of the Big Five parameters, we will continue to push the conceptual understanding of a confidence interval.</p>
</div>
<div id="one-population-mean-mu" class="section level1">
<h1>One Population Mean, <span class="math">\(\mu\)</span></h1>
<p>Let’s start looking at the confidence interval for the example from <code>imagpop</code> we looked at above. We will go through the Four Steps for this first example, using the “packaged” R function to compute the interval for us. Then, we will go through the construction of the confidence interval step by step in detail.</p>
<p>Let’s continue to consider the research question posed at the beginning of this chapter. The <code>imagpop</code> dataset is a nice one to use for purposes of demonstration because it represents an entire population, so we can have values for parameters and statistics.</p>
<blockquote>
<p><strong>Research Question</strong>: What is the average annual income for the folks from the <code>imagpop</code> population?</p>
</blockquote>
<div id="the-four-steps" class="section level2">
<h2>The Four Steps</h2>
<p><strong>Step One:</strong> <em>Definition of Parameter(s)</em></p>
<p>Let <span class="math">\(\mu=\)</span> mean annual income for the <code>imagpop</code> population.</p>
<p>For this example, we will go back to pretending that we are a very powerful being and we know the true value of <span class="math">\(\mu\)</span>. We are going to watch as a statistician, who does not know <span class="math">\(\mu\)</span>, constructs a 95% confidence interval to estimate <span class="math">\(\mu\)</span>.</p>
<p>Our goal with this example is to understand how a confidence interval is constructed and how it produces a range of believable values for the population parameter.</p>
<p>Let’s start by having our statistician draw a simple random sample of size 50 from this population and calculate the sample mean from it, as we did above.</p>
<pre class="r"><code>mysample&lt;-popsamp(50,imagpop)
xbar&lt;-mean(~income,data=mysample)
xbar</code></pre>
<pre><code>## [1] 42030</code></pre>
<p>So, <span class="math">\(\bar{x}=\)</span> $42030 is the sample estimate of the population mean, <span class="math">\(\mu\)</span>.</p>
<p>However, since we are a powerful all-knowing being, we know the population mean.</p>
<pre class="r"><code>mu&lt;-mean(~income,data=imagpop)
mu</code></pre>
<pre><code>## [1] 40317</code></pre>
<p>So the population mean is <span class="math">\(\mu=\)</span> $40317.</p>
<p><em>Important Note</em>: In reality, if you actually knew the population parameter, there would be no point in drawing a sample to estimate it, much less computing a confidence interval for it. For this reason, the rest of the construction of the confidence interval will deal only with the sample. We are using the <code>imagpop</code> dataset for this example in order to compare the population parameter to the confidence interval.</p>
<p><strong>Step Two:</strong> <em>Safety Check and Calculation of the Confidence Interval</em></p>
<p>In Chapter 9, we learned that the sampling distribution of <span class="math">\(\bar{x}\)</span> looks normal if two conditions hold.</p>
<ul>
<li><p>Condition 1: <strong>The population is roughly normal in shape or the sample size, <span class="math">\(n\)</span>, is at least 30.</strong> Since our sample is size 50, it is big enough. (However, if <span class="math">\(n&lt;30\)</span>, it would be necessary to take a look at the distribution of the incomes from our sample. You could do this by plotting a histogram or a boxplot. Check for outliers or extreme skewness.)</p></li>
<li><p>Condition 2: <strong>The sample is like a SRS from the population, at least in regard to the variables of interest.</strong> Since our sample was drawn using the <code>popsamp</code> function, we know that it was a simple random sample. This assures us that the sample is representative of the population at large and probably does not have any underlying bias.</p></li>
</ul>
<p>Since the safety check is passed, let’s go ahead and compute a 95% confidence interval using the R function <code>ttestGC</code>. (The rationale behind this function will be explained when we look <em>under the hood</em>.)</p>
<pre class="r"><code>ttestGC(~income,data=mysample)</code></pre>
<pre><code>## 
## 
## Inferential Procedures for One Mean mu:
## 
## 
## Descriptive Results:
## 
##  variable  mean    sd  n
##    income 42030 28689 50
## 
## 
## Inferential Results:
## 
## Estimate of mu:   42030 
## SE(x.bar):    4057 
## 
## 95% Confidence Interval for mu:
## 
##           lower.bound         upper.bound          
##           33876.542209        50183.457791</code></pre>
<p>So, our 95% confidence interval for the estimation of the population mean, <span class="math">\(\mu\)</span>, is:</p>
<p><span class="math">\[(\$33877,\$50183).\]</span></p>
<p><strong>Step Three:</strong> <em>Interpretation of the Confidence Interval</em></p>
<p>We are 95% confident that if the true population mean were known, the interval ($33877,$50183) would contain it. In other words, this interval gives the most believable values for <span class="math">\(\mu\)</span>.</p>
<p>We are lucky in this case because we are a powerful being and we actually do know the value of <span class="math">\(\mu\)</span>. So, we can check to see if <span class="math">\(\mu\)</span> is, in fact, contained in the interval we computed. The population mean, <span class="math">\(\mu=\)</span> $40317, so it is contained in the 95% confidence interval that was computed in Step 2.</p>
<p>Once the confidence interval is computed, it either contains <span class="math">\(\mu\)</span> or it does not. There is no probability associated with any <em>specific</em> interval. The probability is associated with the <strong>method</strong> that was used to create such an interval (which we will talk about shortly). For example, if we re-computed this 95% confidence interval for many different samples, say 20, we could expect that about 95% of those intervals would contain <span class="math">\(\mu\)</span> and about 5% would not. Let’s do this!</p>
<pre><code>##    lower upper included
## 1  32560 46916      Yes
## 2  27952 42192      Yes
## 3  35364 46812      Yes
## 4  29621 44939      Yes
## 5  31280 48580      Yes
## 6  38426 55778      Yes
## 7  27975 39473       No
## 8  31776 48700      Yes
## 9  35713 51959      Yes
## 10 34833 51315      Yes
## 11 29483 47961      Yes
## 12 31691 45989      Yes
## 13 35735 53581      Yes
## 14 27303 44509      Yes
## 15 30764 44580      Yes
## 16 36233 51815      Yes
## 17 29142 46102      Yes
## 18 39002 54974      Yes
## 19 36136 50456      Yes
## 20 27583 45265      Yes</code></pre>
<p>You should notice that <em>about</em> 95% of the 100 intervals contain <span class="math">\(\mu=\)</span> $40317. In other words, <em>about</em> <span class="math">\(0.95\cdot 20=\)</span> 19 intervals should contain <span class="math">\(\mu\)</span> and <em>about</em> <span class="math">\(0.05\cdot 20=\)</span> 1 does not.</p>
<p>The following app will help you to explore this idea visually:</p>
<pre class="r"><code>require(manipulate)
CIMean(~income,data=imagpop)</code></pre>
<p>Experiment with changing the sample size and confidence level in the app. Take note of what happens to the yellow confidence interval.</p>
<ul>
<li>In general, what happens to the width of the yellow confidence interval as the sample size gets bigger?</li>
<li>In general, what happens to the width of the yellow confidence interval as the confidence level increases?</li>
</ul>
<p><em>Warning</em>: Another common misinterpretation of the 95% confidence interval computed above would be to say that about 95% of the people in the <code>imagpop</code> population make an annual salary between $33877 and $50183. Don’t fall into this trap! The confidence interval only gives us an interval of believable values for the population mean. It does not give us any information about the range of individual’s incomes.</p>
<p><strong>Step Four:</strong> <em>Write a Conclusion.</em></p>
<p>The conclusion should be a non-technical statement about what the confidence interval tells us about the population parameter.</p>
<p>We can be reasonably sure that the average annual income of the folks from the <code>imagpop</code> population is between $33877 and $50183.</p>
</div>
<div id="under-the-hood" class="section level2">
<h2>Under the Hood</h2>
<p>Understanding the construction of the confidence interval explains a couple of things.</p>
<ul>
<li>It explains the 95% confidence level.</li>
<li>It explains why we use a <span class="math">\(t\)</span>-test.</li>
</ul>
<p>Once the safety check is passed in Step Two, we can assume that <span class="math">\(\bar{x}\)</span> approximately follows a normal distribution. This allows us to think of the center and spread of the distribution using the expected value, <span class="math">\(EV(\bar{x})\)</span>, and the standard deviation, <span class="math">\(SD(\bar{x})\)</span>. See Figure[Xbar Distribution].</p>
<ul>
<li><p><span class="math">\(EV(\bar{x})=\mu\)</span></p></li>
<li><p><span class="math">\(SD(\bar{x})=\dfrac{\sigma}{\sqrt{n}}\)</span></p></li>
</ul>
<div class="figure">
<img src="ch9_files/figure-html/distxbar.png" alt="Xbar Distribution: The distribution of the sample means approximately follows a normal distribution centered at the EV(xbar) with spread given by SD(xbar)." /><p class="caption">Xbar Distribution: The distribution of the sample means approximately follows a normal distribution centered at the EV(xbar) with spread given by SD(xbar).</p>
</div>
<p>Since the 68-95 Rule (Empirical) Rule) applies to normal curves, we know that</p>
<ul>
<li><span class="math">\(\approx 68\%\)</span> of the <span class="math">\(\bar{x}\)</span>’s fall within one standard deviation of <span class="math">\(EV(\bar{x})=\mu\)</span>,</li>
<li><span class="math">\(\approx 95\%\)</span> of the <span class="math">\(\bar{x}\)</span>’s fall within two standard deviations of <span class="math">\(EV(\bar{x})=\mu\)</span>, and</li>
<li><span class="math">\(\approx 99.7\%\)</span> of the <span class="math">\(\bar{x}\)</span>’s fall within three standard deviations of <span class="math">\(EV(\bar{x})=\mu\)</span>.</li>
</ul>
<p>Let’s dig deeper into the second part of the 68-95 Rule. It can be visualized below in Figure[68-95 Rule 2].</p>
<div class="figure">
<img src="ch9_files/figure-html/empruletwo.png" alt="68-95 Rule 2: This graph is the visualization of the second part of the 68-95 Rule for the distribution of the sample means." /><p class="caption">68-95 Rule 2: This graph is the visualization of the second part of the 68-95 Rule for the distribution of the sample means.</p>
</div>
<p>In approximately 95% of all samples of size 50, the sample mean, <span class="math">\(\bar{x}\)</span>, will fall within 2 standard deviations of <span class="math">\(\mu\)</span>, the population parameter.</p>
<p>So, for approximately 95% of all samples, <span class="math">\[\mu-2 \cdot SD\leq \bar{x} \leq \mu+2\cdot SD.\]</span></p>
<p>Rearranging this: <span class="math">\[\bar{x}-2\cdot SD\leq \mu \leq \bar{x}+2\cdot SD.\]</span></p>
<p>In other words, approximately 95% of the time, the population parameter, <span class="math">\(\mu\)</span>, is in the interval <span class="math">\((\bar{x}-2\cdot SD,\mbox{ } \bar{x}+2\cdot SD)\)</span>. This sounds like a confidence interval!</p>
<p>We do have one problem, however: a statistician with no knowledge of the population has no way of knowing <span class="math">\(SD(\bar{x})\)</span> because it requires knowing the standard deviation of the population, <span class="math">\(\sigma\)</span>. Hence we must use our best estimate of the standard deviation—the standard error—in its place.</p>
<p>So, an approximate 95% confidence interval for the population mean can be given by <span class="math">\[(\bar{x}-2\cdot SE,\mbox{  } \bar{x}+2\cdot SE).\]</span></p>
<p>Let’s compute this numerically for our example. We will find the needed values by looking at <code>favstats</code> for the sample.</p>
<pre class="r"><code>favstats(~income,data=mysample)</code></pre>
<pre><code>##   min    Q1 median    Q3    max  mean    sd  n missing
##  4300 21025  35600 54450 130800 42030 28689 50       0</code></pre>
<p>We are now equipped with all the information we need to compute the approximate 95% confidence interval for the population mean.</p>
<ul>
<li><p><span class="math">\(\bar{x}=\)</span> 42030</p></li>
<li><p><span class="math">\(s=\)</span> 28689 <em>Note</em> that <span class="math">\(s\)</span> is the standard deviation of the sample.</p></li>
<li><p><span class="math">\(n=\)</span> 50</p></li>
<li><p><span class="math">\(SE=\dfrac{s}{\sqrt{n}}=\dfrac{28689 }{\sqrt{50}}=4057.3041\)</span></p></li>
<li><span class="math">\(\mbox{CI}=(\bar{x}-2\cdot SE,\mbox{  } \bar{x}+2\cdot SE)=\)</span>(42030 <span class="math">\(-2\cdot\)</span> 28689, 42030 <span class="math">\(+2\cdot\)</span> 28689) = (33915, 50145)</li>
<li><p><em>Note:</em> The <span class="math">\(2\cdot SE\)</span> is the <em>margin of error</em>.</p></li>
</ul>
<p>Based on the 68-95 Rule, we can say that we are <em>about</em> 95% confident that the population mean income for the folks in <code>imagpop</code> falls somewhere in the interval ($33915, $50145). This seems to make sense, but this is not the confidence interval that the <code>ttestGC</code> function gave us originally. The interval from <code>ttestGC</code> was ($33877,$50183). We seem to be off by a little bit.</p>
<p>There are a couple of important details related to the <em>multiplier</em> that account for this difference. The second part of the 68-95 Rule prompted us to use the multiplier 2 when we calculated the 95% confidence interval. This is a good approximation of the multiplier, but it’s not the best we can do.</p>
<p>Let’s take a look at a <strong>standard normal curve</strong> to see if we can find a better multiplier. A standard normal curve has <span class="math">\(\mu=0\)</span> and <span class="math">\(\sigma=1\)</span>. The 68-95 Rule suggests that the shaded area of the curve between <span class="math">\(z=-2\)</span> and <span class="math">\(-z=2\)</span> will be approximately 0.95. See Figure[Two SD].</p>
<div class="figure">
<img src="ch9_files/figure-html/twosd.png" alt="Two SD: This graph shows the true area under the curve within two standard deviations of the mean." /><p class="caption">Two SD: This graph shows the true area under the curve within two standard deviations of the mean.</p>
</div>
<p>The shaded region is actually 0.9545. This is because 68-95 Rule is just an approximation. Since we are looking for a 95% confidence interval, we would like our multiplier, or <span class="math">\(z\)</span>-score, to be such that the shaded region is exactly 0.95, not 0.9545.</p>
<p>We can find the appropriate <span class="math">\(z\)</span> and <span class="math">\(-z\)</span> by making use of the quantiles of the normal curve via the <code>qnorm</code> function that we saw in Chapter 7. If we want 0.95 in the middle, then the shaded area to the left of <span class="math">\(z\)</span> is 0.975 - the 0.95 for the middle and the 0.025 for the left tail.</p>
<pre class="r"><code>qnorm(0.975,mean=0,sd=1)</code></pre>
<pre><code>## [1] 1.96</code></pre>
<p>Likewise, the shaded area to the left of <span class="math">\(-z\)</span> is 0.025.</p>
<pre class="r"><code>qnorm(0.025,mean=0,sd=1)</code></pre>
<pre><code>## [1] -1.96</code></pre>
<p>Let’s check that we have the correct values by looking at the shaded region between these two <span class="math">\(z\)</span>-scores. The graph appears in Figure [Better 95% Multiplier].</p>
<pre class="r"><code>pnormGC(c(-1.96,1.96),region=&quot;between&quot;
        ,mean=0,sd=1,graph=TRUE)</code></pre>
<div class="figure">
<img src="ch9_files/figure-html/better95mult.png" alt="Better 95% Multiplier. 1.96 works out a bit better than 2." /><p class="caption">Better 95% Multiplier. 1.96 works out a bit better than 2.</p>
</div>
<p>That’s better! Let’s adjust our formula for our 95% confidence interval.</p>
<p><span class="math">\[(\bar{x}-1.96 \cdot SE, \mbox{ } \bar{x}+ 1.96\cdot SE)\]</span></p>
<p>We can now re-compute our 95% confidence interval using this new formula: <span class="math">\(\mbox{CI}=(\bar{x}-1.96\cdot SE,\mbox{  } \bar{x}+1.96\cdot SE)=\)</span> (34078, 49982).</p>
<p>Summarizing what we’ve done so far:</p>
<ul>
<li><p>Using R’s <code>t.test</code> function, <span class="math">\(CI=\)</span> ($33877, $50183).</p></li>
<li><p>Using the multiplier 2 from the 68-95 Rule, <span class="math">\(CI=\)</span> ($33915, $50145)</p></li>
<li><p>Now, using the multiplier <span class="math">\(z=1.96\)</span>, <span class="math">\(CI=\)</span> ($34078, $49982)</p></li>
</ul>
<p>We still don’t have the multiplier quite right. Our problem stems from us, as statisticians, not knowing the population standard deviation, <span class="math">\(\sigma\)</span>. Not knowing <span class="math">\(\sigma\)</span> means that we don’t know <span class="math">\(SD(\bar{x})\)</span>. <span class="math">\(SD(\bar{x})\)</span> has crept up in a couple of different places. First, <span class="math">\(SD(\bar{x})\)</span> was replaced with <span class="math">\(SE(\bar{x})\)</span> in the formula for the confidence interval. Also, <span class="math">\(SD(\bar{x})\)</span> is the denominator in the formula for <span class="math">\(z\)</span>-score, which is what we are using as our multiplier. Using <span class="math">\(SE(\bar{x})\)</span> as an approximation for <span class="math">\(SD(\bar{x})\)</span> in both of these situations is okay if we are dealing with large sample sizes. However, for smaller sample sizes, this approximation is often off the mark.</p>
<p>Recall that the formula for the <span class="math">\(z\)</span>-score is</p>
<p><span class="math">\[z=\dfrac{\bar{x}-\mu}{SD(\bar{x})}=\dfrac{\bar{x}-\mu}{\frac{\sigma}{\sqrt{n}}}.\]</span></p>
<p>When we replace the denominator in the formula for the <span class="math">\(z\)</span>-score with <span class="math">\(SE(\bar{x})\)</span>, we call it a <span class="math">\(t\)</span>-statistic. The formula for the <span class="math">\(t\)</span>-statistic is</p>
<p><span class="math">\[t=\dfrac{\bar{x}-\mu}{SE(\bar{x})}=\dfrac{\bar{x}-\mu}{\frac{s}{\sqrt{n}}}.\]</span></p>
<p>Just as the <span class="math">\(z\)</span>-score told us how many standard deviations a value was from the mean, the <span class="math">\(t\)</span>-statistic tells us how many <strong>standard errors</strong> a value is from the mean. Also, just like the <span class="math">\(z\)</span>-score is associated with the normal distribution, the <span class="math">\(t\)</span>-statistic is associated with the <span class="math">\(t\)</span>-distribution. The <span class="math">\(t\)</span>-distribution is a distribution that we have not discussed yet, but it is very similar to the normal distribution.</p>
<ul>
<li><p>The <span class="math">\(t\)</span>-distribution, like the normal distribution, is symmetric and bell-shaped.</p></li>
<li><p>The <span class="math">\(t\)</span>-distribution looks slightly different for different sample sizes. For this reason, the <span class="math">\(t\)</span>-distribution depends on something we call <em>degrees of freedom</em> <span class="math">\(= df= n-1\)</span>.</p></li>
<li><p>The larger the sample size, <span class="math">\(n\)</span>, the more the <span class="math">\(t\)</span>-distribution looks like the normal distribution.</p></li>
</ul>
<p>You can investigate the shape of the <span class="math">\(t\)</span>-distribution and compare it to the normal curve using the following app.</p>
<pre class="r"><code>require(manipulate)
tExplore()</code></pre>
<p>You may have noticed that when the sample size is small, the <span class="math">\(t\)</span>-distribution is more spread out - it carries more weight in its tails. This is a result of <span class="math">\(SE(\bar{x})\)</span> doing a poor job approximating <span class="math">\(SD(\bar{x})\)</span> for small sample sizes. Since <span class="math">\(t\)</span> actually relies on the standard error, it is better to use a <span class="math">\(t\)</span>-multiplier instead of a <span class="math">\(z\)</span>-multiplier for small sample sizes.</p>
<p>For large samples, it would be okay to use the <span class="math">\(z\)</span>-multiplier in the construction of confidence intervals. However, as <span class="math">\(n\)</span> increases, the <span class="math">\(t\)</span>-distribution starts looking more and more like the normal curve. (This is because <span class="math">\(SE(\bar{x})\)</span> is a better approximation of <span class="math">\(SD(\bar{x})\)</span> for large <span class="math">\(n\)</span>.) Since the <span class="math">\(t\)</span>-statistic is a good approximation for the <span class="math">\(z\)</span>-score when the sample size is large, we will remain consistent and <strong>always</strong> use the <span class="math">\(t\)</span>-distribution to calculate confidence intervals for population means, regardless of the sample size. If you are interested in learning more about the distinction between <span class="math">\(t\)</span> and <span class="math">\(z\)</span>, consult the GeekNotes.</p>
<p>We can find the <span class="math">\(t\)</span>-multiplier just as we found <span class="math">\(z=1.96\)</span> before, except we will be dealing with the <span class="math">\(t\)</span>-distribution instead of the normal distribution. For our example, the <span class="math">\(df=n-1=50-1=49\)</span>.</p>
<pre class="r"><code>qt(0.975, df=49)</code></pre>
<pre><code>## [1] 2.01</code></pre>
<pre class="r"><code>qt(0.025,df=49)</code></pre>
<pre><code>## [1] -2.01</code></pre>
<p>Similar to the function <code>pnormGC</code>, the function <code>ptGC</code> allows us to look at the shaded region between these two <span class="math">\(t\)</span>-statistics:</p>
<pre class="r"><code>ptGC(c(-2.01,2.01),region=&quot;between&quot;,df=49,graph=TRUE)</code></pre>
<p>The graph appears in Figure [t-Multiplier].</p>
<div class="figure">
<img src="ch9_files/figure-html/tmultiplier.png" alt="t-Multiplier. The multiplier for sample size 50 is about 2.01." /><p class="caption">t-Multiplier. The multiplier for sample size 50 is about 2.01.</p>
</div>
<p>Since <span class="math">\(t=2.01\)</span> is the correct multiplier, let’s recompute the confidence interval with it and compare to our previous calculations: <span class="math">\(\mbox{CI}=(\bar{x}-2.01\cdot SE,\mbox{  } \bar{x}+2.01\cdot SE)=\)</span> (33877, 50183)</p>
<ul>
<li><p>Using R’s <code>ttestGC</code> function, <span class="math">\(CI=\)</span> ($33877, $50183).</p></li>
<li><p>Using the multiplier 2 from the 68-95 Rule, <span class="math">\(CI=\)</span> ($33915, $50145)</p></li>
<li><p>Using the multiplier <span class="math">\(z=1.96\)</span>, <span class="math">\(CI=\)</span> ($34078, $49982)</p></li>
<li><p>Now, using the multiplier <span class="math">\(t=2.01\)</span>, <span class="math">\(CI=\)</span> ($33877, $50183)</p></li>
</ul>
<p><strong>Summary</strong>: In general, the formula to compute a 95% confidence interval to estimate a population mean is <span class="math">\[\mbox{CI}=(\bar{x}-t \cdot SE,\mbox{  } \bar{x}+t \cdot SE),\]</span> where <span class="math">\(t\)</span> is calculated from the <span class="math">\(t\)</span>-distribution based on the appropriate degress of freedom. Keep in mind that the packaged function, <code>ttestGC</code>, in R does all of this work for you.</p>
<p>Since the construction of confidence intervals for means depends on the <span class="math">\(t\)</span>-distribution, <em>Condition 1</em> of the safety check should really be a condition that verifies that the sampling distribution of the <span class="math">\(t\)</span>-statistic approximately follows a <span class="math">\(t\)</span>-distribution. Above (and in Chapter 7) we said that <span class="math">\(n\geq 30\)</span> ensured us that <span class="math">\(\bar{x}\)</span> was approximately normal. It turns out that <span class="math">\(n\geq 30\)</span> also ensures us that the <span class="math">\(t\)</span>-statistic approximately follow a <span class="math">\(t\)</span>-curve. (So what we actually check in <em>condition 1</em> is the same.) You can investigate this idea with the following app.</p>
<pre class="r"><code>require(manipulate)
tSampler(~income,data=imagpop)</code></pre>
<p>For small sample sizes, you can see how the distribution of the t-statistic differs substantially from the <span class="math">\(t\)</span>-distribution. However, for sample sizes around 30 you can’t really tell the difference. This is why we require that <span class="math">\(n\geq 30\)</span>.</p>
</div>
<div id="additional-example-and-further-ideas" class="section level2">
<h2>Additional Example, and Further Ideas</h2>
<blockquote>
<p><strong>Research Question</strong>: What is the average height of <strong>all</strong> GC students?</p>
</blockquote>
<p><strong>Step One:</strong> <em>Definition of parameter.</em></p>
<p>Let <span class="math">\(\mu=\)</span> mean height of GC student population.</p>
<p><strong>Step Two:</strong> <em>Safety Check and Calculation of the Confidence Interval</em></p>
<p>For this problem, we are using the observations in our <code>m111survey</code> dataset as our sample. The population is <strong>all</strong> GC students.</p>
<p>Let’s check the two conditions of the safety check.</p>
<ul>
<li>Condition 1: <strong>The population is roughly normal in shape or the sample size, <span class="math">\(n\)</span>, is at least 30.</strong> To check this, take a look at <code>favstats</code>.</li>
</ul>
<pre><code>##  min Q1 median    Q3 max  mean    sd  n missing
##   51 65     68 71.75  79 67.99 5.296 71       0</code></pre>
<p>There are 71 people in the survey data, so our sample size <span class="math">\(n=71\)</span> is large enough that we don’t need to investigate the shape of the sample data.</p>
<ul>
<li>Condition 2: <strong>The sample is like a SRS from the population, at least in regard to the variables of interest.</strong></li>
</ul>
<p>Our sample of students in this survey consists of all students enrolled in MAT 111 in a particular semester. You might question if this constitutes a SRS of all GC students. Students certainly decide to enroll in MAT 111 for specific reasons such as it being a requirement for their major. However, their decision probably has nothing to do with their height. In other words, one’s height and the decision to enroll in MAT 111 are unrelated. So, in regards to the variable <code>height</code>, the students in the survey constitute a sample that is representative of the population (at least as much so as any simple random sample would).</p>
<p>Since the safety check is passed, let’s compute a 95% confidence interval.</p>
<pre><code>## 
## 
## Inferential Procedures for One Mean mu:
## 
## 
## Descriptive Results:
## 
##  variable  mean    sd  n
##    height 67.99 5.296 71
## 
## 
## Inferential Results:
## 
## Estimate of mu:   67.99 
## SE(x.bar):    0.6286 
## 
## 95% Confidence Interval for mu:
## 
##           lower.bound         upper.bound          
##           66.732979           69.240260</code></pre>
<p>Our 95% confidence interval for the estimation of the population mean, <span class="math">\(\mu\)</span>, is (66.73,69.24) inches.</p>
<p><strong>Step Three:</strong> <em>Interpretation of the Confidence Interval</em></p>
<p>We are 95% confident that if the average height of the GC student population were known, the interval (66.73,69.24) would contain it. Put another way, (66.73,69.24) gives the most believable values for the average height of the GC student population.</p>
<p><strong>Step Four:</strong> <em>Write a Conclusion.</em></p>
<p>We can be reasonably sure that the average height of the GC student population is between 66.73 inches and 69.24 inches.</p>
<p><strong>Additional Note:</strong> A confidence interval with any other confidence level can be calculated easily by just adding an extra argument, <code>conf.level</code>, into the <code>ttestGC</code> function. We could calculate a 99% confidence interval as follows.</p>
<pre class="r"><code>ttestGC(~height,data=m111survey,conf.level=0.99)</code></pre>
<pre><code>## 
## 
## Inferential Procedures for One Mean mu:
## 
## 
## Descriptive Results:
## 
##  variable  mean    sd  n
##    height 67.99 5.296 71
## 
## 
## Inferential Results:
## 
## Estimate of mu:   67.99 
## SE(x.bar):    0.6286 
## 
## 99% Confidence Interval for mu:
## 
##           lower.bound         upper.bound          
##           66.322230           69.651010</code></pre>
<p>We are 99% confident that if the average height of the GC student population were known, the interval (66.32,69.65) would contain it.</p>
<p>Check out a 68% confidence interval:</p>
<pre class="r"><code>ttestGC(~height,data=m111survey,conf.level=0.68)</code></pre>
<pre><code>## 
## 
## Inferential Procedures for One Mean mu:
## 
## 
## Descriptive Results:
## 
##  variable  mean    sd  n
##    height 67.99 5.296 71
## 
## 
## Inferential Results:
## 
## Estimate of mu:   67.99 
## SE(x.bar):    0.6286 
## 
## 68% Confidence Interval for mu:
## 
##           lower.bound         upper.bound          
##           67.357063           68.616177</code></pre>
<p>Let’s compare the widths of these three confidence intervals - 68%, 95%, and 99%. For the mean height of the GC student population,</p>
<ul>
<li><p>(67.36,68.62) is a 68% CI and has width 1.2591.</p></li>
<li><p>(66.73,69.24) is a 95% CI and has width 2.5073.</p></li>
<li><p>(66.32,69.65) is a 99% CI and has width 3.3288.</p></li>
</ul>
<p>You can see that higher confidence levels produce wider intervals. Wider intervals cover a broader range of numbers so you’re more confident that the population parameter is in that interval. There are two things that affect the width of a confidence interval:</p>
<ul>
<li>multiplier</li>
<li>sample size</li>
</ul>
<p>In this case, the sample size is the same, so the multiplier is responsible for the width of the interval. Recall that the multiplier is the number from the <span class="math">\(t\)</span> distribution that captures a specified percentage of the area between the multiplier and the multiplier’s negative. As we move more standard errors away from the center, the value of <span class="math">\(t\)</span> increases. We also capture more of the area under the curve, which corresponds to a higher confidence level. Thus, when the sample size is held constant, higher confidence levels produce wider confidence intervals. This can be visualized in the three <span class="math">\(t\)</span>-distributions with <span class="math">\(df=70\)</span> shown below. See Figure[68% t-Distribution], Figure[95% t-Distribution], and Figure[99% t-Distribution].</p>
<div class="figure">
<img src="ch9_files/figure-html/t68.png" alt="68% t-Distribution: Visualization of the t-multiplier used in the construction of a 68% confidence interval for one population mean." /><p class="caption">68% t-Distribution: Visualization of the t-multiplier used in the construction of a 68% confidence interval for one population mean.</p>
</div>
<div class="figure">
<img src="ch9_files/figure-html/t95.png" alt="95% t-Distribution: Visualization of the t-multiplier used in the construction of a 95% confidence interval for one population mean." /><p class="caption">95% t-Distribution: Visualization of the t-multiplier used in the construction of a 95% confidence interval for one population mean.</p>
</div>
<div class="figure">
<img src="ch9_files/figure-html/t99.png" alt="99% t-Distribution: Visualization of the t-multiplier used in the construction of a 99% confidence interval for one population mean." /><p class="caption">99% t-Distribution: Visualization of the t-multiplier used in the construction of a 99% confidence interval for one population mean.</p>
</div>
</div>
</div>
<div id="difference-of-two-population-means-mu_1-mu_2" class="section level1">
<h1>Difference of Two Population Means, <span class="math">\(\mu_1-\mu_2\)</span></h1>
<blockquote>
<p><strong>Research Question</strong>: Do GC males sleep more at night, on average, than GC females?</p>
</blockquote>
<div id="the-four-steps-1" class="section level2">
<h2>The Four Steps</h2>
<p><strong>Step One:</strong> <em>Definition of Parameter(s)</em></p>
<p>For this problem, we are dealing with two populations - all GC males and all GC females - for which we don’t know the mean hours of sleep per night of either one.</p>
<blockquote>
<p>Let <span class="math">\(\mu_1=\)</span> the mean hours of sleep per night for all GC females.</p>
</blockquote>
<blockquote>
<p>Let <span class="math">\(\mu_2=\)</span> the mean hours of sleep per night for all GC males.</p>
</blockquote>
<p>We sre interested in the difference, <span class="math">\(\mu_1-\mu_2\)</span>.</p>
<p><strong>Step Two:</strong> <em>Safety Check and Calculation of the Confidence Interval</em></p>
<p>For this parameter, we also have two conditions for our safety check. However, the conditions are slightly different.</p>
<ul>
<li>Condition 1: <strong>Both populations are roughly normal in shape or the sample sizes, <span class="math">\(n_1\)</span> and <span class="math">\(n_2\)</span>, are both at least 30.</strong> To check this condition, let’s take a look at <code>favstats</code>.</li>
</ul>
<pre><code>##   .group min Q1 median    Q3 max  mean    sd  n missing
## 1 female   2  5   6.75 7.125   9 6.325 1.619 40       0
## 2   male   4  5   7.00 7.000  10 6.484 1.557 31       0</code></pre>
<p>The sample size for females is <span class="math">\(n_1=\)</span> 1.6194 and the sample size for males is <span class="math">\(n_2=\)</span> 1.5572, so both samples are large enough.</p>
<p>Since our sample of males was barely ‘large enough’, perhaps it’s a good idea to take a look at the distribution of the samples just to make sure we aren’t dealing with outliers or extreme skewness. See Figure[Sleep Histograms].</p>
<div class="figure">
<img src="ch9_files/figure-html/histsleepbysex.png" alt="Sleep Histograms: The histogram on the left shows the hours of sleep that females in the sample get. The histogram on the right shows the hours of sleep that males in the sample get." /><p class="caption">Sleep Histograms: The histogram on the left shows the hours of sleep that females in the sample get. The histogram on the right shows the hours of sleep that males in the sample get.</p>
</div>
<p>There do not seem to be any outliers and neither histogram looks extremely skewed. Couple that with the fact that both sample sizes were, in fact, large enough, this part of the safety check is okay.</p>
<ul>
<li><p>Condition 2: One of two things is true:</p>
<ul>
<li><p><strong>We did a completely randomized experiment and the two samples are the two treatment groups in the experiment.</strong></p></li>
<li><p><strong>We took two independent random samples from two populations. This means that the samples have nothing to do with one another.</strong></p></li>
</ul></li>
</ul>
<p>Since this research question is a result of an observational study (the explanatory variable was simply observed, not assigned), then we need to verify the second part of this condition. Since the hours of sleep per night that an individual gets is not related to their being enrolled in MAT 111, we can say that with regard to the variable <code>sleep</code>, the sample is representative of the population of GC students. We could extend this statement to say that the sample of males and the sample of females are like simple random samples from their respective populations, at least in terms of the variable <code>sleep</code>.</p>
<p>We will now compute the confidence interval. We could choose to compute a confidence interval for any desired confidence level, but let’s just stick to the typical 95%.</p>
<p>We will use the same <code>ttestGC</code> as we did for one population mean. The argument <code>first</code> should indicate which group is being considered for the first mean. In our assignment of the parameter, we let <span class="math">\(\mu_1\)</span> represent the means of the female, so we need to set <code>first=&quot;female&quot;</code> in the function.</p>
<pre class="r"><code>ttestGC(sleep~sex,data=m111survey,first=&quot;female&quot;)</code></pre>
<pre><code>## 
## 
## Inferential Procedures for the Difference of Two Means mu1-mu2:
##  (Welch&#39;s Approximation Used for Degrees of Freedom)
##   sleep grouped by sex 
## 
## 
## Descriptive Results:
## 
##   group  mean    sd  n
##  female 6.325 1.619 40
##    male 6.484 1.557 31
## 
## 
## Inferential Results:
## 
## Estimate of mu1-mu2:  -0.1589 
## SE(x1.bar - x2.bar):  0.3792 
## 
## 95% Confidence Interval for mu1-mu2:
## 
##           lower.bound         upper.bound          
##           -0.915971           0.598229</code></pre>
<p>Our 95% confidence interval for the estimation of the population difference in the average number of hours a GC female sleeps and the average number of hours a GC male sleeps at night, <span class="math">\(\mu_1-\mu_2\)</span>, is (-0.916,0.5982) hours.</p>
<p><strong>Step Three:</strong> <em>Interpretation of the Confidence Interval</em></p>
<p>We are 95% confident that if the average difference in hours of sleep per night between GC females and GC males were known, the interval (-0.916,0.5982) would contain it. Put another way, (-0.916,0.5982) gives the most believable values for this difference.</p>
<p><strong>Step Four:</strong> <em>Write a Conclusion.</em></p>
<p>We are reasonably sure that the difference in the average hours of sleep per night of GC females and GC males is between -0.916 and 0.5982.</p>
<p>The original Research Question asked whether GC males sleep more, on average, than GC females do. If this were so, then <span class="math">\(\mu_1-\mu_2\)</span> would be negative. Observe, however, that the confidence interval for <span class="math">\(\mu_1-\mu_2\)</span> contains zero and some positive values, as well as negative ones: hence we cannot rule out the view that females sleep as much as males do (<span class="math">\(\mu_1-\mu_2=0\)</span>) or even more than males do (<span class="math">\(\mu_1-\mu_2 &gt; 0\)</span>).</p>
</div>
<div id="under-the-hood-1" class="section level2">
<h2>Under the Hood</h2>
<div id="calculation-of-ci" class="section level3">
<h3>Calculation of CI</h3>
<p>Let’s check out the actual calculation of this interval. The formula follows the same ideas as what we discussed for one population mean.</p>
<p><span class="math">\[\bigg((\bar{x_1}-\bar{x_2}) \mbox{ }\pm \mbox{ }t\cdot SE(\bar{x_1}-\bar{x_2})\bigg)\]</span></p>
<p>where <span class="math">\(SE(\bar{x_1}-\bar{x_2})=\sqrt{\dfrac{s_1^2}{n_1}+\dfrac{s_2^2}{n_2}}\)</span>.</p>
<p>All of these values, except for <span class="math">\(t\)</span> can be found from <code>favstats</code>:</p>
<pre><code>##   .group min Q1 median    Q3 max  mean    sd  n missing
## 1 female   2  5   6.75 7.125   9 6.325 1.619 40       0
## 2   male   4  5   7.00 7.000  10 6.484 1.557 31       0</code></pre>
<ul>
<li><span class="math">\(\bar{x_1}=\)</span> sample mean hours of sleep per night for females = 9</li>
<li><span class="math">\(\bar{x_2}=\)</span> sample mean hours of sleep per night for males = 10</li>
<li><span class="math">\(s_1=\)</span> sample standard deviation for females = 6.325</li>
<li><span class="math">\(s_2=\)</span> sample standard deviation for males = 6.4839</li>
<li><span class="math">\(n_1=\)</span> sample size of females = 1.6194</li>
<li><span class="math">\(n_2=\)</span> sample size for males = 1.5572</li>
</ul>
<p>To find the multiplier <span class="math">\(t\)</span>, recall that the <span class="math">\(t\)</span>-distribution relies on <span class="math">\(df=n-1\)</span>. In this case, we have two sample sizes, <span class="math">\(n_1\)</span> and <span class="math">\(n_2\)</span>. There are various methods for finding <span class="math">\(df\)</span> for this case, but we won’t go into detail here. (If you want to learn more, consult GeekNotes.) To construct this interval by hand, we can look to see what <code>t.test</code> used for <span class="math">\(df\)</span> and perform the calculation from there. From the test results above, <span class="math">\(df=\)</span> 65.81. So, <span class="math">\(t\)</span> can be found as we did before.</p>
<pre class="r"><code>qt(0.975,df=65.81)</code></pre>
<pre><code>## [1] 1.997</code></pre>
<p>Our 95% confidence interval for the difference of means can be computed as follows:</p>
<p><span class="math">\[\bigg((\bar{x_1}-\bar{x_2}) \mbox{ }\pm \mbox{ }t\cdot SE(\bar{x_1}-\bar{x_2})\bigg),\]</span></p>
<p>which becomes</p>
<p><span class="math">\[\bigg((9-10) \pm 1.997 \cdot \sqrt{\dfrac{6.325^2}{1.6194}+\dfrac{6.4839}{1.5572}}  \bigg),\]</span></p>
<p>which in turn reduces to</p>
<p><span class="math">\[\bigg( -15.3593, 13.3593 \bigg).\]</span></p>
</div>
<div id="order-of-groups" class="section level3">
<h3>Order of Groups</h3>
<p>Our original interpretation stated that (-0.916,0.5982) gives the most believable values for the difference between the average amount of sleep a GC female and a GC male get per night. We can actually say more than that.</p>
<p>Notice how the interval covers 0. In other words, zero is a plausible value for the population parameter. If the true mean difference in hours of sleep between females and males were 0, this would tell us that GC females and males get the same amount of sleep per night, on average.</p>
<p>Negative numbers are also included in the interval. If <span class="math">\(\mu_1-\mu_2\)</span> (females - males) were a negative value, this would mean that males get more sleep on average than females.</p>
<p>Positive numbers are included in our confidence interval, as well. If <span class="math">\(\mu_1-\mu_2\)</span> were a positive number, this would mean that females get more sleep on average than males.</p>
<p>Since our confidence interval contains the most believable values for <span class="math">\(\mu_1-\mu_2\)</span>, all of these values are reasonable. In other words, because our confidence interval spans 0, it does not give us a good idea which sex actually gets more sleep on average. When you are examining the difference of means, it is important to know which mean you are treating as <span class="math">\(\mu_1\)</span> and which you are treating as <span class="math">\(\mu_2\)</span>, so that you can correctly interpret positive and negative numbers.</p>
</div>
</div>
<div id="additional-example" class="section level2">
<h2>Additional Example</h2>
<p>Let’s consider a research question that depends on summary data rather than a built-in dataset in R.</p>
<blockquote>
<p><strong>Research Question</strong>: For a randomly selected sample of 36 men from a certain population, the mean head circumference is 57.5 cm with a standard deviation equal to 2.4 cm. For a randomly selected sample of 36 women from the population, the mean head circumference is 55.3 cm with a standard deviation equal to 1.8 cm. What is the average difference in head circumference between men and women for this population?</p>
</blockquote>
<p><strong>Step One</strong>: <em>Definition of Parameter(s)</em></p>
<p>Since we are dealing with two populations - men and women - the parameter of interest is the difference of means.</p>
<blockquote>
<p>Let <span class="math">\(\mu_1=\)</span> the mean head circumference of the females in the population.</p>
</blockquote>
<blockquote>
<p>Let <span class="math">\(\mu_2=\)</span> the mean head circumference of the males in the population.</p>
</blockquote>
<p><strong>Step Two</strong>: <em>Safety Check and Calculation of the Confidence Interval</em></p>
<ul>
<li>Condition 1: <strong>Both populations are roughly normal in shape or the sample sizes, <span class="math">\(n_1\)</span> and <span class="math">\(n_2\)</span>, are both at least 30.</strong></li>
</ul>
<p>Since we are dealing with summary data for this example, we are not able to plot histograms or look at <code>favstats</code>. We will have to rely only on the information given in the problem. Since we are told that both samples are at least of size 30, this is good enough to satisfy this condition.</p>
<ul>
<li>Condition 2: <strong>One of two things is true</strong>:
<ul>
<li>We did a completely randomized experiment and the two samples are the two treatment groups in the experiment.<br /></li>
<li>We took two independent random samples from two populations. This means that the samples have nothing to do with one another.</li>
</ul></li>
</ul>
<p>Again, in this example we are dealing with an observational study, not an experiment. We can also assume that since the samples were drawn randomly from a population of men and a population of women, they have nothing to do with one another. This condition is satisfied as well.</p>
<p>Let’s compute a 95% confidence interval. We will have to input our summary data. Be sure to input the summary consistent with the order that the parameters were assigned in Step 1.</p>
<pre class="r"><code>ttestGC(mean=c(55.3,57.5),sd=c(1.8,2.4),n=c(36,36))</code></pre>
<pre><code>## 
## 
## Inferential Procedures for the Difference of Two Means mu1-mu2:
##  (Welch&#39;s Approximation Used for Degrees of Freedom)
##  Results from summary data.
## 
## 
## Descriptive Results:
## 
##    group mean  sd  n
##  Group 1 55.3 1.8 36
##  Group 2 57.5 2.4 36
## 
## 
## Inferential Results:
## 
## Estimate of mu1-mu2:  -2.2 
## SE(x1.bar - x2.bar):  0.5 
## 
## 95% Confidence Interval for mu1-mu2:
## 
##           lower.bound         upper.bound          
##           -3.198595           -1.201405</code></pre>
<p>The 95% confidence interval for the population mean difference in head circumference (females - males) is (-3.1986, -1.2014) centimeters.</p>
<p><strong>Step Three</strong>: <em>Interpretation of the Confidence Interval</em></p>
<p>We are 95% confident that if the population mean difference in head circumference (females - males) were known, it would lie in the interval (-3.1986, -1.2014).</p>
<p>Since all of the numbers in this interval are negative, we can say with 95% confidence that the average head circumference for males in this population is larger than the average head circumference for females. We could go even further to say that males are likely to have at least a 1.2 cm larger head circumference, on average, than females.</p>
<p><strong>Step Four</strong>: <em>Write a Conclusion</em></p>
<p>We are reasonably sure that the average head circumference for males in this population is larger than the average head circumference for females. The difference between the average head circumferences for females and males is likely to be a number in the range (-3.1986, -1.2014) centimeters.</p>
</div>
</div>
<div id="mean-of-differences-mu_d" class="section level1">
<h1>Mean of Differences, <span class="math">\(\mu_d\)</span></h1>
<blockquote>
<p><strong>Research Question</strong>: Does the ideal height of GC students differ, on average, than their actual height?</p>
</blockquote>
<p>We will use the <code>m111survey</code> dataset to answer this research question.</p>
<div id="the-four-steps-2" class="section level2">
<h2>The Four Steps</h2>
<p><strong>Step One:</strong> <em>Definition of Parameter(s)</em></p>
<p>This was a <em>repeated-design</em> study - we are comparing the answers that each individual gave to two questions. You cannot think of this as two populations—actual height and ideal height—because these two populations are not independent. The actual height and ideal height are coming from the same person! Thus, this is not a difference of means, but it is a <strong>mean of differences</strong>.</p>
<p>Let <span class="math">\(\mu_d=\)</span> the mean of the difference between the ideal height and actual height of all students in the GC population.</p>
<p><strong>Step Two:</strong> <em>Safety Check and Calculation of the Confidence Interval</em></p>
<p>Since we are again dealing with one mean from one population, the safety check will be the same as it was for the <em>one population mean</em> case that we studied above.</p>
<ul>
<li>Condition 1: <strong>The population is roughly normal in shape or the sample size, <span class="math">\(n\)</span>, is at least 30.</strong></li>
</ul>
<p>Let’s look again at <code>favstats</code> for this sample.</p>
<pre><code>##  min Q1 median Q3 max  mean    sd  n missing
##   -4  0      2  3  18 1.946 3.206 69       2</code></pre>
<p>The sample size for this variable is 69 which is larger than 30, so it’s big enough.</p>
<ul>
<li>Condition 2: <strong>The sample is like a simple random sample from the population, at least in regard to the variables of interest.</strong></li>
</ul>
<p>As we’ve said before, the variable <code>height</code> is unrelated to an individual’s decision to enroll in MAT 111. For this reason, our sample from MAT 111 students can be regarded as a simple random sample of all GC students.</p>
<p>Since the safety check is passed, let’s go ahead and compute a 95% confidence interval.</p>
<pre class="r"><code>ttestGC(~ideal_ht-height,data=m111survey) </code></pre>
<pre><code>## 
## 
## Inferential Procedures for the Difference of Means mu-d:
##   ideal_ht minus height 
## 
## 
## Descriptive Results:
## 
##         Difference mean.difference sd.difference  n
##  ideal_ht - height           1.946         3.206 69
## 
## 
## Inferential Results:
## 
## Estimate of mu-d:     1.946 
## SE(d.bar):    0.3859 
## 
## 95% Confidence Interval for mu-d:
## 
##           lower.bound         upper.bound          
##           1.175528            2.715776</code></pre>
<p>Our 95% confidence interval for the estimation of the GC mean difference between ideal and actual heights, <span class="math">\(\mu_d\)</span>, is (1.176,2.716) inches.</p>
<p><strong>Step Three:</strong> <em>Interpretation of the Confidence Interval</em></p>
<p>We are 95% confident that if the true population mean difference in ideal and actual heights were known, the interval (1.176,2.716) would contain it. In other words, this interval gives the most believable values for <span class="math">\(\mu_d\)</span>.</p>
<p>Since this entire interval lies above 0, the reasonable values for <span class="math">\(\mu_d\)</span> are all positive. Since we took <code>ideal height</code> - <code>actual height</code>, this indicates that an individual’s ideal height is likely to be greater than their actual height.</p>
<p><strong>Step Four:</strong> <em>Write a Conclusion.</em></p>
<p>In the GC population, it is likely that the mean difference between a student’s ideal height and their actual height is between 1.176 inches and 2.716 inches. Since the interval lies entirely above the number 0, we are pretty sure that GC students wish that they were taller than they are, on average.</p>
</div>
<div id="under-the-hood-2" class="section level2">
<h2>Under the Hood</h2>
<p>The calculation of a confidence interval for the <strong>mean of differences</strong> works exactly like it did for <strong>one population mean</strong>.</p>
</div>
<div id="additional-example-1" class="section level2">
<h2>Additional Example</h2>
<p>Let’s do an example with summary data. A simple random sample of 30 college women was taken. Each woman was asked to provide her own height, in inches, and her mother’s height, in inches. The difference in heights was computed for each woman. Check out favstats for the data.</p>
<pre><code>##    min     Q1 median    Q3  max  mean    sd  n missing
##  -1.99 -0.105   1.61 3.885 7.56 1.853 2.508 30       0</code></pre>
<blockquote>
<p><strong>Research Question</strong>: Are college women taller, on average, than their mothers?</p>
</blockquote>
<p><strong>Step One</strong> <em>Define the Parameter(s)</em></p>
<p>It might seem that you are dealing with two populations in this case - mothers’ heights and daughters’ heights. However, these two groups are not independent. An individual’s height is very much dependent on the heights of their parents.</p>
<p>For this situation, we are dealing with <em>matched pairs</em>, so we are interested in a <strong>mean of differences</strong>. Each mother and daughter pair constitute one matched pair. T</p>
<p>Let <span class="math">\(\mu_d=\)</span> the mean difference in heights of college women and their mothers.</p>
<p><strong>Step Two:</strong> <em>Safety Check and Construction of the Confidence Interval</em></p>
<ul>
<li>Condition 1: <strong>The population is roughly normal in shape or the sample size, <span class="math">\(n\)</span>, is at least 30.</strong></li>
</ul>
<p>Since we have 30 mother/daughter pairs, our sample size is <span class="math">\(n=30\)</span>. Since this is right on the borderline of ‘big enough’, let’s take a look at the distribution of these differences. We want to be aware of extreme skewness and/or the presence of outliers. Let’s look at this with a boxplot. See Figure[Boxplot of Differences].</p>
<div class="figure">
<img src="ch9_files/figure-html/boxplotheights.png" alt="Boxplot of Differences: The distribution of the differences of college women’s heights and their mother’s heights." /><p class="caption">Boxplot of Differences: The distribution of the differences of college women’s heights and their mother’s heights.</p>
</div>
<p>The distribution looks fairly symmetric without any outliers.</p>
<ul>
<li>Condition 2: <strong>The sample is like a simple random sample from the population, at least in regard to the variables of interest.</strong></li>
</ul>
<p>We are told that a simple random sample was taken, so we will accept that and continue on to the construction of the confidence interval.</p>
<pre class="r"><code>ttestGC(mean=1.852667,sd=2.507909,n=30)</code></pre>
<pre><code>## 
## 
## Inferential Procedures for One Mean mu:
## 
## 
## Descriptive Results:
## 
##   mean    sd  n
##  1.853 2.508 30
## 
## 
## Inferential Results:
## 
## Estimate of mu:   1.853 
## SE(x.bar):    0.4579 
## 
## 95% Confidence Interval for mu:
## 
##           lower.bound         upper.bound          
##           0.916198            2.789136</code></pre>
<p>The 95% confidence interval for the mean difference in heights of college women and their mothers is (0.9162,2.789) inches.</p>
<p><strong>Step Three</strong> <em>Interpretation of the Confidence Interval</em></p>
<p>We are 95% confident that if the true population mean of differences of heights between college women and their mothers were known, it would fall in the interval (0.9162,2.789). Since this is an interval of all positive numbers, we can say with 95% confidence that college women are taller than their mothers, on average.</p>
<p><strong>Step Four</strong> <em>Write a Conclusion</em></p>
<p>A reasonable difference for the average difference in height between a college woman and her mother is any number between 0.9162 inches and 2.789 inches. On average, college women are taller than their mothers.</p>
</div>
</div>
<div id="one-population-proportion-p" class="section level1">
<h1>One Population Proportion, <span class="math">\(p\)</span></h1>
<blockquote>
<p><strong>Research Question</strong>: What is the proportion of GC students that believe in love at first sight?</p>
</blockquote>
<p>Let’s use the <code>m111survey</code> dataset to answer this research question.</p>
<div id="the-four-steps-3" class="section level2">
<h2>The Four Steps</h2>
<p>Here, the parameter of interest is a single population proportion.</p>
<p><strong>Step One:</strong> <em>Definition of Parameter(s)</em></p>
<blockquote>
<p>Let <span class="math">\(p=\)</span> the proportion of all GC students that believe in love at first sight.</p>
</blockquote>
<p><strong>Step Two:</strong> <em>Safety Check and Calculation of the Confidence Interval</em></p>
<p>The statistic we will be using to estimate the population proportion is:</p>
<p><span class="math">\[\hat{p}=\dfrac{X}{n},\]</span></p>
<p>where:</p>
<ul>
<li><span class="math">\(X=\)</span> number of students in the sample that believe in love at first sight, and</li>
<li><span class="math">\(n=\)</span> the total number of students in the sample.</li>
</ul>
<p>Notice that <span class="math">\(X\)</span> is a binomial random variable. A “success” is defined as believing in love at first sight.</p>
<p>We know from Chapter 8 that when the number of trials, <span class="math">\(n\)</span>, is large enough, then the distribution of a binomial random variable looks very much like a normal curve. We saw that our sample was “big enough” if there are least 10 successes and at least 10 failures.</p>
<p>Therefore, the safety check for proportions is only slightly different than the safety checks that we have been doing for means. Again, we want to check that the distribution for the statistic, <span class="math">\(\hat{p}\)</span>, is approximately normal. This is important because, as you may recall, our beginning discussions of the construction of a confidence interval depended on the normal curve and the 68-95 Rule.</p>
<p><strong>Condition 1</strong>: <strong>The sample size is large enough if there are at least 10 successes and at least 10 failures in the sample.</strong></p>
<p>Let’s take a look at the table of counts and table of percents:</p>
<pre><code>## love_first
##  no yes 
##  45  26</code></pre>
<p>So, there are 26 successes (students in the sample that believe in love at first sight) and there are 45 failures (students in the sample that do not believe in love at first sight).</p>
<p>Let’s check the second condition.</p>
<p><strong>Condition 2</strong>: <strong>The sample is like a simple random sample from the population, at least in regard to the variables of interest.</strong></p>
<p>Again, this is debatable since we are dealing with a sample of only those students enrolled in MAT 111 in a given semester. However, believing in love at first sight seems to be totally unrelated to an individual enrolling in MAT 111. For this reason, our sample can be regarded as a SRS from the population of GC students, in regards to the variable <code>love_first</code>.</p>
<p>Let’s construct the confidence interval. The function that we will use for this parameter is <code>binomtestGC</code>. It works similarly to <code>ttestGC</code> that we used for means. The main difference is that we need to input what a “success” is defined to be.</p>
<pre class="r"><code>binomtestGC(~love_first,data=m111survey,success=&quot;yes&quot;)</code></pre>
<pre><code>## 
## Exact Binomial Procedures for a Single Proportion p:
##  Variable under study is love_first 
## 
## 
## Descriptive Results:  26 successes in 71 trials
## 
## Inferential Results:
## 
## Estimate of p:    0.3662 
## SE(p.hat):    0.0572 
## 
## 95% Confidence Interval for p:
## 
##           lower.bound         upper.bound          
##           0.254958            0.488976</code></pre>
<p>The 95% confidence interval for the population proportion is (0.254958 , 0.488976).</p>
<p><strong>Step Three:</strong> <em>Interpretation of the Confidence Interval</em></p>
<p>If we knew the true proportion of GC students who believe in love at first sight, it would likely fall between 0.254958 and 0.488976.</p>
<p>Let’s put this another way. If we computed a large number of these 95% confidence intervals to estimate the population proportion, we would expect that about 95% of the intervals would contain the true population proportion and about 5% would not. You can investigate this idea further with the following app.</p>
<pre class="r"><code>require(manipulate)
CIProp()</code></pre>
<p>As you use this app, consider the following question:</p>
<ul>
<li>Leaving the value of <span class="math">\(p\)</span> and confidence level the same, what happens when you increase the number of trials?</li>
<li>Leaving the value of <span class="math">\(p\)</span> and the number of trials the same, what happens when you increase the confidence level?</li>
</ul>
<p><strong>Step Four:</strong> <em>Write a Conclusion.</em></p>
<p>It is likely that the true proportion of GC students that believe in love at first sight is between 0.254958 and 0.488976. We are reasonably sure that the percentage of GC students that believe in love at first sight is somewhere in the range (25.4958%, 48.8976%).</p>
</div>
<div id="under-the-hood-3" class="section level2">
<h2>Under the Hood</h2>
<div id="how-the-test-works" class="section level3">
<h3>How the Test Works</h3>
<p>The concept of a confidence interval is the same with proportions as it was with means. The construction is similar, as well. We won’t bore you with the details again, but we should point out a couple of things about our test.</p>
<p>The <code>binomtestGC</code> finds the multiplier for construction of the confidence interval from the <strong>binomial distribution</strong>. When the sample size, <span class="math">\(n\)</span>, is large enough, we know that the binomial distribution can be well approximated using the normal curve. If we did not have access to R and our sample size was large enough, we could compute an approximate 95% confidence interval using a multiplier from the normal curve. It would look like</p>
<p><span class="math">\[\bigg(\hat{p} \pm z\cdot \sqrt{n\cdot \hat{p}\cdot (1-\hat{p})}\bigg),\]</span></p>
<p>where <span class="math">\(z\)</span> would be found using the normal curve.</p>
<p>However, this is not the calculation that R’s <code>binomtestGC</code> does! The multiplier for the confidence interval output from <code>binomtestGC</code> is found using the binomial distribution. This is nice because it does not rely on having large sample sizes so that we have a good approximation to the normal curve. In other words, we don’t have the sample size restriction when we use <code>binomtestGC</code>. When you use <code>binomtestGC</code>, you do not need to worry about Condition 1 of the safety check.</p>
<p>There is a function, <code>proptestGC</code>, that computes this confidence interval based on the normal approximation to the binomial distribution. You will see this function again in Chapters 10, so we will mention it here so you are not alarmed when the confidence interval from <code>proptestGC</code> is different than the confidence interval in <code>binomtestGC</code>.</p>
<pre class="r"><code>proptestGC(~love_first,data=m111survey,success=&quot;yes&quot;)</code></pre>
<pre><code>## 
## 
## Inferential Procedures for a Single Proportion p:
##  Variable under study is love_first 
##  Continuity Correction Applied to Test Statistic
## 
## 
## Descriptive Results:
## 
##  yes  n estimated.prop
##   26 71         0.3662
## 
## 
## Inferential Results:
## 
## Estimate of p:    0.3662 
## SE(p.hat):    0.05717 
## 
## 95% Confidence Interval for p:
## 
##           lower.bound         upper.bound          
##           0.254136            0.478258</code></pre>
<p>Notice that this interval is different than the one computed by <code>binomtestGC</code>. The <code>proptestGC</code> function constructs the confidence interval using the normal approximation which is only good if the sample size is “big enough”. For this reason, we suggest that you stick to <code>binomtestGC</code> when you are looking for a confidence interval for one proportion.</p>
<p>We will not go into any more detail about this now. The important thing is knowing how to interpret the confidence interval.</p>
</div>
<div id="values-in-the-confidence-interval" class="section level3">
<h3>Values in the Confidence Interval</h3>
<p>As you compute more confidence intervals for one population proportion, you will notice that you will never see values less than 0 or greater than 1 included in the interval. This is because a proportion is always a number between 0 and 1. Always keep in mind that a confidence interval reports the most believable values for a parameter, so the interval <em>should</em> only include legitimate values for that parameter.</p>
</div>
</div>
<div id="additional-example-2" class="section level2">
<h2>Additional Example</h2>
<blockquote>
<p><strong>Research Question</strong>: In a simple random sample of 1000 American adults, a researcher reported that 59% of the people surveyed said that they believe the world will come to an end. What is the proportion of all American adults that believe the world will come to an end?</p>
</blockquote>
<p><strong>Step One:</strong> <em>Definition of Parameter(s)</em></p>
<blockquote>
<p>Let <span class="math">\(p=\)</span> the proportion of all American adults that believe the world will come to an end.</p>
</blockquote>
<p><strong>Step Two:</strong> <em>Safety Check and Calculation of the Confidence Interval</em></p>
<p>Since we plan to use <code>binomtestGC</code>, we do not need to worry about the sample size since this test calculates the confidence interval from the binomial distribution. Thus, the only condition we need to verify for the safety check is Condition 2.</p>
<p><strong>Condition 2:</strong> <strong>The sample is like a simple random sample from the population, at least in regard to the variables of interest.</strong> For this problem, we are told that the sample was a SRS, so let’s proceed with the calculation of the confidence interval.</p>
<p>We first need to find the number of successes. Let’s define “success” as believing that the world will come to an end. Out of the 1000 people surveyed, 59% said they believe that the world will come to an end. This means that the number of successes is 590 because <span class="math">\(\dfrac{590}{1000}=0.59\)</span>.</p>
<pre class="r"><code>binomtestGC(x=590, n=1000)</code></pre>
<pre><code>## 
## Exact Binomial Procedures for a Single Proportion p:
##  Results based on Summary Data
## 
## 
## Descriptive Results:  590 successes in 1000 trials
## 
## Inferential Results:
## 
## Estimate of p:    0.59 
## SE(p.hat):    0.0156 
## 
## 95% Confidence Interval for p:
## 
##           lower.bound         upper.bound          
##           0.558788            0.620680</code></pre>
<p>Our 95% confidence interval for the population proportion is (0.558788, 0.62068).</p>
<p><strong>Step Three:</strong> <em>Interpretation of the Confidence Interval</em></p>
<p>We are 95% confident that if the true proportion of American adults who believe the world is going to end was known, it would fall between 0.5588 and 0.6207.</p>
<p><strong>Step Four:</strong> <em>Write a Conclusion</em></p>
<p>We are reasonably sure that the percentage of American adults who believe the world is going to end is in the range ( 55.8788%, 62.068%).</p>
</div>
</div>
<div id="difference-of-two-population-proportions-p_1-p_2" class="section level1">
<h1>Difference of Two Population Proportions, <span class="math">\(p_1-p_2\)</span></h1>
<blockquote>
<p><strong>Research Question</strong>: In the GC population, are females more likely than males to believe in extra-terrestrial life?</p>
</blockquote>
<p>We will use <code>m111survey</code> to answer this question.</p>
<div id="the-four-steps-4" class="section level2">
<h2>The Four Steps</h2>
<p><strong>Step One:</strong> <em>Definition of Parameter(s)</em></p>
<p>We are dealing with two independent populations - the population of all GC females and the population of all GC males. We are interested in computing the proportion that believe in extra-terrestrial life for each of these populations and then finding the difference. Thus, the parameter is the difference of proportions, <span class="math">\(p_1-p_2\)</span>.</p>
<p>Let <span class="math">\(p_1=\)</span> proportion of all GC females that believe in extra-terrestrial life.</p>
<p>Let <span class="math">\(p_2=\)</span> proportion of all GC males that believe in extra-terrestrial life.</p>
<p>For both populations, a “success” is believing in extra-terrestrial life.</p>
<p><strong>Step Two:</strong> <em>Safety Check and Calculation of the Confidence Interval</em></p>
<p>We will use the function <code>proptestGC</code> to compute the confidence interval for the difference between two population proportions. Since this function relies on the normal approximation to the binomial distribution, we will need to verify both conditions of the safety check.</p>
<p><strong>Condition 1:</strong> <strong>The sample size is large enough if <span class="math">\(n\cdot \hat{p_1}\geq 10\)</span>, <span class="math">\(n\cdot (1-\hat{p_1})\geq 10\)</span>, <span class="math">\(n\cdot \hat{p_2}\geq 10\)</span> and <span class="math">\(n\cdot (1-\hat{p_2})\geq 10\)</span>.</strong></p>
<p>Since a success is believing in extra-terrestrial life, we can verify that we have at least 10 successes and at least 10 failures for each of the samples.</p>
<pre><code>##         extra_life
## sex      no yes
##   female 30  10
##   male   11  20</code></pre>
<p>For the females, ther are 10 successes and 30 failures. For the males, there are 20 successes and 11 failures. Our sample size is big enough that we can use the normal approximation to the binomial, i.e. we can use the <code>proptestGC</code> function to compute the confidence interval.</p>
<p><strong>Condition 2:</strong> <strong>The sample is like a simple random sample from the population, at least in regard to the variables of interest.</strong></p>
<p>We’ve talked about using the <code>m111survey</code> data as a sample of the population of all GC students before. We need to be sure that one’s belief about extra-terrestrial life is independent of their being enrolled in MAT 111. If these two things were related, this would introduce bias into our study. For example, if there was some connection between believing in extra-terrestrial life and needing to take MAT 111, then our sample would not be representative of the entire population of GC students.</p>
<p>Since it seems reasonable to believe that these two things are not related, we can think of our MAT 111 survey data to be like a simple random sample.</p>
<pre class="r"><code>proptestGC(~sex+extra_life,data=m111survey,success=&quot;yes&quot;)</code></pre>
<pre><code>## 
## 
## Inferential Procedures for the Difference of Two Proportions p1-p2:
##   extra_life grouped by sex 
## 
## 
## Descriptive Results:
## 
##        yes  n estimated.prop
## female  10 40         0.2500
## male    20 31         0.6452
## 
## 
## Inferential Results:
## 
## Estimate of p1-p2:    -0.3952 
## SE(p1.hat - p2.hat):  0.1099 
## 
## 95% Confidence Interval for p1-p2:
## 
##           lower.bound         upper.bound          
##           -0.610510           -0.179812</code></pre>
<p>The 95% confidence interval for the difference in two population proportions is (-0.6105, -0.1798).</p>
<p><strong>Step Three:</strong> <em>Interpretation of the Confidence Interval</em></p>
<p>We are 95% confident that if the true difference in proportions of GC females and GC males that believe in extra-terrestrial life were known, it would be included in the interval (-0.6105,-0.1798).</p>
<p>Notice that all of the values in this interval are negative. This means that we are 95% confident that the difference in population proportions, <span class="math">\(p_1-p_2\)</span>, is negative. This difference will be negative if <span class="math">\(p_2&gt;p_1\)</span>. This means that we are pretty confident, based on the data we have collected, that the population proportion of GC males that believe in extra-terrestrial life is GREATER than the proportion of GC females that believe in extra-terrestrial life.</p>
<p><strong>Step Four:</strong> <em>Write a Conclusion.</em></p>
<p>We are reasonably sure that the proportion of GC males that believe in extra-terrestrial life is greater than the proportion of GC females that believe in extra -terrestrial life. Moreover, it is likely that about 0% to 17.9812% more males believe in extra-terrestrial life than females.</p>
</div>
<div id="under-the-hood-4" class="section level2">
<h2>Under the Hood</h2>
<p>In a confidence interval for one population proportion, we said that you should only get values between 0 and 1. Do not be alarmed that you may get negative values in a confidence interval for the <strong>difference</strong> in two population proportions. Always keep in mind what parameter you are estimating and what values are legitimate for that parameter!</p>
</div>
<div id="additional-example-3" class="section level2">
<h2>Additional Example</h2>
<blockquote>
<p><strong>Research Question</strong>: A study was done to determine whether there is a relationship between snoring and the risk of heart disease. A simple random sample selected a total of 2484 American adults. Among 1105 snorers in the study, 85 had heart disease, while only 24 of the non-snorers had heart disease. In this population, are snorers more likely than non-snorers to have heart disease?</p>
</blockquote>
<p><strong>Step One:</strong> <em>Definition of Parameter(s)</em></p>
<blockquote>
<p>Let <span class="math">\(p_1=\)</span> population proportion of snorers that have heart disease.</p>
</blockquote>
<blockquote>
<p>Let <span class="math">\(p_2=\)</span> population proportion of non-snorers that have heart disease.</p>
</blockquote>
<p>We are interested in estimating <span class="math">\(p_1-p_2\)</span>.</p>
<p><strong>Step Two:</strong> <em>Safety Check and Calculation of Confidence Interval</em></p>
<p>Since we are dealing with the difference of proportions, we will have to use the <code>proptestGC</code>, so both conditions of the safety check need to hold.</p>
<p><strong>Condition 1:</strong> <strong>The sample size is large enough if we have at least 10 successes from the sample of snorers, at least 10 failures from the sample of snorers, at least 10 successes from the sample of non-snorers, and at least 10 failures from the sample of non-snorers.</strong></p>
<p>For the sample of 1105 snorers, there are 85 successes (snorers with heart disease) and <span class="math">\(1105-85=1020\)</span> failures (snoreres without heart disease).</p>
<p>For the sample of 1379 non-snorers, there are 24 successes (non-snorers with heart disease) and <span class="math">\(1379-24=1355\)</span> failures (non-snorers without heart disease).</p>
<p><strong>Condition 2:</strong> <strong>The sample is like a simple random sample from the population, at least in regard to the variables of interest.</strong> We can assume that since the study participants were selected via a simple random sample, they constitute a representative sample of the population.</p>
<p>Let’s compute a 99% confidence interval.</p>
<pre class="r"><code>proptestGC(x=c(85,24), n=c(1105,1379), conf.level=0.99)</code></pre>
<pre><code>## 
## 
## Inferential Procedures for the Difference of Two Proportions p1-p2:
##  Results taken from summary data.
## 
## 
## Descriptive Results:
## 
##         successes    n estimated.prop
## Group 1        85 1105        0.07692
## Group 2        24 1379        0.01740
## 
## 
## Inferential Results:
## 
## Estimate of p1-p2:    0.05952 
## SE(p1.hat - p2.hat):  0.008756 
## 
## 99% Confidence Interval for p1-p2:
## 
##           lower.bound         upper.bound          
##           0.036966            0.082072</code></pre>
<p>The 99% confidence interval for the difference in the two population proportions is (0.037, 0.0821).</p>
<p><strong>Step Three:</strong> <em>Interpretation of the Confidence Interval</em></p>
<p>We are 95% confident that if the true difference in proportions of American adult snorers and non-snorers that have heart disease were known, it would be included in the interval (0.037, 0.0821).</p>
<p>Notice that all of the values in this interval are positive. This means that we are 99% confident that the difference in population proportions, <span class="math">\(p_1-p_2\)</span>, is positive. This difference will be positive if <span class="math">\(p_1&gt;p_2\)</span>, which means that the population proportion of snorers with heart disease is greater than the population proportion of non-snorers with heart disease.</p>
<p><strong>Step Four:</strong> <em>Write a Conclusion.</em></p>
<p>We are reasonably sure that the proportion of American adult snorers with heart disease is greater than the proportion of non-snorers with heart disease. Moreover, it is likely that about 3.6966% to 8.2072% more snorers have heart disease than non-snorers.</p>
</div>
</div>
<div id="thoughts-on-r" class="section level1">
<h1>Thoughts on R</h1>
<div id="new-r-functions" class="section level2">
<h2>New R Functions</h2>
<p>Know how to use this functions:</p>
<ul>
<li><code>ttestGC</code></li>
<li><code>binomtestGC</code></li>
<li><code>proptestGC</code></li>
</ul>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
