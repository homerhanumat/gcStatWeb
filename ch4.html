<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<title>Chapter 4: Two Numerical Variables</title>

<script src="ch4_files/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link href="ch4_files/bootstrap-2.3.2/css/bootstrap.min.css" rel="stylesheet" />
<link href="ch4_files/bootstrap-2.3.2/css/bootstrap-responsive.min.css" rel="stylesheet" />
<script src="ch4_files/bootstrap-2.3.2/js/bootstrap.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="ch4_files/highlight/default.css"
      type="text/css" />
<script src="ch4_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>


<link rel="stylesheet" href="css/coursenotes.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
</style>
<div class="container-fluid main-container">


<div id="header">
<h1 class="title">Chapter 4: Two Numerical Variables</h1>
</div>

<div id="TOC">
<ul>
<li><a href="#outline">Outline</a></li>
<li><a href="#statistical-relationships">Statistical Relationships</a><ul>
<li><a href="#scatterplots">Scatterplots</a><ul>
<li><a href="#overall-patterns">Overall Patterns</a></li>
<li><a href="#direction">Direction</a></li>
</ul></li>
<li><a href="#correlation">Correlation</a><ul>
<li><a href="#strength-of-association">Strength of Association</a></li>
</ul></li>
<li><a href="#regression-equation">Regression Equation</a><ul>
<li><a href="#residuals">Residuals</a></li>
<li><a href="#predictions">Predictions</a></li>
<li><a href="#interpretation-of-slope-and-intercept">Interpretation of Slope and Intercept</a></li>
<li><a href="#how-well-does-our-regression-line-fit">How Well does our Regression Line fit?</a></li>
<li><a href="#properties-of-r2">Properties of <span class="math">\(r^2\)</span></a></li>
</ul></li>
</ul></li>
<li><a href="#cautions">Cautions</a><ul>
<li><a href="#extrapolation">Extrapolation</a></li>
<li><a href="#influential-observations">Influential Observations</a></li>
<li><a href="#association-versus-causation">Association versus Causation</a></li>
<li><a href="#simpsons-paradox">Simpson’s Paradox</a></li>
</ul></li>
<li><a href="#curvilinear-fits">Curvilinear Fits</a></li>
<li><a href="#thoughts-on-r">Thoughts on R</a><ul>
<li><a href="#new-r-functions">New R Functions</a></li>
</ul></li>
</ul>
</div>

<div id="outline" class="section level1">
<h1>Outline</h1>
<p>In the previous chapter, we investigated methods for describing relationships between two <em>factor</em> variables, using</p>
<ul>
<li>twoway tables and row percents for numerical desriptive statistics;</li>
<li>barcharts for graphical descitpive statistics;</li>
<li>the chi-square test, for inference.</li>
</ul>
<p>In this chapter, we are interested in describing relationships between <strong>two</strong> <em>numerical</em> variables.</p>
<p>In chapter 2, we learned about methods to graphically and numerically summarize <strong>one</strong> numerical variable. The methods we used were:</p>
<p><strong>Graphical</strong></p>
<ul>
<li>Histogram</li>
<li>Density Plot</li>
<li>Stem Plot</li>
<li>Box Plot (and the really cool Violin Plot)</li>
</ul>
<p><strong>Numerical</strong></p>
<ul>
<li>Median</li>
<li>Percentiles (quantiles)</li>
<li>5-number summary</li>
<li>Mean</li>
<li>SD</li>
<li>IQR</li>
</ul>
<p>We will now be learning to describe how <em>two numerical variables</em> relate to one another. Throughout this chapter, we’re going to work with three different datasets: <code>m111survey</code>, <code>pennstate1</code>, and <code>ucdavis1</code>. The datasets <code>pennstate1</code> and <code>ucdavis1</code> are both surveys of students at their respective schools, very similar to what we’ve seen in <code>m111survey</code>. You can put these datasets into your Global Environment, take a quick look at them, and learn more about them with:</p>
<pre class="r"><code>data(m111survey)
View(m111survey)
help(m111survey)

data(pennstate1)
View(pennstate1)
help(pennstate1)

data(ucdavis1)
View(ucdavis1)
help(ucdavis1)</code></pre>
</div>
<div id="statistical-relationships" class="section level1">
<h1>Statistical Relationships</h1>
<p>When we look at relationships between numerical variables, there are 2 main kinds of relationships that interest us.</p>
<ul>
<li><em>Deterministic</em> relationships</li>
<li><em>Statistical</em> relationships</li>
</ul>
<p><em>Deterministic</em> relationships are the type you are used to seeing in algebra class. In this kind of relationship, the value of one variable can be <em>exactly</em> determined by the value of the other variable.</p>
<p>For example, consider the relationship between degrees Fahrenheit and degrees Celcius. If <span class="math">\(y= ^\circ\)</span> C and <span class="math">\(x= ^\circ\)</span> F, the deterministic relationship can be written</p>
<p><span class="math">\[y=\frac{5}{9}(x-32).\]</span></p>
<p>The degrees Celcius (<span class="math">\(y\)</span>) is <em>exactly</em> determined by knowing the degrees Fahrenheit (<span class="math">\(x\)</span>). The graph of this equation is a line. See Figure [Deterministic Relationship].</p>
<div class="figure">
<img src="ch4_files/figure-html/fcplot-1.png" alt="Deterministic Relationship. This graph shows the relationship between Fahrenheit and Celcius." /><p class="caption">Deterministic Relationship. This graph shows the relationship between Fahrenheit and Celcius.</p>
</div>
<p>We can see from the graph that there is no variation in the pattern. Every temperature in Fahrenheit has exactly one corresponding temperature in Celcius. We might regard Fahrenheit and Celcius as having a perfect relationship.</p>
<p><em>Statistical</em> relationships are the ones that we will study in this class. In this kind of relationship, there is variation from the average pattern. If we know the value of one variable, we can <em>estimate</em> the typical value of the other variable. However, this is only an estimation. There is no certainty!</p>
<p>For example, suppose we want to use the length of a person’s right handspan (the measurement of one’s outstretched right hand from thumb to pinky) to predict the person’s height. There is certainly a <em>relationship</em> between the length of one’s handspan and their height. People with large hands tend to be taller than people with small hands. However, there is not an equation (or a line) that will <em>exactly</em> tell us the height of a person with a certain handspan. Not every person with a handspan of 20 centimeters is exactly the same height. There is variation in their heights. What we’ve just described is a statistical relationship.</p>
<p>There are 3 tools that we will use to describe <em>statistical</em> relationships:</p>
<ul>
<li>Scatterplots</li>
<li>Correlation</li>
<li>Regression Equation</li>
</ul>
<div id="scatterplots" class="section level2">
<h2>Scatterplots</h2>
<p>As we just discussed, our intuition tells us that a person with a large right handspan tends to be tall. Let’s investigate this idea with the following research question.</p>
<blockquote>
<p><strong>Research Question</strong>: At Pennstate, how is a student’s right handspan related to his/her height?</p>
</blockquote>
<p>A <em>scatterplot</em> is how we will graphically display the relationship between two numerical variables. Scatterplots allow us to visually identify</p>
<ul>
<li>overall patterns,</li>
<li>directions, and</li>
<li>strength of association</li>
</ul>
<p>between two numerical variables. The best way to get a feel for the relationship between two numerical variables is to take a look at the scatterplot.</p>
<div id="overall-patterns" class="section level3">
<h3>Overall Patterns</h3>
<p>Let’s use the function <code>xyplot()</code> in <code>R</code> to create a scatterplot of the variables <strong>RtSpan</strong> and <strong>Height</strong> from the <code>pennstate1</code> dataset.</p>
<pre class="r"><code>xyplot(Height~RtSpan,data=pennstate1,
       xlab=&quot;Right Handspan (cm)&quot;, ylab=&quot;Height (in)&quot;) </code></pre>
<div class="figure">
<img src="ch4_files/figure-html/xyhandheight-1.png" alt="Hand/Height Scatterplot. Relationship Between Right Handspan and Height" /><p class="caption">Hand/Height Scatterplot. Relationship Between Right Handspan and Height</p>
</div>
<p>Each point, (<span class="math">\(x,y\)</span>), that you see on the scatterplot represents an individual in the dataset - one of the 190 Penn State students in the survey. The <span class="math">\(x\)</span>-coordinate of the point is that student’s right handspan, in centimeters. The <span class="math">\(y\)</span>-coordinate of the point is that student’s height, in inches.</p>
<p>The ‘formula-data’ input syntax for <code>xyplot()</code> should be starting to become familiar to you. This is the same syntax that we used to produce the graphical outputs from chapter 2. <code>R</code> will plot the variable in front of the <code>~</code> along the vertical axis and the variable behind the <code>~</code> along the horizontal axis. Typically, we put the explanatory variable along the horizontal axis and the response variable along the vertical axis.</p>
<p>You can control how the points in the scatterplot appear using <code>pch</code> and <code>col</code> in the <code>xyplot()</code> function. For example, we can make the scatterplot have solid red points. See Figure[Red Points].</p>
<pre class="r"><code>xyplot(Height~RtSpan,data=pennstate1,
       xlab=&quot;Right Handspan (cm)&quot;, ylab=&quot;Height (in)&quot;, 
       col=&quot;red&quot;,pch=19) </code></pre>
<div class="figure">
<img src="ch4_files/figure-html/xyhandheightred-1.png" alt="Red Points: Relationship Between Right Handspan and Height using solid red points" /><p class="caption">Red Points: Relationship Between Right Handspan and Height using solid red points</p>
</div>
<p><strong>Note</strong>: If you are interested in making fancier scatterplots, there are different values of <code>pch</code> that produce various shapes for the points in the scatterplot. To learn more, consult GeekNotes. <code>R</code> will also provide a complete list of all available colors with:</p>
<pre class="r"><code>colors()</code></pre>
<p>When we look at the overall pattern in the scatterplot in Figure[Hand/Height Scatterplot], it appears that students with large right handspans also tend to be tall. One question that may arise is whether this observed relationship is simply the result of the student’s sex. On average, males tend to have larger hands than females and also tend to be taller than females. As we did in chapter 2, we can look at parallel scatterplots in separate panels by “conditioning” on a category, such as <strong>Sex</strong>. See Figure[Parallel Hand/Height by Sex].</p>
<pre class="r"><code>xyplot(Height~RtSpan|Sex,data=pennstate1,
       xlab=&quot;Right Handspan (cm)&quot;, ylab=&quot;Height (in)&quot;,pch=19)</code></pre>
<div class="figure">
<img src="ch4_files/figure-html/parallelxyhandheight-1.png" alt="Parallel Hand/Height by Sex: Scatterplots showing the relationship between one’s right handspan and height appear in separate panels." /><p class="caption">Parallel Hand/Height by Sex: Scatterplots showing the relationship between one’s right handspan and height appear in separate panels.</p>
</div>
<p>We can see that the relationship we observed in the original scatterplot seems to hold separately for both males and females.</p>
<p>Parallel scatterplots can sometimes be hard to compare since they have separate x-axes. We can “overlay” these scatterplots, using one color for the points representing the males and another color for the points representing the females. Overlaying can be accomplished using the <em>groups</em> argument. See Figure[Overlayed Hand/Height by Sex].</p>
<pre class="r"><code>xyplot(Height~RtSpan,groups=Sex,data=pennstate1,
       xlab=&quot;Right Handspan (cm)&quot;, ylab=&quot;Height (in)&quot;,
       pch=19,auto.key=TRUE)</code></pre>
<div class="figure">
<img src="ch4_files/figure-html/overlayedxyhandheight-1.png" alt="Overlayed Hand/Height by Sex: One scatterplot showing the relationship between right handspan and height colored by sex." /><p class="caption">Overlayed Hand/Height by Sex: One scatterplot showing the relationship between right handspan and height colored by sex.</p>
</div>
<p><strong>Note</strong>: Given several numerical variables, R can produce a group of scatterplots, one for each pair of variables—all in one graph. Such a graph is called a <em>scatterplot matrix</em>. For more information, consult GeekNotes.</p>
</div>
<div id="direction" class="section level3">
<h3>Direction</h3>
<p>Viewing the scatterplot allows us to detect overall patterns. One way to describe the scatterplot is by giving a name to the <em>direction</em> of the observed pattern.</p>
<p>In the scatterplot Figure[Hand/Height Scatterplot], we could say that the variables <code>RtSpan</code> and <code>Height</code> are <em>positively associated</em> since students with larger handspans tend to be on the tall side and students with smaller handspans tend to be on the short side.</p>
<p>Positive Linear Association</p>
<p>: Two numerical variables have a <em>positive linear association</em> if high values of one variable tend to accompany high values of the other and low values of one variable tend to accompany low values of the other.</p>
<p>To give a visualization for positively associated variables, let’s add a vertical line that marks the <em>mean</em> of the right handspans and a horizontal line that marks the <em>mean</em> of the heights to break the scatterplot into four “boxes”. See Figure[Positive Association].</p>
<div class="figure">
<img src="ch4_files/figure-html/positivexyhandheight-1.png" alt="Positive Association: Most of the points are in the upper right and lower left box showing a positive association between the variables." /><p class="caption">Positive Association: Most of the points are in the upper right and lower left box showing a positive association between the variables.</p>
</div>
<p>Notice that there are more points in the upper right box and lower left box than in the other two boxes. The upper right box includes points from individuals with higher than average right handspans <strong>and</strong> higher than average heights. The lower left box includes points from individuals with lower than average right handspans <strong>and</strong> lower than average heights. When most of the points in a scatterplot are located in these two boxes, the variables have a positive linear association.</p>
<dl>
<dt>Negative Linear Association</dt>
<dd>Two variables have a <em>negative linear association</em> if high values of one variable tend to accompany low values of the other and low values of one variable tend to accompany high values of the other.
</dd>
</dl>
<p>Let’s take a look at an example of negatively associated variables from the <code>m111survey</code> dataset - <code>GPA</code> and <code>height</code>.</p>
<div class="figure">
<img src="ch4_files/figure-html/negativexyhandheight-1.png" alt="Negative Association: Most of the points are in the lower right and upper left box showing a negative association between the variables." /><p class="caption">Negative Association: Most of the points are in the lower right and upper left box showing a negative association between the variables.</p>
</div>
<p>Notice that in this scatterplot, there are more points in the upper left box and the lower right box. (See Figure[Negative Association].) The upper left box includes points from individuals with lower than average GPA’s <strong>and</strong> higher than average heights. The lower right box includes points from individuals with higher than average GPA’s <strong>and</strong> lower than average heights. When most of the points in a scatterplot are located in these two boxes, the variables have a <em>negative linear association</em>.</p>
<dl>
<dt>No Associaton</dt>
<dd>Two variables have <em>no association</em> if there is no apparent relationship between the two variables.
</dd>
</dl>
<p>Let’s look at the <em>association</em> between an individual’s height, <code>Height</code>, and the hours of sleep they got last night, <code>HrsSleep</code>, from the <code>pennstate1</code> dataset.</p>
<div class="figure">
<img src="ch4_files/figure-html/noassocxyhandheight-1.png" alt="No Association: There is no apparent pattern in where the points lie." /><p class="caption">No Association: There is no apparent pattern in where the points lie.</p>
</div>
<p>In the scatterplot in Figure[No Association], it appears that all of the boxes have <em>about</em> the same number of points in them. When this is the case, the variables have <em>no linear association</em>.</p>
<p>So far we have been interested in variables that appear to have a <em>linear association</em>, and our goal has been to describe the linear association that we see in a scatterplot using the equation of straight line. Data with nonlinear association certainly exists! For example, <em>curvilinear</em> data follows the trend of a curve, rather than a line. You can see an example of this by looking at the <code>fuel</code> dataset. Read the <code>help</code> file on this data to understand what the variables are.</p>
<pre class="r"><code>data(fuel)
View(fuel)
help(fuel)</code></pre>
<div class="figure">
<img src="ch4_files/figure-html/xyfuel-1.png" alt="Curvilinear: Efficiency (liters of fuel required to travel 100 kilometers) versus Speed (kilometers per hour)" /><p class="caption">Curvilinear: Efficiency (liters of fuel required to travel 100 kilometers) versus Speed (kilometers per hour)</p>
</div>
<p>The scatterplot clearly shows that this data would not be well represented by a line. (See Figure[Curvilinear].) As a vehicle’s speed increases up to about 60 kph, the amount of fuel required to travel 100 kilometers decreases. However, as a vehicle’s speed increases from about 60 kph, the fuel efficiency increases: this is one type of curvilinear relationship. We will offer a rudimentary discussion of nonlinear relationships later on in this Chapter.</p>
<p>For now, we’ll stick with linear relationships.</p>
</div>
</div>
<div id="correlation" class="section level2">
<h2>Correlation</h2>
<div id="strength-of-association" class="section level3">
<h3>Strength of Association</h3>
<p>So far, we’ve investigated relationships between two numerical variables by looking at the scatterplot and making note of the observed pattern of the points, and their direction of association. However, as we can see in the following scatterplots, we should also consider the <em>strength</em> of association between two variables. See Figure<a href="#strength-of-association">Strength of Association</a>.</p>
<p><img src="ch4_files/figure-html/twopositivexy-1.png" alt="Strength of Association: Two examples of pairs of variables with positive linear association. The first scatterplot shows a positive linear association between Height (in) and Fastest Speed Ever Driven (mph). The second scatterplot shows a stronger positive linear association between Height (in) and Ideal Height (in)." /> <img src="ch4_files/figure-html/twopositivexy-2.png" alt="Strength of Association: Two examples of pairs of variables with positive linear association. The first scatterplot shows a positive linear association between Height (in) and Fastest Speed Ever Driven (mph). The second scatterplot shows a stronger positive linear association between Height (in) and Ideal Height (in)." /></p>
<p>These scatterplots both display 2 variables that are <em>positively</em> associated, but there is less “scatter” in the second plot. The variable <strong>height</strong> appears to have a stronger association with <strong>ideal_ht</strong> than it does with <strong>fastest</strong>. To distinguish between the amount of “scatter” in a plot, it is useful to assign a numerical value to the strength of association.</p>
<dl>
<dt>Correlation</dt>
<dd><em>Correlation</em> is the numerical measure of the direction and strength of the linear association between two numerical variables.
</dd>
</dl>
<p>The formula for the <em>correlation coefficient</em>, <span class="math">\(r\)</span>, is written:</p>
<p><span class="math">\[ r=\frac{1}{n-1}\sum{\bigg(\frac{x_i-\bar{x}}{s_x}\bigg)\bigg(\frac{y_i-\bar{y}}{s_y}\bigg)}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math">\(n\)</span> denotes the number of values in the list</li>
<li><span class="math">\(\sum\)</span> means summing</li>
<li><span class="math">\(x_i\)</span> denotes the individual <span class="math">\(x\)</span> values</li>
<li><span class="math">\(\bar{x}\)</span> denotes the average of the <span class="math">\(x\)</span>’s</li>
<li><span class="math">\(s_x\)</span> denotes the SD of the <span class="math">\(x\)</span>’s</li>
<li><span class="math">\(y_i\)</span> denotes the individual <span class="math">\(y\)</span> values</li>
<li><span class="math">\(\bar{y}\)</span> denotes the average of the <span class="math">\(y\)</span>’s</li>
<li><span class="math">\(s_y\)</span> denotes the SD of the <span class="math">\(y\)</span>’s</li>
</ul>
<p>Since this calculation can be cumbersome, we will use R’s built in function, <code>cor()</code>. The correlation coefficient for the variables <strong>height</strong> and <strong>fastest</strong> from the <code>m111survey</code> dataset plotted in the first scatterplot in Figure<a href="#strength-of-association">Strength of Association</a> can be found by:</p>
<pre class="r"><code>cor(fastest~height,data=m111survey,use=&quot;na.or.complete&quot;)</code></pre>
<pre><code>## [1] 0.1708742</code></pre>
<p>Let’s compare this to the correlation coefficient for the variables <strong>height</strong> and <strong>ideal_ht</strong> from the <code>m111survey</code> dataset plotted in the second scatterplot in Figure<a href="#strength-of-association">Strength of Association</a>.</p>
<pre class="r"><code>cor(ideal_ht~height,data=m111survey,use=&quot;na.or.complete&quot;)</code></pre>
<pre><code>## [1] 0.832047</code></pre>
<p>What we’ve just observed holds true in general.</p>
<ul>
<li><p><em>Positively associated</em> variables have a <em>positive</em> correlation coefficient, <span class="math">\(r&gt;0\)</span>. Consult Geek Notes for a detailed explanation.</p></li>
<li><p>The stronger the association is between variables, the larger the correlation coefficient, <span class="math">\(r\)</span>, will be.</p></li>
</ul>
<p>The same ideas hold true for <em>negatively</em> associated variables and variables with <em>no</em> association. Let’s summarize all of the properties of the correlation coefficient, <span class="math">\(r\)</span>.</p>
<p><strong>Properties of <span class="math">\(r\)</span></strong></p>
<ul>
<li><p><span class="math">\(r\)</span> always falls between 1 and -1.</p></li>
<li><p>The sign of r indicates the <em>direction</em> of the relationship.</p>
<ul>
<li><span class="math">\(r&gt;0\)</span> indicates a <em>positive linear association</em></li>
<li><span class="math">\(r&lt;0\)</span> indicates a <em>negative linear association</em></li>
</ul></li>
<li><p>The <em>magnitude</em> of r indicates the <em>strength</em> of the relationship. See Figure[Correlation Values].</p>
<ul>
<li><span class="math">\(r = 1\)</span> indicates a <em>perfect positive</em> linear relationship. All points fall exactly on a line sloping upward.</li>
<li><span class="math">\(r = -1\)</span> indicates a <em>perfect negative</em> linear relationship. All points fall exactly on a line sloping downward.</li>
<li><span class="math">\(r = 0\)</span> indicates <em>no</em> linear relationship.</li>
</ul></li>
</ul>
<div class="figure">
<img src="ch4_files/figure-html/correlationvalues-1.png" alt="Correlation Values: The first scatterplot represents two variables that have a perfect positive linear relationship, r=1. The second scatterplot represents two variables that have a perfect negative linear relationship, r=-1. The third scatterplot represents two variables that have no linear relationship, r=0." /><p class="caption">Correlation Values: The first scatterplot represents two variables that have a perfect positive linear relationship, <span class="math">\(r=1\)</span>. The second scatterplot represents two variables that have a perfect negative linear relationship, <span class="math">\(r=-1\)</span>. The third scatterplot represents two variables that have no linear relationship, <span class="math">\(r=0\)</span>.</p>
</div>
<p>You can investigate this with the following app, as well.</p>
<pre class="r"><code>require(manipulate)
VaryCorrelation()</code></pre>
<p>Let’s look at an example.</p>
<blockquote>
<p><strong>Research Question</strong>: At UCDavis, how is a student’s mom’s height (<strong>momheight</strong>) related to their dad’s height (<strong>dadheight</strong>)?</p>
</blockquote>
<p>Since we are interested in how a mother’s height is related to a father’s height, we will treat <strong>dadheight</strong> as the explanatory variable and <strong>momheight</strong> as the response variable. We will start by taking a look at the scatterplot. See Figure[Mom/Dad Height].</p>
<pre class="r"><code>xyplot(momheight~dadheight,data=ucdavis1)</code></pre>
<div class="figure">
<img src="ch4_files/figure-html/xymomdadheight-1.png" alt="Mom/Dad Height" /><p class="caption">Mom/Dad Height</p>
</div>
<p>Since our cloud of points seems to be somewhat shaped upward to the right, it seems that students with tall dads tend to have tall moms as well and students with short dads also have short moms. This suggests that the correlation coefficient will be positive. However, since the cloud of points is not very tightly clustered, we might think that this association is not very strong. We might predict that the correlation coefficient, <span class="math">\(r\)</span>, will be positive but closer to 0 than it is to 1. Let’s find out.</p>
<pre class="r"><code>cor(momheight~dadheight,data=ucdavis1,use=&quot;na.or.complete&quot;)</code></pre>
<pre><code>## [1] 0.2571501</code></pre>
</div>
</div>
<div id="regression-equation" class="section level2">
<h2>Regression Equation</h2>
<p><em>Regression analysis</em> is used to numerically explain the linear relationship between two numerical variables using the equation of a line. It is much more specific than the visual analysis we’ve been making by looking at scatterplots.</p>
<p>Recall that the equation of the line <span class="math">\(y=\frac{5}{9}(x-32)\)</span> was used to describe the <em>deterministic</em> relationship between degrees Celcius and degrees Fahrenheit. This equation let us determine the <em>exact</em> temperature in Celcius by knowing the temperature in Fahrenheit.</p>
<p>The <em>regression equation</em> is the equation of a line that is used to <em>predict</em> the value for the response variable (<span class="math">\(y\)</span>) from a known value of the explanatory variable (<span class="math">\(x\)</span>) in a <em>statistical</em> relationship. It describes how, <em>on average</em>, the response variable is related to the explanatory variable.</p>
<p>In general, the equation of the regression line is</p>
<p><span class="math">\[\hat{y}=a+bx,\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math">\(a\)</span> is the <em><span class="math">\(y\)</span>-intercept</em>. (The <span class="math">\(y\)</span>-intercept is the point where the line crosses the vertical axis. It is the height of the line at <span class="math">\(x=0\)</span>.)</p></li>
<li><p><span class="math">\(b\)</span> is the <em>slope</em>. (The slope is the inclination or orientation of the line. It is calculated by the ratio <span class="math">\(\frac{\mbox{rise}}{\mbox{run}}\)</span>.)</p></li>
<li><p><span class="math">\(x\)</span> is the known value of the explanatory variable.</p></li>
<li><p><span class="math">\(\hat{y}\)</span> is the <em>predicted</em> value of the response variable.</p></li>
</ul>
<p>The regression line is the line that best approximates the data in the scatterplot. Returning to our original research question involving the variables <strong>Height</strong> and <strong>RtSpan</strong> from the <code>pennstate1</code> dataset, let’s take a look at the regression line. See Figure[Hand/Height Regression Line].</p>
<div class="figure">
<img src="ch4_files/figure-html/regressionlineheightrtspan-1.png" alt="Hand/Height Regression Line: The regression line is plotted on the scatterplot showing the relationship between right handspan and height." /><p class="caption">Hand/Height Regression Line: The regression line is plotted on the scatterplot showing the relationship between right handspan and height.</p>
</div>
<p>Each point <span class="math">\((x,y)\)</span> on the scatterplot is an observation—known <span class="math">\(x\)</span> and <span class="math">\(y\)</span>-values. These points correspond to the measurements for an actual individual in the sample. Each point <span class="math">\((x,\hat{y})\)</span> on the regression line is a known <span class="math">\(x\)</span>-value and its predicted response, <span class="math">\(\hat{y}\)</span>.</p>
<div id="residuals" class="section level3">
<h3>Residuals</h3>
<p>You can see how the regression line in Figure[Hand/Height Regression Line] seems to do a good job describing the trend of the points in the scatterplot. In fact, it “best fits” the data. The regression (best fit) line is the line that is collectively the closest, in terms of vertical measurement, to all of the points on the scatterplot. These vertical measurements are called <em>residuals</em>. Residuals measure the size of the prediction errors.</p>
<dl>
<dt>Residuals</dt>
<dd>For a given data point <span class="math">\((x,y)\)</span>, the <em>residual</em> is the difference between the observed response and the response that is predicted by the regression line.
</dd>
</dl>
<p>This can be written</p>
<p><span class="math">\[y - \hat{y}.\]</span></p>
<p>On the graph, a residual is the vertical distance between a point and the regression line. Several residuals are plotted on the scatterplot shown in Figure<a href="#residuals">Residuals</a>.</p>
<pre><code>## Predict Height is about 58.07,
## give or take 3.285 or so for chance variation.</code></pre>
<pre><code>## Predict Height is about 61.79,
## give or take 3.209 or so for chance variation.</code></pre>
<pre><code>## Predict Height is about 63.03,
## give or take 3.191 or so for chance variation.</code></pre>
<div class="figure">
<img src="ch4_files/figure-html/handheightresiduals-1.png" alt="Residuals" /><p class="caption">Residuals</p>
</div>
<p>A residual can be computed for every point on the scatterplot. The regression line <span class="math">\(\hat{y} = a + bx\)</span> is determined by choosing <span class="math">\(a\)</span> (intercept) and <span class="math">\(b\)</span> (slope) so that the sum of the (squared) residuals is minimized.</p>
<p><span class="math">\[\mbox{Sum of Squares }= \sum(\mbox{ residuals})^2 = \sum(y_i-\hat{y})^2 \]</span></p>
<p>Investigate how to minimize the sum of squared residuals with the following app.</p>
<pre class="r"><code>require(manipulate)
FindRegLine()</code></pre>
<p>If you are interested in how the actual values of <span class="math">\(a\)</span> and <span class="math">\(b\)</span> are calculated, consult Geek Notes.</p>
</div>
<div id="predictions" class="section level3">
<h3>Predictions</h3>
<p>Now that we know what the regression equation means and how it is found, let’s use it to make some statements about the Pennstate scatterplot of heights and right handspans.</p>
<blockquote>
<p><strong>Research Question</strong>: What is the predicted height of a Pennstate student with a right handspan measurement of 22 cm?</p>
</blockquote>
<p>We have already seen the graph of the regression line in Figure[Hand/Height Regression Line]. Now, we would like to know the equation for this line. R has a built in function, <code>lmGC()</code>, to compute this equation. (The <code>lm</code> stands for “linear model”.) This function also has the option to graph the regression line (set <code>graph=TRUE</code>).</p>
<pre class="r"><code>lmGC(Height~RtSpan,data=pennstate1,graph=TRUE)</code></pre>
<pre><code>## 
##  Linear Regression
## 
## Correlation coefficient r =  0.6314 
## 
## Equation of Regression Line:
## 
##   Height = 41.9593 + 1.2394 * RtSpan 
## 
## Residual Standard Error: s   = 3.1486 
## R^2 (unadjusted):        R^2 = 0.3987</code></pre>
<div class="figure">
<img src="ch4_files/figure-html/heighthandxyreg-1.png" alt="Height/Hand Regression: In the lmGC function, the parameter graph=TRUE will plot the regression line as well as giving the equation of the regresion line." /><p class="caption">Height/Hand Regression: In the lmGC function, the parameter graph=TRUE will plot the regression line as well as giving the equation of the regresion line.</p>
</div>
<p>So the equation of our regression line is:</p>
<p><span class="math">\[\hat{y}=41.959349+1.239448 x\]</span></p>
<p>We can now answer the research question two different ways. We can use R as a calculator and plug <span class="math">\(x=22\)</span> into the equation above.</p>
<pre class="r"><code>41.95935+1.239448*22</code></pre>
<pre><code>## [1] 69.22721</code></pre>
<p>A Penn State student with a right handspan of 22 centimeters will have a predicted height of 69.22721 inches.</p>
<p>Another way to do this is using the <code>predict()</code> function. The <code>predict()</code> function requires two inputs:</p>
<ul>
<li>a linear model,</li>
<li>a value of the explanatory variables, <span class="math">\(x\)</span>.</li>
</ul>
<p>In order to use the <code>predict()</code> function, you should store your linear model in a variable.</p>
<pre class="r"><code>handheightmod &lt;- lmGC(Height~RtSpan,data=pennstate1)
predict(handheightmod,22)</code></pre>
<pre><code>## Predict Height is about 69.23,
## give or take 3.158 or so for chance variation.</code></pre>
<p>In addition to the prediction, R gives you a <em>prediction standard error</em> — a rough estimate of how much your prediction of the person’s height is liable to differ from his or her actual height. In general, the regression line’s prediction could easily differ from the actual <span class="math">\(y\)</span> value (the height) by as much as a couple of prediction standard errors.</p>
<p>If you would like to have a better feel for where the actual value of <span class="math">\(y\)</span> might lie, consider asking for a <em>prediction interval</em>, as follows:</p>
<pre class="r"><code>predict(handheightmod,22,level=0.95)</code></pre>
<pre><code>## Predict Height is about 69.23,
## give or take 3.158 or so for chance variation.
## 
## 95%-prediction interval:
##           lower.bound         upper.bound          
##           62.997191           75.457220</code></pre>
<p>You can be about 95%-confident that the actual height of a person with a handspan of 22 centimeters is somewhere between 63 and 74.5 inches — the stated bounds of the 95%-prediction interval above.</p>
<p>You can ask for prediction intervals at any level of confidence between 0% and 100%,simply by varying the value of the <code>level</code> parameter. For an 80%-prediction interval, run the command:</p>
<pre class="r"><code>predict(handheightmod,22,level=0.80)</code></pre>
<pre><code>## Predict Height is about 69.23,
## give or take 3.158 or so for chance variation.
## 
## 80%-prediction interval:
##           lower.bound         upper.bound          
##           65.165568           73.288843</code></pre>
<p>The prediction interval is narrower than before, but you pay a price for the extra precision: now you are only 80%-confident that the actual height lies somewhere inside of the interval.</p>
</div>
<div id="interpretation-of-slope-and-intercept" class="section level3">
<h3>Interpretation of Slope and Intercept</h3>
<p>It’s also important that we know how to interpret the <em>slope</em> and <em>intercept</em> of a regression line. Let’s take a look at the <code>pushups</code> dataset.</p>
<pre class="r"><code>data(pushups)
View(pushups)
help(pushups)</code></pre>
<p>Now, we can view the equation of the regression line as well as the scatterplot with the regression line plotted. See Figure[Pushups].</p>
<pre><code>## 
##  Linear Regression
## 
## Correlation coefficient r =  -0.4675 
## 
## Equation of Regression Line:
## 
##   weight = 247.4886 + -0.8243 * pushups 
## 
## Residual Standard Error: s   = 39.5677 
## R^2 (unadjusted):        R^2 = 0.2185</code></pre>
<div class="figure">
<img src="ch4_files/figure-html/xyweightpushups-1.png" alt="Pushups: Scatterplot and Regression line for Pushups versus Weight Relationship" /><p class="caption">Pushups: Scatterplot and Regression line for Pushups versus Weight Relationship</p>
</div>
<ul>
<li><p>What does the intercept mean? 247.49 is the <em>predicted</em> weight of a GC football player who cannot do any pushups in 2 minutes.</p></li>
<li><p>What does the slope mean? The <em>predicted</em> weight of a football player changes by -0.82 pounds as the max number of pushups in two minutes increases by 1. In other words, for every one pushup increase, the predicted weight of the football player decreases by 0.82 pounds. This tells us that there is a <em>negative</em> association between max number of pushups and weight!</p></li>
</ul>
<p><strong>Note:</strong> The interpretation of the intercept doesn’t always make sense! Consider, for example, what would happen if we used <strong>weight</strong> as the explanatory variable and <strong>pushups</strong> as the response. See Figure[Slope/Intercept Interpretation].</p>
<pre><code>## 
##  Linear Regression
## 
## Correlation coefficient r =  -0.4675 
## 
## Equation of Regression Line:
## 
##   pushups = 97.6857 + -0.2651 * weight 
## 
## Residual Standard Error: s   = 22.4414 
## R^2 (unadjusted):        R^2 = 0.2185</code></pre>
<div class="figure">
<img src="ch4_files/figure-html/xypushupsweight-1.png" alt="Slope/Intercept Interpretation: Slope and intercept interpretation for the regression line used to predict maximum number of pushups from weight of a football player." /><p class="caption">Slope/Intercept Interpretation: Slope and intercept interpretation for the regression line used to predict maximum number of pushups from weight of a football player.</p>
</div>
<ul>
<li><p>Intercept Interpretation: 247.49 is the <em>predicted</em> maximum number of pushups of a GC football player who weighs 0 pounds can do in 2 minutes. This doesn’t make any logical sense!</p></li>
<li><p>Slope Interpretation: The <em>predicted</em> maximum number of pushups that a football player can do in two minutes changes by -0.82 as the weight of the football player increases by 1 pound. In other words, for every one pound increase in weight, the predicted maximum number of pushups decreases by 0.82.</p></li>
</ul>
</div>
<div id="how-well-does-our-regression-line-fit" class="section level3">
<h3>How Well does our Regression Line fit?</h3>
<p>If all the data in a scatterplot lie <em>exactly on</em> the regression line, we say that our regression line perfectly explains the data. However, this is rarely the case. We usually have points that do not lie on the regression line. Anywhere that this happens, we have <em>variation</em> that is not explained by the regression line. If the data is not on a line, then a line will not be a <em>perfect</em> explanation of the data.</p>
<p>One way we can measure this variation is the <em>residual standard error</em> — sometimes called RSE, or even <span class="math">\(s\)</span>, for short. This quantity does exactly what its name implies - it measures the <em>spread</em> of the residuals (those vertical distances between observations and the regression line). This value is in the output of the <code>lmGC</code> function.</p>
<p>However, we run into a problem when using the residual standard error to measure the variation in the plot. The residual standard error for the <strong>Height</strong> and <strong>RtSpan</strong> data from <code>pennstate1</code> is 3.148589.</p>
<pre class="r"><code>lmGC(Height~RtSpan,data=pennstate1)</code></pre>
<pre><code>## 
##  Linear Regression
## 
## Correlation coefficient r =  0.6314 
## 
## Equation of Regression Line:
## 
##   Height = 41.9593 + 1.2394 * RtSpan 
## 
## Residual Standard Error: s   = 3.1486 
## R^2 (unadjusted):        R^2 = 0.3987</code></pre>
<p>For this model, the unit of measure for height is inches. Consider what happens when we change this model by converting the unit of measure for height to feet. Take a look at the scatterplots and regression lines for these two models. See Figure[Different Units].</p>
<p><img src="ch4_files/figure-html/unitconversionmodels-1.png" alt="Different Units: Compare the scatterplot and regression line for RtSpan (cm) vs. Height (inches) and the scatterplot and regression line for RtSpan (cm) vs. Height (feet)." /> <img src="ch4_files/figure-html/unitconversionmodels-2.png" alt="Different Units: Compare the scatterplot and regression line for RtSpan (cm) vs. Height (inches) and the scatterplot and regression line for RtSpan (cm) vs. Height (feet)." /></p>
<p>This change in unit of the response variable does not affect the way the scatterplot of regression line looks. The regression line did an equally good job of fitting the data, in both scatterplots. It does, however, directly affect the value of the residuals. The residuals in the first plot have a much larger value than the residuals in the second plot. This causes the <em>spread</em> of the residuals (the residual standard error) to be bigger in the first plot. Hence, residual standard error is not the best way of measuring the variation accounted for by the regression line.</p>
<p>Another measure of the “explained variation” in the scatterplot is the <em>squared correlation</em>, <span class="math">\(r^2\)</span>. This also measures how well our regression line fits the data. However, it tells us the <em>proportion</em> of variation in the response variable that is explained by the explanatory variable. A change in unit (or scale) will not affect the value of <span class="math">\(r^2\)</span>.</p>
</div>
<div id="properties-of-r2" class="section level3">
<h3>Properties of <span class="math">\(r^2\)</span></h3>
<ul>
<li><span class="math">\(r^2\)</span> always has a value between 0 and 1.</li>
<li><span class="math">\(r^2=1\)</span> implies perfect linear association between explanatory and response variables.</li>
<li><span class="math">\(r^2=0\)</span> implies no linear association.</li>
</ul>
<p><strong>Beware</strong>: A low <span class="math">\(r^2\)</span> value does not necessarily mean that there is <em>no</em> relationship between the explanatory variable and the response. It might mean that a linear model is not an appropriate model! So, a low <span class="math">\(r^2\)</span> value means one of two things for us:</p>
<ul>
<li>There is a linear relationship between the variables, but there’s just alot of scatter in the data.</li>
<li>You might have the wrong model. In other words, the data might not follow a linear pattern. For example, consider the relationship we saw in the curvilinear <code>fuel</code> data from before. See Figure[R-squared for Nonlinear Data].</li>
</ul>
<div class="figure">
<img src="ch4_files/figure-html/rsquaredfuel-1.png" alt="R-squared for Nonlinear Data" /><p class="caption">R-squared for Nonlinear Data</p>
</div>
<pre><code>## 
##  Linear Regression
## 
## Correlation coefficient r =  -0.1716 
## 
## Equation of Regression Line:
## 
##   efficiency = 11.0579 + -0.0147 * speed 
## 
## Residual Standard Error: s   = 3.9047 
## R^2 (unadjusted):        R^2 = 0.0295</code></pre>
<p>Here, we get a small <span class="math">\(r^2\)</span> value. However, there is certainly a relationship between <strong>efficiency</strong> and <strong>speed</strong> - just not a <em>linear</em> one! It is always important that you visually examine the data before drawing conclusions based on certain statistics!</p>
</div>
</div>
</div>
<div id="cautions" class="section level1">
<h1>Cautions</h1>
<div id="extrapolation" class="section level2">
<h2>Extrapolation</h2>
<p>The regression line we found for predicting <strong>Height</strong> from <strong>RtSpan</strong> using the <code>pennstate1</code> dataset, <span class="math">\(\hat{y}=41.959349+1.239448x\)</span>. can be used for <em>interpolation</em>, i.e. predicting a height for a person that was not in the original dataset, but is within the range of right handspans covered by the dataset. However, it is inappropriate to use this regression line to predict a height for someone with a hand span that is outside of the range covered by the dataset.</p>
<dl>
<dt>Extrapolation</dt>
<dd><em>Extrapolation</em> is using a regression line to predict <span class="math">\(\hat{y}\)</span>-values for <span class="math">\(x\)</span>-values outside the range of observed <span class="math">\(x\)</span>-values.
</dd>
</dl>
<p>Sultan Kosen holds the Guiness World Record for the tallest living male. (His right handspan measures 30.48 cm, which is considerably bigger than all of the hand-spans in our dataset.) If we extrapolate using the regression line from this dataset, then we would predict his height to be <span class="math">\(\hat{y}=41.959349+1.239448\cdot 30.48=79.36\)</span> inches <span class="math">\(\approx\)</span> 6’ 7.5“. But he was really 8’ 3”! The prediction based on extrapolation was not very accurate.</p>
</div>
<div id="influential-observations" class="section level2">
<h2>Influential Observations</h2>
<dl>
<dt>Influential Observations</dt>
<dd><em>Influential observations</em> are observations which have a large effect on correlation and regression:
</dd>
</dl>
<ul>
<li>They have extreme <span class="math">\(x\)</span>-values.</li>
<li>They inflate or deflate correlation.</li>
<li>They affect the slope of the regression line.</li>
</ul>
<p>Investigate influential observations with the following app.</p>
<pre class="r"><code>require(manipulate)
Points2Watch()</code></pre>
</div>
<div id="association-versus-causation" class="section level2">
<h2>Association versus Causation</h2>
<p>Two variables having a high positive (or high negative) correlation between two variables suggests that there is a linear association that exists between them. However, correlation does <em>not</em> imply causation. In other words, strong correlation does not imply that one variable <em>causes</em> the other.</p>
<p>Consider the variables <strong>height</strong> and <strong>fastest</strong> in the <code>m111survey</code> dataset. The scatterplot and correlation coefficient suggest that taller people tend to have driven at a higher maximum speed. See Figure[Height/Fastest Scatterplot].</p>
<div class="figure">
<img src="ch4_files/figure-html/fastestheightxy-1.png" alt="Height/Fastest Scatterplot" /><p class="caption">Height/Fastest Scatterplot</p>
</div>
<pre class="r"><code>cor(height~fastest,data=m111survey,use=&quot;na.or.complete&quot;)</code></pre>
<pre><code>## [1] 0.1708742</code></pre>
<p>There a positive correlation between height and top speed. However, this does not necessarily mean that being tall <em>causes</em> you to drive faster. It’s certainly not the case that tall people can’t help but have a heavy foot! There may be a confounding variable that is (at least partially) responsible for the observed association. One possible confounder is <strong>sex</strong>. See Figure[Height/Fastest by Sex]. This suggests that males tend to drive faster than females. Since males also tend to be taller than females, this helps to explain the pattern we saw in the original scatterplot.</p>
<div class="figure">
<img src="ch4_files/figure-html/fastestheightxysex-1.png" alt="Height/Fastest by Sex: Grouping shows that sex may be a confounding variable that helps to explain why taller people tend to drive faster." /><p class="caption">Height/Fastest by Sex: Grouping shows that sex may be a confounding variable that helps to explain why taller people tend to drive faster.</p>
</div>
</div>
<div id="simpsons-paradox" class="section level2">
<h2>Simpson’s Paradox</h2>
<p><strong>Recall</strong>: Simpson’s Paradox occurs when the direction of the relationship between two variables is one way when you look at the aggregate data, but turns out the opposite way when you break up the data into subgroups based on a third variable. <em>You saw this back in Chapter 3 with two categorical variables!</em></p>
<p>Simpson’s Paradox can arise with two numerical variables as well!</p>
<p>Let’s look at a new dataset, <code>sat</code>.</p>
<pre class="r"><code>data(sat)
View(sat)
help(sat)</code></pre>
<p>We can describe the relationship between <strong>salary</strong> (mean annual teacher salary by state in $1000s) and <strong>sat</strong> (sum of mean Verbal and mean Math scores by state) using the correlation coefficient, scatterplot, and regression line. See Figure[SAT].</p>
<pre><code>## 
##  Linear Regression
## 
## Correlation coefficient r =  -0.4399 
## 
## Equation of Regression Line:
## 
##   sat = 1158.859 + -5.5396 * salary 
## 
## Residual Standard Error: s   = 67.8893 
## R^2 (unadjusted):        R^2 = 0.1935</code></pre>
<div class="figure">
<img src="ch4_files/figure-html/satanalysis-1.png" alt="SAT: Association between Average Annual Teacher Salary and Average Cumulative SAT Score (by state)" /><p class="caption">SAT: Association between Average Annual Teacher Salary and Average Cumulative SAT Score (by state)</p>
</div>
<p>Our regression analysis suggests that as teachers are paid higher salaries, the average SAT score drops. However, our intuition tells us that higher salaries would attract better teachers, who in turn would better prepare students for the SAT. What’s going on here?</p>
<p>Use the following app to investigate what happens when we take into account a third variable, <code>frac</code>. This is the percentage of students in the state who take the SAT.</p>
<pre class="r"><code>require(manipulate)
DtrellScat(sat~salary|frac,data=sat)</code></pre>
<p>We can see that within most subgroups, the slope of the regression line is positive, while the regression line for the overall data has a negative slope.</p>
<p>How can we explain this paradox?</p>
<p><strong>First Observation:</strong> It turns out that states where education is considered a high priority pay their teachers higher salaries. But since education is a high priority in these states, a high proportion of students in these states want to go to college, so a high proportion of them take the SAT. Similarly, states where education isn’t such an important part of the economy or the culture tend to pay their teachers less, and they also tend to have fewer students taking the SAT. So we’ve got a <strong>positive</strong> association between <strong>frac</strong> and <strong>salary</strong>.</p>
<p>Check this out in Figure[Frac/Salary Scatterplot].</p>
<div class="figure">
<img src="ch4_files/figure-html/salaryfracxy-1.png" alt="Frac/Salary Scatterplot: Percentage of students in the state that take the SAT vs. the average annual teacher salary." /><p class="caption">Frac/Salary Scatterplot: Percentage of students in the state that take the SAT vs. the average annual teacher salary.</p>
</div>
<p><strong>Second Observation</strong>: When a high percentage of students in a state take the SAT, this pool of test-takers is bound to include both the very strong students and those who are not so strong, so the mean SAT score is liable to be low. On the other hand, in states where fewer students take the SAT, the students who opt to take the test are likely to be highly motivated, definitely college-bound, very studious individuals. This elite pool tends to boost the mean SAT score for the state. So we’ve got a <strong>negative</strong> association between <strong>frac</strong> and <strong>sat</strong>.</p>
<p>Again, check this out in Figure[Frac/SAT Scatterplot].</p>
<div class="figure">
<img src="ch4_files/figure-html/satfracxy-1.png" alt="Frac/SAT Scatterplot: Percentage of students in the state that take the SAT vs. the average cumulative SAT score." /><p class="caption">Frac/SAT Scatterplot: Percentage of students in the state that take the SAT vs. the average cumulative SAT score.</p>
</div>
<p>Finally, put it together: the positive association between <strong>salary</strong> and <strong>frac</strong> and the negative association between <strong>frac</strong> and <strong>sat</strong> results in the negative association between <strong>salary</strong> and <strong>sat</strong>.</p>
<p>To put it in a nontechnical nutshell: states where education is important tend to pay their teachers well, but they also “handicap” themselves by encouraging most students (including the weaker ones) to take tests like the SAT. The handicap is so pronounced that higher values of <strong>salary</strong> tend to go along with lower values of <strong>sat</strong>.</p>
</div>
</div>
<div id="curvilinear-fits" class="section level1">
<h1>Curvilinear Fits</h1>
<p>Frequently we have brought up the idea that the relationship between <span class="math">\(x\)</span> and <span class="math">\(y\)</span> — our two numerical variables — might not be a linear one. Let’s look into this idea bit further.</p>
<p>Consider the data frame <code>henderson</code> from the <code>tigertats</code> package:</p>
<pre class="r"><code>data(henderson)
View(henderson)
help(henderson)</code></pre>
<p>Ricky Henderson, who played in the Major Leagues from 1979 to 2003, is widely considered to be one of the finest leadoff hitters in baseball history. The <code>henderson</code> data frame gives some fo his most important offensive statistics, by season. Let’s have a look at his slugging average (the average number of bases he got per at-bat) over the years (see Figure [Career Slugging]:</p>
<div class="figure">
<img src="ch4_files/figure-html/hendslugplot-1.png" alt="Slugging for Ricky Henderson, 1979-2001" /><p class="caption">Slugging for Ricky Henderson, 1979-2001</p>
</div>
<p>We have included a regression line, but as you can see the fit doesn’t look right at all: the points on the scatter plot don’t appear to follow a line. It’s not just that there is a lot of “scatter” around the regression line; instead it seems that some other pattern – not a linear one — is at work in determining how Ricky’s slugging average varied over the years.</p>
<p>One way to confirm our suspicion is to run a <em>check</em> on the linear fit, using the <code>check</code> parameter for <code>lmGC()</code>:</p>
<pre class="r"><code>lmGC(SLG~Season,data=henderson,check=TRUE)</code></pre>
<pre><code>## 
##  Linear Regression
## 
## Correlation coefficient r =  -0.2719 
## 
## Equation of Regression Line:
## 
##   SLG = 5.7685 + -0.0027 * Season 
## 
## Residual Standard Error: s   = 0.0661 
## R^2 (unadjusted):        R^2 = 0.0739</code></pre>
<div class="figure">
<img src="ch4_files/figure-html/hendsluglinecheck-1.png" />
</div>
<p>In addition to the usual linear model information, the scatter plot contains two new features:</p>
<ul>
<li>a <em>loess</em> curve;</li>
<li>an approximate 95%-confidence band around the loess curve.</li>
</ul>
<p>The term “loess” is short for “local estimation”: a loess curve is an attempt to use only the data itself to estimate the “real” deterministic part of the relationship between the <span class="math">\(x\)</span> and <span class="math">\(y\)</span> variables. In particular, it doesn’t assume that the relationship is a linear one! hence it wobbles around, following the general pattern of the point son the scatter plot.</p>
<p>The confidence band is the slightly shaded area that surrounds the loess curve. A rough-and-ready way to interpret the interval is to say that we are pretty confident that the “real” relationship between <strong>Season</strong> and <strong>SLG</strong> — if it could somehow be graphed on the scatter plot – would lie somewhere within the band. The loess curve is simply our best “data-based” guess at that unknown relationship.</p>
<p>Note that the regression line wanders a bit outside of the band, and is often quite near the edge of it: this is a good indication that the “real” relationship between <strong>Season</strong> and <strong>SLG</strong> is not a linear one.</p>
<p>So we have “checked” the linear fit, and found it wanting. Let’s use our common sense to consider what the “real” relationship might be like.</p>
<p>For many professional athletes, the first couple of years in the majors leagues are tough ones, as they go up against other top-of-the-line players. But after a while they adjust —if they don’t their career stats come to an end quickly! — and a relatively long “peak” period of play ensues. Eventually, though, age and injuries catch up with the athlete, and his/her performance begins to decline.</p>
<p>Hence the relationship between season of play and some performance measure such as slugging average ought to be a curvilinear one, with a rise and then a fall.</p>
<p>Rise-and-fall curvilinear relationships can be modeled mathematically by means of quadratic equations, which have the general form:</p>
<p><span class="math">\[y=ax^2+bx+c,\]</span></p>
<p>since these equations graph as parabolas. Let’s try to fit a second-degree curve (a parabola) to our data. This is accomplished using a new function, <code>polyfitGC()</code>. The function works like <code>lmGC()</code>, but has a new parameter <code>degree</code> to indicate the degree of the fitting curve:</p>
<ul>
<li><code>degree=2</code> fits a quadratic (a one-humped rise/fall or fall/rise parabola ) to the data;</li>
<li><code>degree=3</code> fits a cubic (as many as two “humps”“);</li>
<li><code>degree=4</code> fits a cubic (as many as three “humps”“);</li>
<li>and so on to higher degrees.</li>
</ul>
<p>Here’s the R-code for our quadratic fit:</p>
<pre class="r"><code>polyfitGC(SLG~Season,data=henderson,
          degree=2,graph=TRUE)</code></pre>
<p>The output to the console does not give the equation of the parabola, but we do get the residual standard error and the <span class="math">\(R^2\)</span> value. (Note that there is no correlation: <span class="math">\(r\)</span> doesn’t make sense outside the context of linear fits.)</p>
<p>The graph looks promising: the parabola does appear to run through the points on the scatter plot a lot better than the line did. Also note that the <span class="math">\(R^2\)</span> value is a good bit higher than it was with the linear fit (0.3825 as compared to 0.0739).</p>
<p>We can check this new fit as follows:</p>
<pre class="r"><code>polyfitGC(SLG~Season,data=henderson,
          degree=2,check=TRUE)</code></pre>
<pre><code>## Polynomial Regression, Degree = 2
## 
## Residual Standard Error: s   = 0.0553 
## R^2 (unadjusted):        R^2 = 0.3825</code></pre>
<div class="figure">
<img src="ch4_files/figure-html/hend2dcheck-1.png" alt="Checking Graph for the Quadratic Fit" /><p class="caption">Checking Graph for the Quadratic Fit</p>
</div>
<p>Examining Figure [Checking Graph], we see that the parabola stays well within the band at all times: our quadratic fit looks quite promising as a way to predict <span class="math">\(y\)</span> values from known <span class="math">\(x\)</span> values!</p>
<p>The the data go only up through the 2001 season, but Henderson played 72 games with the Boston Red Sox in 2002, and 30 games in 2003 for the LA Dodgers. Let’s use the parabola and the linear fit to predict his slugging averages for those two seasons.</p>
<p>First, the prediction based on the linear fit:</p>
<pre class="r"><code>linModel &lt;- lmGC(SLG~Season,data=henderson)
predict(linModel,x=2002)
predict(linModel,x=2003)</code></pre>
<p>Now the prediction based on the quadratic fit:</p>
<pre class="r"><code>quadModel &lt;- polyfitGC(SLG~Season,degree=2,data=henderson)
predict(quadModel,x=2002)
predict(quadModel,x=2003)</code></pre>
<p>Here are the predictions from each model, compared with Henderson’s actual statistics:</p>
<table>
<caption>Predicted/Actual Slugging Averages for R. Henderson</caption>
<thead>
<tr class="header">
<th align="center">Season</th>
<th align="center">Linear Prediction</th>
<th align="center">Quadratic Prediction</th>
<th align="center">Actual</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">2002</td>
<td align="center">0.386</td>
<td align="center">0.293</td>
<td align="center">0.352</td>
</tr>
<tr class="even">
<td align="center">2003</td>
<td align="center">0.383</td>
<td align="center">0.267</td>
<td align="center">0.306</td>
</tr>
</tbody>
</table>
<p>The regression line was better for 2002, but it falls too slowly: the parabola makes the better prediction for 2003. If we had to predict how Ricky would have done had he stayed on for the 2004 season, surely we would go with the prediction provided by the quadratic fit!</p>
</div>
<div id="thoughts-on-r" class="section level1">
<h1>Thoughts on R</h1>
<div id="new-r-functions" class="section level2">
<h2>New R Functions</h2>
<p>Know how to use these functions:</p>
<ul>
<li><code>xyplot()</code><br /></li>
<li><code>lmGC()</code></li>
<li><code>predict()</code></li>
<li><code>polyfitGC()</code></li>
</ul>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
