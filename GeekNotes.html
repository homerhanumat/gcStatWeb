<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<title>Appendix: Geek Notes</title>

<script src="GeekNotes_files/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link href="GeekNotes_files/bootstrap-2.3.2/css/bootstrap.min.css" rel="stylesheet" />
<link href="GeekNotes_files/bootstrap-2.3.2/css/bootstrap-responsive.min.css" rel="stylesheet" />
<script src="GeekNotes_files/bootstrap-2.3.2/js/bootstrap.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="GeekNotes_files/highlight/default.css"
      type="text/css" />
<script src="GeekNotes_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>


<link rel="stylesheet" href="css/coursenotes.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
</style>
<div class="container-fluid main-container">


<div id="header">
<h1 class="title">Appendix: Geek Notes</h1>
</div>

<div id="TOC">
<ul>
<li><a href="#chapter-2">Chapter 2</a><ul>
<li><a href="#more-on-structure">More on Structure</a></li>
<li><a href="#fancier-histograms">Fancier Histograms</a></li>
<li><a href="#combined-plots">Combined Plots</a></li>
<li><a href="#adding-rugs">Adding Rugs</a></li>
<li><a href="#tuning-density-plots">Tuning Density Plots</a></li>
<li><a href="#more-on-standard-deviation">More on Standard Deviation</a></li>
</ul></li>
<li><a href="#chapter-2-1">Chapter 2</a><ul>
<li><a href="#cleveland-dotplots">Cleveland Dotplots</a></li>
</ul></li>
<li><a href="#chapter-3">Chapter 3</a><ul>
<li><a href="#fixed-and-random-effects-in-simulation">Fixed and Random effects in Simulation</a></li>
</ul></li>
<li><a href="#chapter-4">Chapter 4</a><ul>
<li><a href="#point-shapes-in-scatterplots-using-pch">Point Shapes in Scatterplots using <code>pch</code></a></li>
<li><a href="#scatterplot-matrix">Scatterplot Matrix</a></li>
<li><a href="#the-rationale-for-values-of-the-correlation-coefficient-r">The Rationale for Values of the Correlation Coefficient, <span class="math">\(r\)</span></a></li>
<li><a href="#computation-of-the-coefficients-in-the-regression-equation">Computation of the Coefficients in the Regression Equation</a></li>
</ul></li>
<li><a href="#chapter-5">Chapter 5</a><ul>
<li><a href="#the-rbind-function">The <code>rbind</code> Function</a></li>
<li><a href="#the-cbind-function">The <code>cbind</code> Function</a></li>
</ul></li>
<li><a href="#chapter-6">Chapter 6</a><ul>
<li><a href="#the-role-of-limits-in-density-plots">The Role of Limits in Density Plots</a></li>
<li><a href="#more-about-legends">More about Legends</a></li>
<li><a href="#more-on-strip-plots">More on Strip-plots</a></li>
<li><a href="#assessing-statistical-significance">Assessing Statistical Significance</a></li>
<li><a href="#interaction">Interaction</a></li>
</ul></li>
<li><a href="#chapter-8">Chapter 8</a><ul>
<li><a href="#we-lied-about-the-sd-formulas">We Lied About the SD Formulas!</a></li>
<li><a href="#are-we-only-ever-interested-in-population-parameters">Are We Only Ever Interested in Population Parameters?</a></li>
</ul></li>
<li><a href="#chapter-9">Chapter 9</a><ul>
<li><a href="#distinction-between-t-and-z-in-confidence-intervals-for-means">Distinction Between <span class="math">\(t\)</span> and <span class="math">\(z\)</span> in Confidence Intervals for Means</a></li>
<li><a href="#how-does-r-find-df">How Does R Find <span class="math">\(df\)</span>?</a></li>
</ul></li>
</ul>
</div>

<div id="chapter-2" class="section level1">
<h1>Chapter 2</h1>
<div id="more-on-structure" class="section level2">
<h2>More on Structure</h2>
<p>Everything in R is an object. Every object has a structure. In FDN 111 , we learn that the structure of an object consists of its parts and the way that the parts relate together. R can show us the structure of an object using the <strong>str</strong> function. We have already seen this for data frames:</p>
<pre class="r"><code>str(m111survey)</code></pre>
<pre><code>## &#39;data.frame&#39;:    71 obs. of  12 variables:
##  $ height         : num  76 74 64 62 72 70.8 70 79 59 67 ...
##  $ ideal_ht       : num  78 76 NA 65 72 NA 72 76 61 67 ...
##  $ sleep          : num  9.5 7 9 7 8 10 4 6 7 7 ...
##  $ fastest        : int  119 110 85 100 95 100 85 160 90 90 ...
##  $ weight_feel    : Factor w/ 3 levels &quot;1_underweight&quot;,..: 1 2 2 1 1 3 2 2 2 3 ...
##  $ love_first     : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ extra_life     : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 1 2 1 2 2 2 1 ...
##  $ seat           : Factor w/ 3 levels &quot;1_front&quot;,&quot;2_middle&quot;,..: 1 2 2 1 3 1 1 3 3 2 ...
##  $ GPA            : num  3.56 2.5 3.8 3.5 3.2 3.1 3.68 2.7 2.8 NA ...
##  $ enough_Sleep   : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 2 1 2 1 2 ...
##  $ sex            : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 1 1 2 2 2 2 1 1 ...
##  $ diff.ideal.act.: num  2 2 NA 3 0 NA 2 -3 2 0 ...</code></pre>
<p>The parts of a data frame are the variables. The way they relate together to make an actual data frame is that all have the same length (71 in this case). This allows R to combine the variable in columns, and to interpret the rows as individuals.</p>
<p>You can think of a data frame as being like a book. The “chapters” of the book are the variables.</p>
<p>If a data frame is like a book, then a package, such as tigerstats, is a like collection of books. The authors of R must take this analogy pretty seriously, because one way to load is package is as follows:</p>
<pre class="r"><code>library(tigerstats)</code></pre>
<p>The <code>library()</code> function takes all of the books in <code>tigerstats</code> out of storage and puts them on the shelves of R’s library.</p>
<p>Just like you, R is a reader, so R reads for structure, too. Look at the following code (and see Figure [A Simple Histogram] for the results):</p>
<pre class="r"><code>histogram(~fastest, data=m111survey)</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekhistfastest-1.png" alt="A Simple Histogram." /><p class="caption">A Simple Histogram.</p>
</div>
<p>You can think of the code as saying to R: “Put on your <code>histogram()</code> glasses. Then take up the book named <code>m111survey</code>, turn to chapter <strong>fastest</strong>, and read that chapter with your <code>histogram()</code> glasses.”</p>
<p>When R gets interprets that code, it “reads” <strong>fastest</strong> with histogram glasses. It can do so because of the structure of fastest:</p>
<pre class="r"><code>str(m111survey$fastest)</code></pre>
<pre><code>##  int [1:71] 119 110 85 100 95 100 85 160 90 90 ...</code></pre>
<p>R sees that is <strong>fastest</strong> is a numerical vector. It can use histogram glasses to read that vector and produce the histogram you see on the screen.</p>
<p>Suppose you were to ask R to make a histogram of <strong>sex</strong>. The result appears in Figure [Bad Histogram].</p>
<pre class="r"><code>histogram(~sex,data=m111survey)</code></pre>
<pre><code>## Warning in mean.default(evalF$right[, 1], ...): argument is not numeric or
## logical: returning NA</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekhistsex-1.png" alt="Bad Histogram. You should not try to make a histogram from a factor variable." /><p class="caption">Bad Histogram. You should not try to make a histogram from a factor variable.</p>
</div>
<p>You don’t get a histogram; you get something that looks like a cross between a density histrogram and a barchart. R was programmed to look at the structure of the input variable. If it’s a factor rather than a numerical vector and a histogram was requested, then R looks turns the factor into a numerical variable, as best it can. In this case, “female” was turned into a 1 and “male” was turned into a 2. The rectangle over female extends from 0.5 to 1.5, and the recangle over “male” extends from 1.5 to 2.5 Very kindly, R prints the values “female” and “male”, rather than 1 and 2, so it’s doing the best it can to give you a pair of “glasses” through which you can read the data.</p>
<p>We said that everything in R is an object, and every object has a structure. Therefore, even graphs have a structure. Try this:</p>
<pre class="r"><code>FastHist &lt;- histogram(~fastest,data=m111survey,
                      main=&quot;Fastest Speed Ever Driven&quot;,
                      xlab=&quot;speed in mph&quot;,
                      type=&quot;density&quot;)</code></pre>
<p>Where’s the graph? Well, we didn’t ask for it to go the screen; instead we asked for it to be stored as an object named <code>FastHist</code>. Let’s look at the structure of the object:</p>
<pre class="r"><code>str(FastHist)</code></pre>
<p>Run the chunk above. It’s an enormous list (of 45 items). When you look through it, you see that it appears to contains the information need to build a histogram.</p>
<p>The “building” occurs when we `<code>print()</code> the object:</p>
<pre class="r"><code>print(FastHist)</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekfasthiststr-1.png" alt="Now we get the histogram." /><p class="caption">Now we get the histogram.</p>
</div>
<p>The <code>print()</code> function uses the information in <code>FastHist</code> to produce the histogram you see on in Figure [Now we get the histogram]. (When you ask for a histogram directly, you are actually asking R to print the histogram object created by the <code>histogram()</code> function.)</p>
<p>Of course when we read a histogram, we usually read the one we see on the screen, so we think of its structure differently than R does. In general, we think of the structure of a graph as:</p>
<ul>
<li>the axes</li>
<li>the panel (the part that is enclosed by the axes)</li>
<li>the annotations (title, axis labels, legend, etc.)</li>
</ul>
</div>
<div id="fancier-histograms" class="section level2">
<h2>Fancier Histograms</h2>
<p>In a density histogram, it can make a lot of sense to let the rectangles have different widths. For example, look at the tornado damage amounts in <code>tornado</code>:</p>
<pre class="r"><code>histogram(~damage,data=tornado,
           main=&quot;Average Annual Tornado\nDamage, by State&quot;,
           xlab=&quot;Damage in Millions of Dollars&quot;,
            type=&quot;density&quot;)</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geektornado1-1.png" alt="Tornado damge, with default breaks. All rectangles have the same width." /><p class="caption">Tornado damge, with default breaks. All rectangles have the same width.</p>
</div>
<p>The distribution (see Figure [Tornado damge, with default breaks]) is very right-skewed, but most of the states suffered very little damage. Let’s get a finer-grained picture of these states by picking our own breaks:</p>
<pre class="r"><code>data(tornado)
histogram(~damage,data=tornado,
           main=&quot;Average Annual Tornado\nDamage, by State&quot;,
           xlab=&quot;Damage in Millions of Dollars&quot;,
            type=&quot;density&quot;,
            breaks=c(0,2,4,6,10,15,20,25,30,40,50,60,70,80,90,100))</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geektornado2-1.png" alt="Tornado damage, with customized rectangles." /><p class="caption">Tornado damage, with customized rectangles.</p>
</div>
<p>Figure [Tornado damage, with customized rectangles] shows the result. You should play around with the sequence of breaks, to find one that “tells the story” of the data well.</p>
</div>
<div id="combined-plots" class="section level2">
<h2>Combined Plots</h2>
<p>If you would like to make violin plot combined with a box-and-whisker plot, here is how to do it:</p>
<pre class="r"><code>bwplot(GPA~seat,data=m111survey,
       main=&quot;Grade Point Average,\nby Seating Preference&quot;,
       xlab=&quot;Seating Preference&quot;,
       ylab=&quot;GPA&quot;,
       panel = function(box.ratio,...) {
                  panel.violin(..., col = &quot;bisque&quot;,
                               from=0,to=4)
                  panel.bwplot(..., box.ratio = 0.1)
                })</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekgpaseatviolin2-1.png" alt="Combined Plot. Box-and-Whisker plots combined with violin plots are very cool." /><p class="caption">Combined Plot. Box-and-Whisker plots combined with violin plots are very cool.</p>
</div>
<p>The result is shown in Figure [Combined Plot].</p>
<p>In order to get more than one graph into the “panel” area of a plot, you modify something called the “panel” function. In advanced courses (or own your own) you canlearn more about how R’s graphics systems work, but for now just try copying and modifying the code you see in the Course Notes.</p>
</div>
<div id="adding-rugs" class="section level2">
<h2>Adding Rugs</h2>
<p>Adding the argument <code>panel.rug</code> to the panel function gives a “rug” of individual data values along the x-axis.</p>
<pre class="r"><code>bwplot(~damage,data=tornado,
           main=&quot;Average Annual Tornado\nDamage, by State&quot;,
           xlab=&quot;Damage in Millions of Dollars&quot;,
           panel=function(x,...) {
             panel.violin(x,col=&quot;bisque&quot;,...)
             panel.bwplot(x,...)
             panel.rug(x,col=&quot;red&quot;,...)
            }
          )</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekbwrug-1.png" alt="Damage with Rug. We added a rug to this plot." /><p class="caption">Damage with Rug. We added a ‘rug’ to this plot.</p>
</div>
<p>The result appears in Figure [Damage with Rug].</p>
</div>
<div id="tuning-density-plots" class="section level2">
<h2>Tuning Density Plots</h2>
<p>Adding a list of <em>density arguments</em> fine tunes features of the density plot. For example, <code>bw</code> specifies how “wiggly” the plot will be; <code>from</code> and <code>to</code> tell R where to begin and end estimation of the density curve.</p>
<p>Here is an example of what can be done (see Figure [Setting Bandwidth] for the results):</p>
<pre class="r"><code>histogram(~damage,data=tornado,
           main=&quot;Average Annual Tornado\nDamage, by State&quot;,
           xlab=&quot;Damage in Millions of Dollars&quot;,
           type=&quot;density&quot;,
           breaks=c(0,2,4,6,10,15,20,25,30,40,50,60,70,80,90,100),
           panel=function(x,...) {
             panel.histogram(x,...)
             panel.rug(x,col=&quot;red&quot;,...)
             panel.densityplot(x,col=&quot;blue&quot;,
                        darg=list(bw=3,from=0,to=100),...)
            }
          )</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekhisttuning-1.png" alt="Setting Bandwidth. The bandwidth of the density plot was set to 3." /><p class="caption">Setting Bandwidth. The bandwidth of the density plot was set to 3.</p>
</div>
<p>R constructs a density plot by combining lots of little bell-shaped curves (called <em>kernals</em>), one centered at each point in the data. The bandwidth <code>bw</code> tells R how spread out these kernals should be: the bigger the bandwidth, the shorter and wider the kernal, and the stiffer the density curve will be. With a small bandwidth, the kernals are skinny and tall, giving the density plot a wiggly appearance, especially near isolated data points.</p>
<p>How do you know what the bandwidth should be? For now, you just have to try various values. The following <code>manipulate()</code> app helps you experiment with different values of the bandwidth.</p>
<pre class="r"><code>require(manipulate)
manipulate(
  bandwidth=slider(0.5,20,init=5,label=&quot;Bandwidth (1 = wiggly, 20 = stiff)&quot;),
  histogram(~damage,data=tornado,
           main=&quot;Average Annual Tornado\nDamage, by State&quot;,
           xlab=&quot;Damage in Millions of Dollars&quot;,
           type=&quot;density&quot;,
           breaks=c(0,2,4,6,10,15,20,25,30,40,50,60,70,80,90,100),
           panel=function(x,...) {
             panel.histogram(x,...)
             panel.rug(x,col=&quot;red&quot;,...)
             panel.densityplot(x,col=&quot;blue&quot;,
                        darg=list(bw=bandwidth,from=0,to=100),...)
            }
          )
)</code></pre>
<p>When the bandwidth is set too low, the wiggles in the density plot are too sensitive to chance clusters of data points – clusters that probably would not appear in the same place in a repeated study. When the bandwidth is set too high, the density plot is not able to capture the overall shape of the distribution.</p>
</div>
<div id="more-on-standard-deviation" class="section level2">
<h2>More on Standard Deviation</h2>
<p>Recall that when we compute the sample standard deviation, we don’t quite average the squared deviations. Instead, we divide by one less than the number of data values:</p>
<p><span class="math">\[s = \sqrt{(\sum{(x_i - \bar{x})^2})/(n-1)}.\]</span></p>
<p>What if we have the entire population? Then the SD is called <span class="math">\(\sigma\)</span>, and it is computed like this:</p>
<p><span class="math">\[\sigma = \sqrt{(\sum{(x_i - \mu)^2})/N},\]</span></p>
<p>where <span class="math">\(\mu\)</span> is the mean of the population and <span class="math">\(N\)</span> is the number of individuals in the population. So you might well ask: “Why do we divide by one less than the number of items when we have a sample, but not when we have the entire population?”</p>
<p>To answer this, we first have to back up to the idea of variance. The sample variance is:</p>
<p><span class="math">\[s^2 = \frac{\sum{(x_i - \bar{x})^2}} {n-1},\]</span></p>
<p>and the population variance is</p>
<p><span class="math">\[\sigma^2 = \frac{\sum{(x_i - \mu)^2}} {N}.\]</span></p>
<p>The formula for the population variance makes perfect sense. Although the <span class="math">\(n-1\)</span> in the formula for sample variance does not appear to make good sense, it has been cleverly designed so that the sample variance will be a good estimate of the population variance.</p>
<p>What do we mean by “good estimate”? Let’s suppose that a statistician wants to estimate the variance of the heights of <code>imagpop</code>, but she only has time to take a sample of size, say, <span class="math">\(n=4\)</span>. Unknown to her the population variance is:</p>
<pre class="r"><code>sigmasq &lt;- var(imagpop$height)*(9999/10000)
sigmasq</code></pre>
<pre><code>## [1] 15.26323</code></pre>
<p>Her sample might, on the other hand, might look like this:</p>
<pre class="r"><code>HerSamp &lt;- popsamp(n=4,pop=imagpop)
HerSamp</code></pre>
<pre><code>##         sex math income cappun height idealheight diff kkardashtemp
## 3109 female   no   6900  favor   60.1          62  1.9           41
## 9924 female   no  10100  favor   69.2          71  1.8           32
## 9608 female  yes  25800 oppose   63.3          65  1.7            2
## 4567 female   no  18900 oppose   72.7          74  1.3           25</code></pre>
<p>Then her estimate of the variance would be</p>
<pre class="r"><code>var(HerSamp$height)</code></pre>
<pre><code>## [1] 32.26917</code></pre>
<p>Her estimate might be high or low: it depends on the luck of the draw. But suppose that many, many statisticians (10,000 of them, let’s say) were to each take a random sample of size 4 and compute the sample variance. Then the results would be like this:</p>
<pre class="r"><code>SimVars &lt;- do(10000)*var(popsamp(n=4,pop=imagpop)$height)</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<p>Individually, their estimates would be all over the place (see Figure[Variance Estimates] for the plot:</p>
<pre class="r"><code>head(SimVars,n=10)</code></pre>
<pre><code>##       result
## 1   3.355833
## 2   4.182500
## 3  17.455833
## 4  23.082500
## 5  55.829167
## 6   7.126667
## 7  41.469167
## 8   8.853333
## 9  30.962500
## 10 14.440000</code></pre>
<pre class="r"><code>histogram(~SimVars$result)</code></pre>
<p><img src="GeekNotes_files/figure-html/geekhistsimvars-1.png" alt="Variance Estimates." /> But on average, they would get:</p>
<pre class="r"><code>mean(SimVars$result)</code></pre>
<pre><code>## [1] 15.30223</code></pre>
<p>Notice that this about the same as the population variance <span class="math">\(\sigma^2 = 15.2632308\)</span>.</p>
<p>On average, over many, many samples, the sample variance equals the population variance. We say that the sample variance is an <em>unbiased</em> estimator of the population variance. On the other hand, if the statisticians were to compute the sample variance by dividing by <span class="math">\(n=4\)</span> instead of dividing by <span class="math">\(n-1=3\)</span>, then they would get results that are, on average, <em>too small</em>:</p>
<pre class="r"><code>BadVars &lt;- SimVars$result*3/4  #so that there is now a 4 on the bottom
mean(BadVars)</code></pre>
<pre><code>## [1] 11.47667</code></pre>
<p>Sure enough, the results, on average are only about 3/4th the size of the true <span class="math">\(\sigma^2\)</span>. Dividing by <span class="math">\(n\)</span> in the sample variance would give you a biased estimator of population variance!</p>
</div>
</div>
<div id="chapter-2-1" class="section level1">
<h1>Chapter 2</h1>
<div id="cleveland-dotplots" class="section level2">
<h2>Cleveland Dotplots</h2>
<p>Barcharts are very popular for investigating categorical variables, but modern statisticians believe that the <em>Cleveland dot plot</em> is more useful in most situations.</p>
<pre class="r"><code>SexSeatrp &lt;-100*prop.table(xtabs(~seat+sex,data=m111survey),margin=1)
dotplot(SexSeatrp,groups=FALSE,horizontal=FALSE,type=c(&quot;p&quot;,&quot;h&quot;),
        ylab=&quot;Percent&quot;,xlab=&quot;Feeling About Weight&quot;,
        main=&quot;Feeling About Weight, by Sex&quot;)</code></pre>
<p><img src="GeekNotes_files/figure-html/geekcleveland1-1.png" alt="Cleveland Plot." /> The resulting plot appears as Figure [Cleveland Plot]. The first line of code above constructs a twoway table and computes row percentages for it, using the <code>prop.table()</code> function to prevent having to deal with the extraneous column of total percentages. Note that in the twoway table the explanatory variable comes second. Reverse the order to see the effect on the layout of the plot.</p>
<p>The second line constructs the dot plot itself. Whereas barcharts indicate percentages by the tops of rectangles, the Cleveland dot plot uses points. Setting the <code>type</code> argument to <code>c(&quot;p&quot;,&quot;h&quot;)</code> indicates that we want points, but also lines extending to the points. The lines are helpful, as the human eye is good at comparing relative lengths of side-by-side segments. The <code>groups</code> argument is FALSE by default; we include it here to emphasize how the plot will change when it is set to TRUE, as in the next example. The results appears in Figure [Cleveland Plot 2].</p>
<pre class="r"><code>dotplot(SexSeatrp,groups=TRUE,horizontal=FALSE,type=c(&quot;p&quot;,&quot;o&quot;),
        ylab=&quot;Proportion&quot;,xlab=&quot;Feeling About Weight&quot;,
        auto.key=list(space=&quot;right&quot;,title=&quot;Sex&quot;,lines=TRUE),
        main=&quot;Feeling About Weight, by Sex&quot;)</code></pre>
<p><img src="GeekNotes_files/figure-html/geekcleveland2-1.png" alt="Cleveland Plot 2. We added lines extending to the dots." /> Setting <code>groups</code> to TRUE puts both sexes in the same panel. Setting <code>type=c(&quot;p&quot;,&quot;o&quot;)</code> produces the points, with points in the same group connected by line segments. The <code>lines</code> argument in <code>auto.key</code> calls for lines as well as points to appear in the legend.</p>
</div>
</div>
<div id="chapter-3" class="section level1">
<h1>Chapter 3</h1>
<div id="fixed-and-random-effects-in-simulation" class="section level2">
<h2>Fixed and Random effects in Simulation</h2>
<p>When we used the ChisqSimSlow apps during the ledgejump study, we set the <code>effects</code> argument to “fixed.” Later on, in the <strong>sex</strong> and <strong>seat</strong> study, we set <code>effects</code> to “random”. What was all that about?</p>
<p>Try the ChisqSimSlow app in the ledgejump study again, and this time pay careful attention to each twoway table as it appears.</p>
<pre class="r"><code>require(manipulate)
ChisqSimSlow(~weather+crowd.behavior,data=ledgejump, effects=&quot;fixed&quot;)</code></pre>
<p>Now try it again, but this time with <code>effects</code> set to “random”:</p>
<pre class="r"><code>require(manipulate)
ChisqSimSlow(~weather+crowd.behavior,data=ledgejump, effects=&quot;random&quot;)</code></pre>
<p>You might notice that when effects are fixed, the number of cool-weather days is always 9, and the number of warm-weather days is always 12, just as in the original data. On the other hand, when effects are random, although the total number of incidents stays constant at 21, the division of them into cool and warm days varies from one resample to another.</p>
<p>In the ledgejump study, the 21 incidents could not reasonably be regarded as a random sample from some larger “population” of incidents. Most likely, the researcher included in his study all of the incidents for which he could determine the relevant information about weather and crowd behavior. This isn’t a <em>random</em> sample from among all incidents. Therefore, there is no randomness involved in how many warm-weather and how many cool-weather incidents were involved: if we could go back in time and watch these incidents play out again, 9 of them would still have been in warm weather, and 12 would have been in cool weather.</p>
<p>But chance <em>is</em> still involved: in the determination of the value of the response variable. In each incident, factors not associated with the random variable are at play. Such factors – the personalities of the people in the crowd, the length of time the would-be jumper stood on the ledge, etc. – are modeled as “chance” and these chance factors help determine whether the crowd is baiting or polite. Recall that if weather and crowd behavior were unrelated, then our best guess was that for each incident there was a 52.4% chance that the crowd would be polite and a 47.6% chance that it would be baiting. In the resampling with fixed effects, there are 9 cool-weather incidents and 12 warm-weather ones, and each incident is given a 52.4% chance to have a polite crowd.</p>
<p>On the other hand, if our twoway table is based on a random sample from a larger population, as the <strong>sex</strong> and <strong>seat</strong> study was, then we say that the effects are <em>random</em>. In the original sex-seat sample, there were 71 individuals: 40 females and 31 males. If we were to repeat the sample again, we would not be guaranteed to have 40 females and 31 males in it. Our best guess, though, based on our sample, is that <span class="math">\(\frac{40}{71} \times 100 = 56.3\)</span>% of the population is female, so in the resampling with random effects, we give each individual a 56.3% chance to be female. Since the resampling is done under the hypothesis that sex and seat are related, the chances for each resample-individual to prefer front, back and middle are the same, regardless of whether the individual is female or male.</p>
<p>Just as the two methods of resampling differ mathematically, so they also differ in the nature and scope of our conclusion in Step Five. In the ledgejump study, fixed effects resampling models the assumption that the 21 incidents themselves would have been the same from sample to sample: the only thing that varies with chance is how the crowd behaves in each incident. Hence your conclusion in Step Five – that the sample data don’t quite provide strong evidence for a relationship between weather and crowd behavior – applies only to those 21 incidents. In the <strong>sex</strong> and <strong>seat</strong> study, on the other hand, the random-effects resampling method models the assumption that the the 71 GC students were a random sample from the larger population of all GC students. The conclusions we draw from this data apply to this larger population.</p>
<p>When we set <em>simulate.p.value</em> to TRUE in <code>chisq.testGC</code>, R does resampling. However, it takes a third approach: the the row sums (tallies of the various values of the X variable) are fixed, as in our fixed effects, but the column sums are also fixed to be the same as the column sums of the original data. In our terminology, you could say the resampling is “double-fixed.” R has its own reasons for the double-fixed approach that we will not cover here.</p>
<p>If you want to fixed effects simulations, just use set the <code>simulate.p.value</code> argument in <code>chisq.testGC()</code> to “fixed”. For random effects, set the argument to “random”.</p>
<p>Be assured that, as sample size increases, all three methods — fixed, random and double-fixed — yield approximations that agree more and more nearly with each other. At small sample sizes, though, they can differ by a few percentage points.</p>
<p><strong>Note to Instructors</strong>: Our use of the terms “fixed effects” and “random effects” is not quite standard, but is analogous to the use of these terms in mixed-effects linear modeling.</p>
</div>
</div>
<div id="chapter-4" class="section level1">
<h1>Chapter 4</h1>
<div id="point-shapes-in-scatterplots-using-pch" class="section level2">
<h2>Point Shapes in Scatterplots using <code>pch</code></h2>
<p>The plot character, <code>pch</code>, is an integer between 1 and 25 that controls how the points on your scatterplot appear. A summary can be seen in Figure[Plot Characters].</p>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekpch-1.png" alt="Plot Characters." /><p class="caption">Plot Characters.</p>
</div>
</div>
<div id="scatterplot-matrix" class="section level2">
<h2>Scatterplot Matrix</h2>
<p>Given several numerical variables, R can produce a group of scatterplots, one for each pair of variables – all in one graph. Such a graph is called a <em>scatterplot matrix</em>. We can create a matrix of scatterplots using the following <code>pairs</code> function in R. You only need to enter in the variables that you want plotted and the dataset that contains them. R will create a square matrix of plots for every combination of variables. See Figure<a href="#scatterplot-matrix">Scatterplot Matrix</a>.</p>
<pre class="r"><code>pairs(~height+sleep+GPA+fastest,data=m111survey)  </code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekxymatrix-1.png" alt="Scatterplot Matrix." /><p class="caption">Scatterplot Matrix.</p>
</div>
<p>Of course, you can always make this look nicer by changing the colors and plot characters. Notice that the scatterplots are arranged in a somewhat symmetric way across the main diagonal (the boxes with the variable names). A scatterplots mirror image uses the same variables, but the explanatory and response variables are reversed.</p>
<p>You can also plot only the upper (or lower) panels. See Figure[Upper Panel] and Figure[LowerPanel].</p>
<pre class="r"><code>pairs(~height+sleep+GPA+fastest,data=m111survey,pch=19,col=&quot;red&quot;,lower.panel=NULL)</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekxymatrixupper-1.png" alt="Upper Panel. Scatterplot matrix showing only the upper panel of scatterplots." /><p class="caption">Upper Panel. Scatterplot matrix showing only the upper panel of scatterplots.</p>
</div>
<pre class="r"><code>pairs(~height+sleep+GPA+fastest,data=m111survey,pch=19,col=&quot;red&quot;,upper.panel=NULL)</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekxymatrixlower-1.png" alt="Lower Panel. Scatterplot matrix showing only the lower panel of scatterplots." /><p class="caption">Lower Panel. Scatterplot matrix showing only the lower panel of scatterplots.</p>
</div>
</div>
<div id="the-rationale-for-values-of-the-correlation-coefficient-r" class="section level2">
<h2>The Rationale for Values of the Correlation Coefficient, <span class="math">\(r\)</span></h2>
<p>Let’s consider why variables with a positive linear association also have a positive correlation coefficient, <span class="math">\(r\)</span>. Consider what value of <span class="math">\(r\)</span> you might <em>expect</em> for <strong>positively correlated</strong> variables. Let’s recall how we plotted the two “mean” lines to break a scatterplot into four “boxes”. See Figure[Four Boxes].</p>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekfourboxesxyhandheight-1.png" alt="Four Boxes. Scatterplot of Right Handspan (cm) versus Height (in). The lines marking the mean of the handspans and the mean of the heights have been plotted to break the scatterplot into four boxes." /><p class="caption">Four Boxes. Scatterplot of Right Handspan (cm) versus Height (in). The lines marking the mean of the handspans and the mean of the heights have been plotted to break the scatterplot into four boxes.</p>
</div>
<p>We’ve looked at this scatterplot before, and determined that it indicates a positive association between <strong>RtSpan</strong> and <strong>Height</strong>. Now, let’s think carefully about how the points in the scatterplot contribute to the value of <span class="math">\(r\)</span>. Check out the formula again:</p>
<p><span class="math">\[ r=\frac{1}{n-1}\sum{\bigg(\frac{x_i-\bar{x}}{s_x}\bigg)\bigg(\frac{y_i-\bar{y}}{s_y}\bigg)}  \]</span></p>
<ul>
<li><p>When an <span class="math">\(x\)</span>-value lies <em>above</em> the mean of the <span class="math">\(x\)</span>’s, it’s <span class="math">\(z\)</span>-score is <strong>positive</strong>. Likewise, a <span class="math">\(y\)</span>-value that lies <em>above</em> the mean of the <span class="math">\(y\)</span>’s has a <strong>positive</strong> <span class="math">\(z\)</span>-score. Every ordered pair in the upper right box has an <span class="math">\(x\)</span> and <span class="math">\(y\)</span>-coordinate with <strong>positive</strong> <span class="math">\(z\)</span>-scores. Multiplying 2 positive <span class="math">\(z\)</span>-scores together gives us a <strong>positive</strong> number. So, every point in the upper right box contributes a positive number to the sum in the formula for <span class="math">\(r\)</span>.</p></li>
<li><p>When an <span class="math">\(x\)</span>-value lies <em>below</em> the mean of the <span class="math">\(x\)</span>’s, it’s <span class="math">\(z\)</span>-score is <strong>negative</strong>. Likewise for <span class="math">\(y\)</span>. Every ordered pair in the lower right box has an <span class="math">\(x\)</span> and <span class="math">\(y\)</span>-coordinate with <strong>negative</strong> <span class="math">\(z\)</span>-scores. Multiplying 2 negative <span class="math">\(z\)</span>-scores together gives us a <strong>positive</strong> number. So, every point in the lower left box has a positive contribution to the value of <span class="math">\(r\)</span>.</p></li>
</ul>
<p>Following the same rationale, the points in the upper left box and lower right box will contribute negative numbers to the sum of <span class="math">\(r\)</span>.</p>
<ul>
<li><p>When an <span class="math">\(x\)</span>-value lies <em>above</em> the mean of the <span class="math">\(x\)</span>’s, it’s <span class="math">\(z\)</span>-score is <strong>positive</strong>. A <span class="math">\(y\)</span>-value that lies <em>below</em> the mean of the <span class="math">\(y\)</span>’s has a <strong>negative</strong> <span class="math">\(z\)</span>-score. Every ordered pair in the lower right box has an <span class="math">\(x\)</span>-coordinate with a <strong>positive</strong> <span class="math">\(z\)</span>-score and a <span class="math">\(y\)</span>-coordinate with a <strong>negative</strong> <span class="math">\(z\)</span>-score. Multiplying a positive and a negative <span class="math">\(z\)</span>-score together gives us a <strong>negative</strong> number. So, every point in the lower right box contributes a negative number to the sum in the formula for <span class="math">\(r\)</span>.</p></li>
<li><p>When an <span class="math">\(x\)</span>-value lies <em>below</em> the mean of the <span class="math">\(x\)</span>’s, it’s <span class="math">\(z\)</span>-score is <strong>negative</strong>. A <span class="math">\(y\)</span>-value that lies <em>above</em> the mean of the <span class="math">\(y\)</span>’s has a <strong>positive</strong> <span class="math">\(z\)</span>-score. Every ordered pair in the upper left box has an <span class="math">\(x\)</span>-coordinate with a <strong>negative</strong> <span class="math">\(z\)</span>-score and a <span class="math">\(y\)</span>-coordinate with a <strong>positive</strong> <span class="math">\(z\)</span>-score. Multiplying a positive and a negative <span class="math">\(z\)</span>-score together gives us a <strong>negative</strong> number. So, every point in the upper left box contributes a negative number to the sum in the formula for <span class="math">\(r\)</span>.</p></li>
</ul>
<p>Since <strong>positively associated</strong> variables have <em>most</em> of their points in the upper right and lower left boxes, <em>most</em> of the numbers being contributed to the summation are <strong>positive</strong>. There are some negative numbers contributed from the points in the other boxes, but not nearly as many. When these values are summed, we end up with a <strong>positive</strong> number for <span class="math">\(r\)</span>. So we say that these variables are <strong>positively correlated</strong>!</p>
<p>In a similar manner, we can argue that since <em>most</em> of the points in a scatterplot of <strong>negatively associated</strong> variables are located in the upper left and lower right boxes, most of the products being contributed to the sum of <span class="math">\(r\)</span> are negative (with a few positive ones sprinkled in). This gives us a <strong>negative</strong> number for <span class="math">\(r\)</span>. So we say that these variables are <strong>negatively correlated</strong>!</p>
</div>
<div id="computation-of-the-coefficients-in-the-regression-equation" class="section level2">
<h2>Computation of the Coefficients in the Regression Equation</h2>
<p>The regression equation is <span class="math">\(\hat{y}=a+bx\)</span>. You might be wondering…how are <span class="math">\(a\)</span> and <span class="math">\(b\)</span> calculated? The formula for the slope <span class="math">\(b\)</span> is:</p>
<p><span class="math">\[\mbox{slope }= b = r \cdot \frac{s_y}{s_x},\]</span></p>
<p>where</p>
<ul>
<li><span class="math">\(r\)</span> is the correlation coefficient,</li>
<li><span class="math">\(s_y\)</span> is the SD of the <span class="math">\(y\)</span>’s in the scatterplot, and</li>
<li><span class="math">\(s_x\)</span> is the SD of the <span class="math">\(x\)</span>’s in the scatterplot.</li>
</ul>
<p>The formula for the intercept <span class="math">\(a\)</span> is:</p>
<p><span class="math">\[\mbox{intercept }= a = \bar{y}-b\cdot\bar{x},\]</span></p>
<p>where</p>
<ul>
<li><span class="math">\(b\)</span> is the slope calculated above,</li>
<li><span class="math">\(\bar{y}\)</span> is the mean of the <span class="math">\(y\)</span>’s in the scatterplot, and</li>
<li><span class="math">\(\bar{x}\)</span> is the mean of the <span class="math">\(x\)</span>’s in the scatterplot.</li>
</ul>
<p>Before interpreting these formulas, let’s look at a little late 19th century history. Sir Francis Galton, a half-cousin of Charles Darwin, made important contributions to many scientific fields, including biology and statistics. He had a special interest in heredity and how traits are passed from parents to their offspring. He noticed that extreme characteristics in parents are not completely passed on to their children.</p>
<p>Consider how fathers’ heights is related to sons’ heights. See Figure[Galton].</p>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekxygalton-1.png" alt="Galton. Relationship Between Father and Sons’ Heights" /><p class="caption">Galton. Relationship Between Father and Sons’ Heights</p>
</div>
<p>It seems reasonable to think that an average height father would probably have an average height son. So surely our “best fit” line should pass through the <em>point of averages</em>, <span class="math">\((\bar{x},\bar{y})\)</span>. See Figure [Point of Averages]</p>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekxygaltonaverages-1.png" alt="Point of Averages. Galton data with the point of averages plotted." /><p class="caption">Point of Averages. Galton data with the point of averages plotted.</p>
</div>
<p>Intuitively, it might also seem that a reasonably tall father, say, 1 standard deviation taller than average would produce a reasonably tall son, also about 1 standard deviation taller than average. The line that would “best fit” this assumption would have slope equal to <span class="math">\(\frac{s_y}{s_x}\)</span>.</p>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekxygaltonsdline-1.png" alt="SD Line. Galton Data with SD line" /><p class="caption">SD Line. Galton Data with SD line</p>
</div>
<p>However, this <em>not</em> the “best fit” line. It does not minimize the Sum of Squares! Check out how the <em>regression</em> line looks in comparison to this <em>standard deviation</em> line.</p>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekxygaltonregression-1.png" alt="Regression. Galton data with SD line and regression line." /><p class="caption">Regression. Galton data with SD line and regression line.</p>
</div>
<p>The slope of the SD line is <span class="math">\(b=\frac{s_y}{s_x}\)</span>. The slope of the regression line is <span class="math">\(b=r\cdot\frac{s_y}{s_x}\)</span>. Since <span class="math">\(r\)</span> is a value between -1 and 1, you can see why this causes the regression line to be more <em>shallow</em>.</p>
<p>This is what is known as the <strong>regression effect</strong> or <strong>regression to the mean</strong>. Extremely tall fathers do tend to have taller than average sons, but the sons don’t tend to be as extreme in height as their fathers. Likewise for short fathers.</p>
<p>Check out the following app to explore this idea further!</p>
<pre class="r"><code>require(manipulate)
ShallowReg()</code></pre>
</div>
</div>
<div id="chapter-5" class="section level1">
<h1>Chapter 5</h1>
<div id="the-rbind-function" class="section level2">
<h2>The <code>rbind</code> Function</h2>
<p>The <code>rbind</code> function combines objects in R by rows. (It is called <code>rbind</code> to stand for “rowbind”.) If you have several lists stored and you want to combine them into one object, you can use <code>rbind</code>.</p>
<pre class="r"><code>list1=c(1,2,3)
list2=c(5, 6, 7)
list3=c(100, 200, 300)
rows=rbind(list1,list2,list3)
rows</code></pre>
<pre><code>##       [,1] [,2] [,3]
## list1    1    2    3
## list2    5    6    7
## list3  100  200  300</code></pre>
<p>Essentially, you have created a matrix. You can access objects out of <code>rows</code> similar to how you would access a value out of a list.</p>
<pre class="r"><code>rows[1,2] #gives the number in the 1st row and 2nd column</code></pre>
<pre><code>## list1 
##     2</code></pre>
<pre class="r"><code>rows[2,1] #gives the number in the 2nd row and 1st column</code></pre>
<pre><code>## list2 
##     5</code></pre>
</div>
<div id="the-cbind-function" class="section level2">
<h2>The <code>cbind</code> Function</h2>
<p>The <code>cbind</code> function is very similar to <code>rbind</code>. It combines objects in R by columns. (It is called <code>cbind</code> to stand for “columnbind”.)</p>
<pre class="r"><code>columns=cbind(list1,list2,list3)
columns</code></pre>
<pre><code>##      list1 list2 list3
## [1,]     1     5   100
## [2,]     2     6   200
## [3,]     3     7   300</code></pre>
<p>You can use <code>cbind</code> and <code>rbind</code> to combine objects other than numbers, such as characters.</p>
<pre class="r"><code>list4=c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)
list5=c(&quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;)
rows=rbind(list4,list5)
rows</code></pre>
<pre><code>##       [,1] [,2] [,3] [,4]
## list4 &quot;A&quot;  &quot;B&quot;  &quot;C&quot;  &quot;D&quot; 
## list5 &quot;E&quot;  &quot;F&quot;  &quot;G&quot;  &quot;H&quot;</code></pre>
<pre class="r"><code>columns=cbind(list4,list5)
columns</code></pre>
<pre><code>##      list4 list5
## [1,] &quot;A&quot;   &quot;E&quot;  
## [2,] &quot;B&quot;   &quot;F&quot;  
## [3,] &quot;C&quot;   &quot;G&quot;  
## [4,] &quot;D&quot;   &quot;H&quot;</code></pre>
</div>
</div>
<div id="chapter-6" class="section level1">
<h1>Chapter 6</h1>
<div id="the-role-of-limits-in-density-plots" class="section level2">
<h2>The Role of Limits in Density Plots</h2>
<p>Recall the grouped density plots, for example:</p>
<pre class="r"><code>densityplot(~sentence,data=attitudes,
            groups=def.race, plot.points=FALSE,
            main=&quot;Race and Recommended Sentence&quot;,
            xlab=&quot;Recommended Sentence&quot;,
            auto.key=list(space=&quot;right&quot;,title=&quot;Suggested\nRace&quot;),
            from=2,to=50)</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekdensitysent-1.png" alt="Race and Sentence. We set limits for the density curves." /><p class="caption">Race and Sentence. We set limits for the density curves.</p>
</div>
<p>The result is shown in Figure [Race and Sentence]. Density plots for different are especially effective when overlaid, because differences in the modes (the “humps”) of the distribution are readily apparent.</p>
<p>In the case of this data, we know that the minimum possible sentence is 2 and the maximum possible is 50. (These limits were specified on the survey forms.) Hence we should communicate these limits to R by means of the <code>from</code> and <code>to</code> arguments. R then constructs the kernel density estimators with these limits in mind.</p>
</div>
<div id="more-about-legends" class="section level2">
<h2>More about Legends</h2>
<p>There are many ways to modify the legend provided by the <code>auto.key</code> argument. These modifications are communicated by setting the values of certain other arguments and combining them in a list. The <code>space</code> argument is set by default to “top”, in which case the legend appears above the graph. It may also be set, to “left”, “right”, or “bottom”. A legend title may also be supplied through the argument title<code>.  Finally, settings a</code>columns` argument controls the layout of the elements in the legend (see Figure [Sentence by Defendant’s Race]:</p>
<pre class="r"><code>densityplot(~sentence,data=attitudes,
            groups=def.race, plot.points=FALSE,
            main=&quot;Race and Recommended Sentence&quot;,
            xlab=&quot;Recommended Sentence&quot;,
            auto.key=list(space=&quot;top&quot;,title=&quot;Suggested Race&quot;,columns=2),
            from=2,to=50)</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekdensitysetenceleg,%20fig-1.png" />
</div>
</div>
<div id="more-on-strip-plots" class="section level2">
<h2>More on Strip-plots</h2>
<p>Strip-plots are most effective when the groups sizes are small: when groups are large, many data values may equal one another, and <em>overplotting</em> will result. There are some techniques available to alleviate the effects, of over-plotting, though, provided the dataset is not too large. The two primary techniques are <em>jittering</em> and <em>translucence.</em></p>
<p>See Figure [Sentence by Major] for the result of the following code:</p>
<pre class="r"><code>stripplot(sentence~major,data=attitudes,
         main=&quot;Sentence By Major&quot;,xlab=&quot;Major&quot;,col=&quot;red&quot;,
         jitter.data=TRUE,alpha=0.5,
         panel= function(...){
           panel.violin(...)
           panel.stripplot(...)
         })</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekstripplotsentmaj-1.png" alt="Sentence by Major. Strip-plot comined with violin plot." /><p class="caption">Sentence by Major. Strip-plot comined with violin plot.</p>
</div>
<p>In the code above, setting the argument <code>jitter.data</code> to <code>TRUE</code> has the effect, in a strip-plot, of moving each point randomly a bit in the direction perpendicular to the axis along which the groups are ordered, thus separating the points from one another. The <code>alpha</code> argument has a default value of 1. When set to a value between 0 and 1, it introduces a degree of translucence to the points. At value 0.5, for example, two over plotted points would appear as dark as a single point would when alpha is set at 1.</p>
</div>
<div id="assessing-statistical-significance" class="section level2">
<h2>Assessing Statistical Significance</h2>
<p>Recall that in a randomized experiment, chance is always involved in the collection of the data, simply because it is involved in the assignment of subjects to treatment groups. Thus we can always ask the question of statistical significance. Let’s investigate that question for the Knife-or-Gun study.</p>
<p>When the consent problem restricts us from applying the results of an experiment to a larger population, we think about the problem in terms of the set of subjects themselves. We adopt what is known as the <em>ticket model</em>.</p>
<p>In the ticket model, we imagine that every subject has a magical ticket. Values of the response variable for that subject under the various possible treatments are written on fixed areas of the ticket. In the Knife or Gun study, we imagine that on the left-hand side of the ticket is written the volume of the dying screams he or she would emit, should he or she be killed with a knife. In the right-hand side of the ticket is written the volume of screams he/she would emit if being killed by a gun.</p>
<p>In the ticket model, the question of whether or not the explanatory variable makes a difference in the response variable boils down to what is on these tickets. For a subject with a ticket like</p>
<blockquote>
<p>(Knife 65, Gun 65),</p>
</blockquote>
<p>the means of slaying makes no difference: she would yell at volume 65 regardless of whether she was killed by knife or by gun. For a subject with a ticket reading</p>
<blockquote>
<p>(Knife 70, Gun 67),</p>
</blockquote>
<p>the means of slaying makes a difference: being killed by a knife would make her yell louder.</p>
<p>The tickets are truly magical, because researchers are allowed to read at most one part of any person’s ticket. That’s because each subject is assigned to just one treatment group. Competing hypotheses about the effect of the means of slaying on the volume of yells can be stated in terms of the ticket model as follows:</p>
<blockquote>
<p><span class="math">\(H_0\)</span> [Means of slaying makes no difference, on average, for the subjects]: The mean of the Knife-side of the tickets of all subjects equals the mean of the Gun-side of the tickets.</p>
</blockquote>
<blockquote>
<p><span class="math">\(H_a\)</span> [On average, dying by gun makes the subjects yell louder]: The mean of the Knife-side of the tickets of all subjects is greater than the mean of the Gun-side of the tickets.</p>
</blockquote>
<p>We have stated our hypotheses. That was Step One of a test of significance.</p>
<p>Now for Step Two: computing a test statistic. A reasonable test statistic would be the difference of sample means:</p>
<pre class="r"><code>compareMean(volume~means,
            data=knifeorgunblock)</code></pre>
<pre><code>## [1] 20.13</code></pre>
<p>The difference of means is 20.13, indicating that on average the Knife subjects yelled 20.13 decibels louder than the Gun subjects did.</p>
<p>Next, Step Three: computing the P-value. We would like to know the probability of getting a difference in sample means at least as big as the one we actually got, if <span class="math">\(H_0\)</span> is actually true.</p>
<p>To find this probability we imagine—temporarily and for the sake of argument only—that the NUll is really true. In fact, we’ll make the extra-strong assumption that the Null is super-duper true: that means of slaying makes no difference for ANY subject. In that case, for every the number on the Knife-side equals the number on the Gun-side. If that’s true, then we actually know all of the numbers on all of the tickets. (Reading one side—that, is, killing the subject—tells us what the other side says.)</p>
<p>This neat fact puts us in the happy position of being able to simulate what would happen if we were to repeat the experiment many, many times. We would have the same 20 subjects each time: only the group assignments would differ. But no matter the group assignment, we can tell what each person’s dying screams will be.</p>
<p>For convenience, we’ll write a function that pretends to run the who experiment all over again, with blocking, computing the difference in the mean volumes of yells for each group, each time, and recording the difference:</p>
<pre class="r"><code>set.seed(12345)
KnifeGunSim &lt;- do(500)*compareMean(volume~treat.grp,
                    data=RandomExp(knifeorgunblock,
                      sizes=c(10,10),groups=c(&quot;k&quot;,&quot;g&quot;),
                      block=&quot;hoghollerer&quot;))</code></pre>
<p>Let’s look at the first few simulations::</p>
<pre class="r"><code>head(KnifeGunSim,n=5)</code></pre>
<pre><code>##   result
## 1   6.35
## 2   0.77
## 3  -1.11
## 4  -2.27
## 5  -1.55</code></pre>
<p>Remember: these differences are all based on the assumption that means of slaying has no effect at all on the volume of dying screams. So, about how big are the differences, when the Null is right? Let’s see:</p>
<pre class="r"><code>favstats(~result,data=KnifeGunSim)</code></pre>
<pre><code>##     min    Q1 median    Q3   max     mean       sd   n missing
##  -13.93 -3.89  -0.12 3.835 15.91 -0.17104 5.495935 500       0</code></pre>
<p>As you might expect, the typical difference is quite small: about 0, give or take 5.5 or so. The difference we saw in the study (20.13) was about four SDs above what the Null would expect.</p>
<p>In fact, the maximum of the simulated differences was only 12.73: not once in our 500 simulations did the test statistic exceed the value of the test statistic that we got in the actual study.</p>
<p>This gives us Step Four in a test of significance: the P-value is very small, probably less than one in 500, so we reject <span class="math">\(H_0\)</span>.</p>
<p>This study provided very strong evidence that, <em>for these 20 subjects</em>, slaying with a knife evokes louder yells than slaying with a gun does.</p>
</div>
<div id="interaction" class="section level2">
<h2>Interaction</h2>
<p>There is one other important concept that often applies in experiments, that wee think bears a leisurely discussion: it is the concept of <em>interaction</em>.</p>
<pre class="r"><code>data(ToothGrowth)
View(ToothGrowth)
help(ToothGrowth)</code></pre>
<pre class="r"><code>bwplot(len~as.factor(dose)|supp,data=ToothGrowth)</code></pre>
<div class="figure">
<img src="GeekNotes_files/figure-html/geekbwlendose-1.png" alt="Tooth growth." /><p class="caption">Tooth growth.</p>
</div>
<p>Figure [Tooth growth] shows boxplots of the data. In both panels, the boxes rise as you read to the right. Hence, for both values of the explanatory variable <strong>supp</strong>, the length of tooth increases as dosage (also an explanatory variable) increases. However, the increase in length as dosage of Vitamin c increases from 1 to 2 is greater when the dosage method is by ascorbic acid (VC) than when the Vitamin C is administered in the form of orange juice (OJ). Hence, the effect of *dose<strong> on </strong>len<strong> differs with differing values of the other explanatory variable </strong>supp<strong>. Because of this difference, the variables </strong>dose<strong> and </strong>supp** are said to be <em>interact</em>. The formal definition follows:</p>
<dl>
<dt>Interaction</dt>
<dd>Two explanatory variables <span class="math">\(X_1\)</span> and <span class="math">\(X_2\)</span> are said to <em>interact</em> when the relationship between <span class="math">\(X_1\)</span> and the response variable <span class="math">\(Y\)</span> differs as the values of the other variable <span class="math">\(X_2\)</span> differ.
</dd>
</dl>
<blockquote>
<p><strong>Practice</strong>: In each of the situations below, say whether there is a confounding variable present, or whether there is interaction. In the confounding case, identify the confounding variable and explain why it is a confounder. In the interaction case, identify the two explanatory variables that interact.</p>
</blockquote>
<blockquote>
<p>(1). In a study of the effect of sports participation and sex on academic performance, it is found that the mean GPA of male athletes is 0.7 points less than the mean GPA of female athletes, but the mean GPA of male non-athletes is only 0.2 points lower than the mean GPA of female non=athletes.</p>
</blockquote>
<blockquote>
<p>(2). In a study of the effect of alcohol on the risk of cancer, it is found that heavy drinkers get cancer at a higher rate than moderate drinkers do. However, it is known that smokers also tend to drink more than non-smokers, and that smoking causes various forms of cancer.</p>
</blockquote>
<p>As another example, consider the <code>pushups</code> data frame:</p>
<pre class="r"><code>data(pushups)
View(pushups)
help(pushups)</code></pre>
<p>Play with the data using the a Dynamic Trellis app:</p>
<pre class="r"><code>require(manipulate)
DtrellScat(pushups~weight|position,data=pushups)</code></pre>
<p>The relationship between weight and push-ups varies depending on position: for Skill players the relationship is somewhat positive (the scatterplot rises as you read to the right), but for Skill players the relationship is somewhat negative (scatterplot falls as you move to the right). Thus, variables <strong>weight</strong> and <strong>position</strong> appear to interact. One might wonder, though, whether the observed interaction is statistically significant: ater all, there weren’t many Line players in the study to begin with.</p>
</div>
</div>
<div id="chapter-8" class="section level1">
<h1>Chapter 8</h1>
<div id="we-lied-about-the-sd-formulas" class="section level2">
<h2>We Lied About the SD Formulas!</h2>
<p>Recall the SimpleRandom app: let’s play with it one more time:</p>
<pre class="r"><code>require(manipulate)
SimpleRandom()</code></pre>
<p>This time, pick one of the variables and move the slider up to the sample size 10,000. Click on the slider several times, keeping it set at 10,000. Watch the output to the console.</p>
<p>You probably noticed that the sample statistics did not change from sample to sample, and that they were equal to the population parameters every time. This makes sense, because when the sample size is the same as the size of the population, then simple random sampling produces a sample that HAS to be the population, each and every time!</p>
<p>But wait a minute: if the sample statistic is ALWAYS equal to the population parameter, then the likely amount by which the statistic differs from the parameter is ZERO. Hence the SD of the estimator should be zero. Fro example, if we are estimating the mean height of <code>imagpop</code>, then the SD of <span class="math">\(\bar{x}\)</span> should be zero. But the formula we gave for the SD is:</p>
<p><span class="math">\[\frac{\sigma}{\sqrt{n}}=\frac{\sigma}{\sqrt{10000}}=\frac{\sigma}{100},\]</span></p>
<p>which has to be BIGGER than zero. Therefore the formula is wrong.</p>
<p>Well, it is wrong for simple random sampling. It is correct for random sampling <em>with replacement</em> form the population. The correct formula for the SD of <span class="math">\(\bar{x}\)</span>, when we are taking a simple random sample – sampling without replacement – is:</p>
<p><span class="math">\[Sd(\bar{x})=\frac{\sigma}{\sqrt{n}} \times \sqrt{\frac{N-n}{N-1}},\]</span></p>
<p>where <span class="math">\(n\)</span> is the sample size and <span class="math">\(N\)</span> is the size of the population. The quantity</p>
<p><span class="math">\[\sqrt{\frac{N-n}{N-1}}\]</span></p>
<p>is called the <em>correction factor</em>.</p>
<p>As you can see, at sample size <span class="math">\(n = 10000\)</span> and population size <span class="math">\(N=10000\)</span> the quantity <span class="math">\(N-n\)</span> will be zero, forcing the correction factor to be zero, and thus forcing the SD of <span class="math">\(\bar{x}\)</span> to be zero as well.</p>
<p>Usually we don’t bother with the correction factor in practice, because usually <span class="math">\(n\)</span> is small in comparison to <span class="math">\(N\)</span>. For example, when we take a SRS of size <span class="math">\(n=2500\)</span> from the population of the United States (<span class="math">\(N \approx 312000000\)</span>), then the correction factor is equal to:</p>
<pre class="r"><code>N &lt;- 312000000
n &lt;- 2500
CorrFac &lt;- sqrt((N-n)/(N-1))
CorrFac</code></pre>
<pre><code>## [1] 0.999996</code></pre>
<p>The correction factor is approximately 0.999996,which so close to 1 that it is rounded to one in the Knitted version of this document. We know that multiplying a number by 1, won’t change the original number, so multiplying the “Wrong” SD formula by the correction factor barely changes the number at all.</p>
<p>If you happen to know the population size, however, there is no harm in using the correct SD formula, with the correction factor.</p>
<p>The same correction factor shows up in the correct SD formulas for <span class="math">\(\hat{p}\)</span> and for <span class="math">\(\bar{d}\)</span>, and there are correction factors for the SDs of the other two Basic Five parameters, too.</p>
</div>
<div id="are-we-only-ever-interested-in-population-parameters" class="section level2">
<h2>Are We Only Ever Interested in Population Parameters?</h2>
<p>We have spent the whole chapter on population parameters and the statistics that we use to estimate them. But are we only ever interested in population parameters? Are statistics never used to estimate anything else?</p>
<p>The quick answer is No, there are times when the number we want to estimate is not a parameter for a population. For example, sometimes we want to estimate a probability:</p>
<ul>
<li>If we would like to know the probability <span class="math">\(p\)</span> for a coin to land Heads, then we might toss the coin many times, compute the proportion <span class="math">\(\hat{p}\)</span> of times that the coin landed Heads, and use this to estimate <span class="math">\(p\)</span>. We weren’t actually taking a sample, because there isn’t really a “population” of coin tosses to sample from.</li>
<li>Another example: we often estimate a P-value by simulation. Again the P-value is a number – the probability of getting data as extreme as the data we actually got, if the Null is true – and we estimate it by simulating the study on the computer many times with a set-up in which the Null is true. Here again, we are simulation many times, but not sampling from some “population” of all possible simulations.</li>
</ul>
<p>When we estimate a probability by simulation, we still call the probability a “parameter”. It’s just not a <em>population</em> parameter.</p>
<p>On the other hand, some problems that do not appear to be about population parameters really are problems about populations parameters, in disguise. A good example would be a question about the relationship between two categorical variables, for example:</p>
<p><strong>Research Question:</strong> At Georgetown College, is sex related to seating preference?</p>
<p>We could frame the question about relationship as a question about some populations proportions. Let:</p>
<ul>
<li><span class="math">\(p_{male,front}\)</span> = the proportion of all GC males who prefer to sit in front;</li>
<li><span class="math">\(p_{female,front}\)</span> = the proportion of all GC females who prefer to sit in front;</li>
<li><span class="math">\(p_{male,middle}\)</span> = the proportion of all GC males who prefer to sit in the middle;</li>
<li><span class="math">\(p_{female,middle}\)</span> = the proportion of all GC females who prefer to sit in the middle;</li>
<li><span class="math">\(p_{male,back}\)</span> = the proportion of all GC males who prefer to sit in back;</li>
<li><span class="math">\(p_{female,back}\)</span> = the proportion of all GC females who prefer to sit in back;</li>
</ul>
<p>Then someone who believes that sex and seat are unrelated at GC believes three things</p>
<ul>
<li><span class="math">\(p_{male,front} = p_{female,front}\)</span></li>
<li><span class="math">\(p_{male,middle} = p_{female,middle}\)</span></li>
<li><span class="math">\(p_{male,back} = p_{female,back}\)</span></li>
</ul>
<p>Someone who thinks that the two variables are related believes that at least one of the three equalities above is incorrect.</p>
</div>
</div>
<div id="chapter-9" class="section level1">
<h1>Chapter 9</h1>
<div id="distinction-between-t-and-z-in-confidence-intervals-for-means" class="section level2">
<h2>Distinction Between <span class="math">\(t\)</span> and <span class="math">\(z\)</span> in Confidence Intervals for Means</h2>
<p>In order to more fully understand the distinction between using the <span class="math">\(t\)</span>-multiplier (from the <span class="math">\(t\)</span>-distribution) and using the <span class="math">\(z\)</span>-multiplier (from the normal distribution) in the construction of confidence intervals for <strong>means</strong>, let’s first remind ourselves of the statement of the Central Limit Theorem.</p>
<p><strong>Central Limit Theorem</strong>: For any population with a finite mean <span class="math">\(\mu\)</span> and finite standard deviation <span class="math">\(\sigma\)</span>, the distribution of the sample mean <span class="math">\(\bar{x}\)</span> gets closer and closer to</p>
<p><span class="math">\[norm(\mu,\frac{\sigma}{\sqrt{n}})\]</span></p>
<p>as the sample size <span class="math">\(n\)</span> gets larger and larger.</p>
<p>Now, let’s consider four cases:</p>
<p><strong>Case 1:</strong> The population standard deviation, <span class="math">\(\sigma\)</span>, is <strong>known</strong> and the population is normally distributed.</p>
<p>When we know <span class="math">\(\sigma\)</span>, we do not have to approximate the SD with the SE in the formula for the confidence interval. In addition, we can find our <span class="math">\(z\)</span>-score = <span class="math">\(\dfrac{\bar{x}-\mu}{\frac{\sigma}{\sqrt{n}}}\)</span> exactly.</p>
<p>Furthermore, if the population from which we are drawing our sample is normal, then our sample estimate, <span class="math">\(\bar{x}\)</span>, is also going to exactly follow a normal distribution, <em>regardless of the sample size</em>. This means that the <span class="math">\(z\)</span>-score comes from the normal curve.</p>
<p>So, <span class="math">\(\dfrac{\bar{x}-\mu}{\dfrac{\sigma}{\sqrt{n}}}\)</span> exactly follows a standard normal distribution, regardless of the sample size.</p>
<p>In this situation, the <span class="math">\(z\)</span>-multiplier is actually the correct multiplier to use. However, this situation rarely crops up. It is very unlikely that we would know the distribution of our population and know the population standard deviation.</p>
<p><strong>Case 2:</strong> The population standard deviation, <span class="math">\(\sigma\)</span>, is <strong>known</strong> and the population is not normally distributed.</p>
<p>The difference here is that we either don’t know if the population is normally distributed or we know that it is not. For this reason, we would be unable to say that the estimator, <span class="math">\(\bar{x}\)</span>, follows a normal distribution. However, the Central Limit Theorem ensures that for large enough sample sizes, <span class="math">\(\bar{x}\)</span> is <em>approximately</em> normally distributed. So, in this case, even though we are not making an approximation to the SD(<span class="math">\(\bar{x}\)</span>) (since we know <span class="math">\(\sigma\)</span>), we are making an approximation when we use a <span class="math">\(z\)</span>-multplier (since we don’t know that <span class="math">\(\bar{x}\)</span> follows a normal distribution exactly).</p>
<p>So, <span class="math">\(\dfrac{\bar{x}-\mu}{\dfrac{\sigma}{\sqrt{n}}}\)</span> approximately follows a standard normal distribution for large sample sizes. The Central Limit Theorem guarantees nothing about small sample sizes.</p>
<p>For this situation, it is still acceptable to use the <span class="math">\(z\)</span>-multiplier as long as your sample is not too small.</p>
<p><strong>Case 3:</strong> The population standard deviation, <span class="math">\(\sigma\)</span>, is <strong>unknown</strong> and the population is normally distributed.</p>
<p>For this case, <span class="math">\(\bar{x}\)</span> is normally distributed regardless of the sample size. However, since we do not know <span class="math">\(\sigma\)</span>, we must use the <span class="math">\(t\)</span>-multiplier, <span class="math">\(\dfrac{\bar{x}-\mu}{\dfrac{s}{\sqrt{n}}}\)</span>, with <span class="math">\(n-1\)</span> degrees of freedom. The numerator of this ratio is normally distributed, but the denominator is not. Since <span class="math">\(s\)</span> is a random variable, the denominator is a random variable. Thus, we have a ratio of two random variables and this has a <span class="math">\(t\)</span> distribution.</p>
<p>So, <span class="math">\(\dfrac{\bar{x}-\mu}{\dfrac{s}{\sqrt{n}}}\)</span> exactly follows a <span class="math">\(t\)</span>- distribution with <span class="math">\(n-1\)</span> degrees of freedom, regardless of the sample size.</p>
<p>For this situation, you should always use the <span class="math">\(t\)</span>-distribution with <span class="math">\(n-1\)</span> degrees of freedom.</p>
<p><strong>Case 4:</strong> The population standard deviation, <span class="math">\(\sigma\)</span>, is <strong>unknown</strong> and population is not normally distributed.</p>
<p>Here again, we must rely on the Central Limit Theorem. For large sample sizes, <span class="math">\(\dfrac{\bar{x}-\mu}{\dfrac{s}{\sqrt{n}}}\)</span> will approach a standard normal distribution.</p>
<p>The decision on which multiplier to use for this situation is somewhat ambiguous. For small sample sizes, you can’t do anything without <em>assuming</em> that the population is normally distributed. Even if this assumption is not really correct, the <span class="math">\(t\)</span>-distribution is likely to be approximately right. For large sample sizes, you have the Central Limit Theorem to ensure your assuption of normality. Regardless of whether you decide to use the <span class="math">\(t\)</span> or <span class="math">\(z\)</span>-multiplier, you are still using an approximation. If you use the <span class="math">\(z\)</span>-mutliplier, you are assuming that the sample size is big enough that <span class="math">\(\dfrac{\bar{x}-\mu}{\dfrac{s}{\sqrt{n}}}\)</span> is well approximated by the standard normal distribution, i.e., that the Central Limit Theorem holds. If you use the <span class="math">\(t\)</span>-mutliplier, you are assuming that the population can be well approximated by the normal distribution.</p>
<p>The likelihood of us knowing the real value of <span class="math">\(\sigma\)</span> are slim to none, being that <span class="math">\(\sigma\)</span> is a population parameter. Chances are, we will be dealing with Case 4. We are relying on an assumption for this case, regardless of the multiplier we choose to use. So which one should we choose?</p>
<p>Since the <span class="math">\(t\)</span>-distribution carries more weight in it’s tails than the normal distritbution, the <span class="math">\(t\)</span>-multipliers are always a little bigger than the <span class="math">\(z\)</span>-mutlipliers (for the same confidence level). For this reason, the confidence interval that is calculated using a <span class="math">\(t\)</span>-multiplier will be slightly wider than the confidence interval calculated using the <span class="math">\(z\)</span>-mutliplier. Using the <span class="math">\(t\)</span>-mutliplier makes our confidence interval estimate more conservative. This is one reason why we choose to always stick to using the <span class="math">\(t\)</span>-distribution in the calculation of confidence intervals for means.</p>
</div>
<div id="how-does-r-find-df" class="section level2">
<h2>How Does R Find <span class="math">\(df\)</span>?</h2>
<p>When you are dealing with a situation where you have sampled from two independent populations, degrees of freedom is more difficult to calculate. We can’t just take <span class="math">\(n-1\)</span> because we have two sample sizes, <span class="math">\(n_1\)</span> and <span class="math">\(n_2\)</span>. There are different methods for calculating <span class="math">\(df\)</span>. One method is to use one less than the smaller of the two sample sizes for the degrees of freedom. In other words, <span class="math">\[df=min(n_1-1,n_2-1).\]</span></p>
<p>If the standard deviations of the two samples are equal, another method is to use two less than the sum of the two sample sizes for the degrees of freedom. In other words, <span class="math">\[df=n_1+n_2-2.\]</span> In fact, by setting <code>var.equal=TRUE</code> in the <code>ttestGC</code> function, R will use this formula for <span class="math">\(df\)</span>.</p>
<p>By default, the function <code>ttestGC</code> in R uses the Welch-Satterthwaite equation to calculate degrees of freedom for the 2 sample test of means.</p>
<p><span class="math">\[df =\dfrac{\bigg(\dfrac{s_1^2}{n_1}+\dfrac{s_2^2}{n_2}\bigg)^2}{\dfrac{s_1^4}{n_1^2(n_1-1)}+\dfrac{s_2^2}{n_2^2(n_2-1)}}\]</span></p>
<blockquote>
<p><em>Research Question:</em> Do GC males sleep more at night, on average, than GC females?</p>
</blockquote>
<p>The <code>ttestGC</code> function gives us the following:</p>
<pre class="r"><code>ttestGC(sleep~sex,data=m111survey)</code></pre>
<pre><code>## 
## 
## Inferential Procedures for the Difference of Two Means mu1-mu2:
##  (Welch&#39;s Approximation Used for Degrees of Freedom)
##   sleep grouped by sex 
## 
## 
## Descriptive Results:
## 
##   group  mean    sd  n
##  female 6.325 1.619 40
##    male 6.484 1.557 31
## 
## 
## Inferential Results:
## 
## Estimate of mu1-mu2:  -0.1589 
## SE(x1.bar - x2.bar):  0.3792 
## 
## 95% Confidence Interval for mu1-mu2:
## 
##           lower.bound         upper.bound          
##           -0.915971           0.598229</code></pre>
<p>The <span class="math">\(df=\)</span> 65.81. Let’s use the Welch-Satterthwaite equation to verify this.</p>
<pre><code>##   .group min Q1 median    Q3 max     mean       sd  n missing
## 1 female   2  5   6.75 7.125   9 6.325000 1.619394 40       0
## 2   male   4  5   7.00 7.000  10 6.483871 1.557155 31       0</code></pre>
<p>The following statistics will be used in our calculation of <span class="math">\(df\)</span>:</p>
<ul>
<li><p><span class="math">\(s_1=\)</span> standard deviation of the females amount of sleep = 6.325.</p></li>
<li><p><span class="math">\(s_2=\)</span> standard deviation of the males amount of sleep = 6.483871.</p></li>
<li><p><span class="math">\(n_1=\)</span> sample size of females = 1.6193937.</p></li>
<li><p><span class="math">\(n_1=\)</span> sample size of females = 1.5571548.</p></li>
</ul>
<p>So, <span class="math">\(df=\dfrac{\bigg(\dfrac{s_1^2}{n_1}+\dfrac{s_2^2}{n_2}\bigg)^2}{\dfrac{s_1^4}{n_1^2(n_1-1)}+\dfrac{s_2^2}{n_2^2(n_2-1)}} = \dfrac{\bigg(\dfrac{6.325^2}{1.6193937}+\dfrac{6.483871^2}{1.5571548}\bigg)^2}{\dfrac{6.325^4}{1.6193937^2(1.6193937-1)}+\dfrac{6.483871^4}{1.5571548^2(1.5571548-1)}} =\)</span> 1.1654895 .</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
